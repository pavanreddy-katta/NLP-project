{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f2cd8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import networkx as nx\n",
    "from torch_geometric.utils import erdos_renyi_graph, to_networkx, from_networkx\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "import math\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import transformers\n",
    "from transformers import RobertaTokenizer, BertTokenizer, RobertaModel, BertModel, AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "import pprint\n",
    "import time\n",
    "import timeit\n",
    "\n",
    "from graphModels import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5caf19d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 1024\n",
    "CHUNK_LEN = 200\n",
    "OVERLAP_LEN = int(CHUNK_LEN/2)\n",
    "\n",
    "TRAIN_BATCH_SIZE = 16\n",
    "EPOCH = 20\n",
    "lr=1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82b1daa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocess:\n",
    "    \n",
    "    def __init__(self, trainPath, devPath, testPath):\n",
    "        self.train_df = pd.read_csv(trainPath, sep = '\\t', header=0)\n",
    "        self.train_df['review'] = self.train_df['headline'].str.cat(self.train_df['text'], sep=' ')\n",
    "        \n",
    "        self.valid_df = pd.read_csv(devPath, sep = '\\t', header=0)\n",
    "        self.valid_df['review'] = self.valid_df['headline'].str.cat(self.valid_df['text'], sep=' ')\n",
    "        \n",
    "        self.test_df = pd.read_csv(testPath, sep = '\\t', header=0)\n",
    "        self.test_df['review'] = self.test_df['headline'].str.cat(self.test_df['text'], sep=' ')\n",
    "        \n",
    "    \n",
    "    def clean_text(self, sentence):\n",
    "        cleaned_sentence = re.sub(r'[^a-zA-Z0-9\\s]', ' ', sentence)\n",
    "        cleaned_sentence = re.sub(r'\\s+', ' ', cleaned_sentence).strip()\n",
    "        return cleaned_sentence.lower()\n",
    "        \n",
    "    def get_clean(self):\n",
    "        self.train_df['cleaned_text'] = self.train_df['review'].apply(self.clean_text)\n",
    "        self.valid_df['cleaned_text'] = self.valid_df['review'].apply(self.clean_text)\n",
    "        self.test_df['cleaned_text'] = self.test_df['review'].apply(self.clean_text)\n",
    "        return self.train_df[['cleaned_text', 'label']], self.valid_df[['cleaned_text', 'label']], self.test_df[['cleaned_text', 'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dea279e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pr = Preprocess(\"/scratch/smanduru/NLP/project/data/amazon_2048/amazon-books-2048-train.tsv\",\n",
    "               \"/scratch/smanduru/NLP/project/data/amazon_2048/amazon-books-2048-dev.tsv\",\n",
    "               \"/scratch/smanduru/NLP/project/data/amazon_2048/amazon-books-2048-test.tsv\")\n",
    "\n",
    "train, valid, test = pr.get_clean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90041d68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1000, 2), (1000, 2), (8000, 2))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid.shape, test.shape, train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da6d8df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# roberta_tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79ba3e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, tokenizer, max_len, df, chunk_len=200, overlap_len=50, approach=\"all\", max_size_dataset=None, min_len=249):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.overlap_len = overlap_len\n",
    "        self.chunk_len = chunk_len\n",
    "        self.approach = approach\n",
    "        self.min_len = min_len\n",
    "        self.max_size_dataset = max_size_dataset\n",
    "        self.data, self.label = self.process_data(df)\n",
    "        \n",
    "    def process_data(self, df):\n",
    "        self.num_class = len(set(df['label'].values))\n",
    "        return df['cleaned_text'].values, df['label'].values\n",
    "    \n",
    "    def long_terms_tokenizer(self, data_tokenize, targets):\n",
    "        long_terms_token = []\n",
    "        input_ids_list = []\n",
    "        attention_mask_list = []\n",
    "        token_type_ids_list = []\n",
    "        targets_list = []\n",
    "\n",
    "        previous_input_ids = data_tokenize[\"input_ids\"].reshape(-1)\n",
    "        previous_attention_mask = data_tokenize[\"attention_mask\"].reshape(-1)\n",
    "        previous_token_type_ids = data_tokenize[\"token_type_ids\"].reshape(-1)\n",
    "        remain = data_tokenize.get(\"overflowing_tokens\")\n",
    "        targets = torch.tensor(targets, dtype=torch.int)\n",
    "        \n",
    "        start_token = torch.tensor([101], dtype=torch.long)\n",
    "        end_token = torch.tensor([102], dtype=torch.long)\n",
    "\n",
    "        total_token = len(previous_input_ids) -2 # remove head 101, tail 102\n",
    "        stride = self.overlap_len - 2\n",
    "        number_chunks = math.floor(total_token/stride)\n",
    "\n",
    "        mask_list = torch.ones(self.chunk_len, dtype=torch.long)\n",
    "        type_list = torch.zeros(self.chunk_len, dtype=torch.long)\n",
    "        \n",
    "        for current in range(number_chunks-1):\n",
    "            input_ids = previous_input_ids[current*stride:current*stride+self.chunk_len-2]\n",
    "            input_ids = torch.cat((start_token, input_ids, end_token))\n",
    "            input_ids_list.append(input_ids)\n",
    "\n",
    "            attention_mask_list.append(mask_list)\n",
    "            token_type_ids_list.append(type_list)\n",
    "            targets_list.append(targets)\n",
    "\n",
    "        if len(input_ids_list) == 0:\n",
    "            input_ids = torch.ones(self.chunk_len-2, dtype=torch.long)\n",
    "            input_ids = torch.cat((start_token, input_ids, end_token))\n",
    "            input_ids_list.append(input_ids)\n",
    "\n",
    "            attention_mask_list.append(mask_list)\n",
    "            token_type_ids_list.append(type_list)\n",
    "            targets_list.append(targets)\n",
    "\n",
    "        return({\n",
    "            'ids': input_ids_list,\n",
    "            'mask': attention_mask_list,\n",
    "            'token_type_ids': token_type_ids_list,\n",
    "            'targets': targets_list,\n",
    "            'len': [torch.tensor(len(targets_list), dtype=torch.long)]\n",
    "        })\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        review = str(self.data[idx])\n",
    "        targets = int(self.label[idx])\n",
    "        data = self.tokenizer.encode_plus(\n",
    "            review,\n",
    "            max_length=self.max_len,\n",
    "            pad_to_max_length=False,\n",
    "            add_special_tokens=True,\n",
    "            return_attention_mask=True,\n",
    "            return_token_type_ids=True,\n",
    "            return_overflowing_tokens=True,\n",
    "            return_tensors='pt')\n",
    "        \n",
    "        long_token = self.long_terms_tokenizer(data, targets)\n",
    "        return long_token\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.label.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3648515a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(\n",
    "    tokenizer = bert_tokenizer,\n",
    "    max_len = MAX_LEN,\n",
    "    chunk_len = CHUNK_LEN,\n",
    "    overlap_len = OVERLAP_LEN,\n",
    "    df = train)\n",
    "\n",
    "\n",
    "valid_dataset = CustomDataset(\n",
    "    tokenizer = bert_tokenizer,\n",
    "    max_len = MAX_LEN,\n",
    "    chunk_len = CHUNK_LEN,\n",
    "    overlap_len = OVERLAP_LEN,\n",
    "    df = valid)\n",
    "\n",
    "    \n",
    "test_dataset = CustomDataset(\n",
    "    tokenizer = bert_tokenizer,\n",
    "    max_len = MAX_LEN,\n",
    "    chunk_len = CHUNK_LEN,\n",
    "    overlap_len = OVERLAP_LEN,\n",
    "    df = test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f61d2c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_collate1(batches):\n",
    "    return [{key: torch.stack(value) for key, value in batch.items()} for batch in batches]\n",
    "\n",
    "train_loader = DataLoader(train_dataset,\n",
    "                          batch_size = TRAIN_BATCH_SIZE, \n",
    "                          shuffle = True, \n",
    "                          collate_fn = my_collate1)\n",
    "\n",
    "valid_loader = DataLoader(valid_dataset,\n",
    "                          batch_size = 32, \n",
    "                          shuffle = False, \n",
    "                          collate_fn = my_collate1)\n",
    "\n",
    "test_loader = DataLoader(test_dataset,\n",
    "                          batch_size = 32, \n",
    "                          shuffle = False, \n",
    "                          collate_fn = my_collate1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e2ce60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking Data Loading\n",
    "\n",
    "# for batch_idx, batch in enumerate(test_loader):\n",
    "#     ids = batch[batch_idx]['ids']\n",
    "#     mask = batch[batch_idx]['mask']\n",
    "#     token_type_ids = batch[batch_idx]['token_type_ids']\n",
    "#     targets = batch[batch_idx]['targets']\n",
    "#     length = batch[batch_idx]['len']\n",
    "\n",
    "#     # Now you can print or process these items as needed\n",
    "#     print(f\"Batch {batch_idx + 1} IDs: {ids}\")\n",
    "#     print(f\"Batch {batch_idx + 1} Mask: {mask}\")\n",
    "#     print(f\"Batch {batch_idx + 1} Token Type IDs: {token_type_ids}\")\n",
    "#     print(f\"Batch {batch_idx + 1} Targets: {targets}\")\n",
    "#     print(f\"Batch {batch_idx + 1} Length: {length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb30f2ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8057ad20",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_training_steps = int(len(train_dataset) / TRAIN_BATCH_SIZE * EPOCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d00aae18",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hi_Bert_Classification_Model_GCN(nn.Module):\n",
    "    \n",
    "    \"\"\" A Model for bert fine tuning, put an lstm on top of BERT encoding \"\"\"\n",
    "\n",
    "    def __init__(self, graph_type, num_class, device, adj_method, pooling_method='mean'):\n",
    "        super(Hi_Bert_Classification_Model_GCN, self).__init__()\n",
    "        self.graph_type = graph_type\n",
    "        self.bert_path = 'bert-base-uncased'\n",
    "        self.bert = transformers.BertModel.from_pretrained(self.bert_path)\n",
    "        \n",
    "        # self.roberta = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "\n",
    "        self.lstm_layer_number = 2\n",
    "        'default 128 and 32'\n",
    "        self.lstm_hidden_size = 128\n",
    "        self.hidden_dim = 32\n",
    "        \n",
    "        # self.bert_lstm = nn.Linear(768, self.lstm_hidden_size)\n",
    "        self.device = device\n",
    "        self.pooling_method=pooling_method\n",
    "\n",
    "        self.mapping = nn.Linear(768, self.lstm_hidden_size).to(device)\n",
    "\n",
    "        'start GCN'\n",
    "        if self.graph_type == 'gcn':\n",
    "            self.gcn = GCN(input_dim=self.lstm_hidden_size, hidden_dim=32, output_dim=num_class).to(device)\n",
    "        elif self.graph_type == 'gat':\n",
    "            self.gcn = GAT(input_dim=self.lstm_hidden_size, hidden_dim=32, output_dim=num_class).to(device)\n",
    "        elif self.graph_type == 'graphsage':\n",
    "            self.gcn = GraphSAGE(input_dim=self.lstm_hidden_size, hidden_dim=32, output_dim=num_class).to(device)\n",
    "        elif self.graph_type == 'linear':\n",
    "            self.gcn = LinearFirst(input_dim=self.lstm_hidden_size, hidden_dim=32, output_dim=num_class).to(device)\n",
    "        elif self.graph_type == 'rank':\n",
    "            self.gcn = SimpleRank(input_dim=self.lstm_hidden_size, hidden_dim=32, output_dim=num_class).to(device)\n",
    "        elif self.graph_type == 'diffpool':\n",
    "            self.gcn = DiffPool(self.device,max_nodes=10,input_dim=self.lstm_hidden_size, hidden_dim=32, output_dim=num_class).to(device)\n",
    "        elif self.graph_type == 'hipool':\n",
    "            self.gcn = HiPool(self.device,input_dim=self.lstm_hidden_size, hidden_dim=32, output_dim=num_class).to(device)\n",
    "            \n",
    "        self.adj_method = adj_method\n",
    "\n",
    "\n",
    "    def forward(self, ids, mask, token_type_ids):\n",
    "\n",
    "        # import pdb;pdb.set_trace()\n",
    "        'encode bert'\n",
    "        bert_ids = pad_sequence(ids).permute(1, 0, 2).long().to(self.device)\n",
    "        bert_mask = pad_sequence(mask).permute(1, 0, 2).long().to(self.device)\n",
    "        bert_token_type_ids = pad_sequence(token_type_ids).permute(1, 0, 2).long().to(self.device)\n",
    "        batch_bert = []\n",
    "        for emb_pool, emb_mask, emb_token_type_ids in zip(bert_ids, bert_mask, bert_token_type_ids):\n",
    "            results = self.bert(emb_pool, attention_mask=emb_mask, token_type_ids=emb_token_type_ids)\n",
    "            batch_bert.append(results[1])\n",
    "\n",
    "        sent_bert = torch.stack(batch_bert, 0)\n",
    "        'GCN starts'\n",
    "        sent_bert = self.mapping(sent_bert)\n",
    "        node_number = sent_bert.shape[1]\n",
    "        \n",
    "\n",
    "        'random, using networkx'\n",
    "        if self.adj_method == 'random':\n",
    "            generated_adj = nx.dense_gnm_random_graph(node_number, node_number)\n",
    "        elif self.adj_method == 'er':\n",
    "            generated_adj = nx.erdos_renyi_graph(node_number, node_number)\n",
    "        elif self.adj_method == 'binom':\n",
    "            generated_adj = nx.binomial_graph(node_number, p=0.5)\n",
    "        elif self.adj_method == 'path':\n",
    "            generated_adj = nx.path_graph(node_number)\n",
    "        elif self.adj_method == 'complete':\n",
    "            generated_adj = nx.complete_graph(node_number)\n",
    "        elif self.adj_method == 'kk':\n",
    "            generated_adj = kronecker_generator(node_number)\n",
    "        elif self.adj_method == 'watts':\n",
    "            if node_number-1 > 0:\n",
    "                generated_adj = nx.watts_strogatz_graph(node_number, k=node_number-1, p=0.5)\n",
    "            else:\n",
    "                generated_adj = nx.watts_strogatz_graph(node_number, k=node_number, p=0.5)\n",
    "        elif self.adj_method == 'ba':\n",
    "            if node_number - 1>0:\n",
    "                generated_adj = nx.barabasi_albert_graph(node_number, m=node_number-1)\n",
    "            else:\n",
    "                generated_adj = nx.barabasi_albert_graph(node_number, m=node_number)\n",
    "        elif self.adj_method == 'bigbird':\n",
    "\n",
    "            # following are attention edges\n",
    "            attention_adj = np.zeros((node_number, node_number))\n",
    "            global_attention_step = 2\n",
    "            attention_adj[:, :global_attention_step] = 1\n",
    "            attention_adj[:global_attention_step, :] = 1\n",
    "            np.fill_diagonal(attention_adj,1) # fill diagonal with 1\n",
    "            half_sliding_window_size = 1\n",
    "            np.fill_diagonal(attention_adj[:,half_sliding_window_size:], 1)\n",
    "            np.fill_diagonal(attention_adj[half_sliding_window_size:, :], 1)\n",
    "            generated_adj = nx.from_numpy_matrix(attention_adj)\n",
    "\n",
    "        else:\n",
    "            generated_adj = nx.dense_gnm_random_graph(node_number, node_number)\n",
    "\n",
    "\n",
    "        nx_adj = from_networkx(generated_adj)\n",
    "        adj = nx_adj['edge_index'].to(self.device)\n",
    "\n",
    "        'combine starts'\n",
    "        # generated_adj2 = nx.dense_gnm_random_graph(node_number,node_number)\n",
    "        # nx_adj = from_networkx(generated_adj)\n",
    "        # adj = nx_adj['edge_index'].to(self.device)\n",
    "        # nx_adj2 = from_networkx(generated_adj2)\n",
    "        # adj2 = nx_adj2['edge_index'].to(self.device)\n",
    "        # adj = torch.cat([adj2, adj], 1)\n",
    "        'combine ends'\n",
    "\n",
    "        if self.adj_method == 'complete':\n",
    "            'complete connected'\n",
    "            adj = torch.ones((node_number,node_number)).to_sparse().indices().to(self.device)\n",
    "\n",
    "        if self.graph_type.endswith('pool'):\n",
    "            'diffpool only accepts dense adj'\n",
    "            adj_matrix = nx.adjacency_matrix(generated_adj).todense()\n",
    "            adj_matrix = torch.from_numpy(np.asarray(adj_matrix)).to(self.device)\n",
    "            adj = (adj,adj_matrix)\n",
    "        # if self.args.graph_type == 'hipool':\n",
    "\n",
    "        # sent_bert shape torch.Size([batch_size, 3, 768])\n",
    "        gcn_output_batch = []\n",
    "        for node_feature in sent_bert:\n",
    "            # import pdb;pdb.set_trace()\n",
    "\n",
    "            gcn_output=self.gcn(node_feature, adj)\n",
    "\n",
    "            'graph-level read out, summation'\n",
    "            gcn_output = torch.sum(gcn_output,0)\n",
    "            gcn_output_batch.append(gcn_output)\n",
    "\n",
    "        # import pdb;\n",
    "        # pdb.set_trace()\n",
    "\n",
    "        gcn_output_batch = torch.stack(gcn_output_batch, 0)\n",
    "\n",
    "        'GCN ends'\n",
    "\n",
    "        # import pdb;\n",
    "        # pdb.set_trace()\n",
    "        return gcn_output_batch,generated_adj # (batch_size, class_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3e1ae0b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = Hi_Bert_Classification_Model_GCN(graph_type = 'graphsage',\n",
    "                                       num_class=train_dataset.num_class,\n",
    "                                       device=device,\n",
    "                                       adj_method='bigbird').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "46728279",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fun(outputs, targets):\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    return loss(outputs, targets)\n",
    "\n",
    "def graph_feature_stats(graph_feature_list):\n",
    "    total_number = len(graph_feature_list)\n",
    "    stats = {k:[] for k in graph_feature_list[0].keys()}\n",
    "    for feature_dict in graph_feature_list:\n",
    "        for key in stats.keys():\n",
    "            stats[key].append(feature_dict[key])\n",
    "    'get mean'\n",
    "    stats_mean = {k:sum(v)/len(v) for (k,v) in stats.items()}\n",
    "    return stats_mean\n",
    "\n",
    "def get_graph_features(graph):\n",
    "    'more https://networkx.org/documentation/stable/reference/algorithms/approximation.html'\n",
    "    try:\n",
    "\n",
    "        # import pdb;pdb.set_trace()\n",
    "        node_number = nx.number_of_nodes(graph)  # int\n",
    "        centrality = nx.degree_centrality(graph) # a dictionary\n",
    "        centrality = sum(centrality.values())/node_number\n",
    "        edge_number = nx.number_of_edges(graph) # int\n",
    "        degrees = dict(graph.degree) # a dictionary\n",
    "        degrees = sum(degrees.values()) /edge_number\n",
    "        density = nx.density(graph) # a float\n",
    "        clustring_coef = nx.average_clustering(graph) # a float Compute the average clustering coefficient for the graph G.\n",
    "        closeness_centrality = nx.closeness_centrality(graph) # dict\n",
    "        closeness_centrality = sum(closeness_centrality.values())/len(closeness_centrality)\n",
    "        number_triangles = nx.triangles(graph) # dict\n",
    "        number_triangles = sum(number_triangles.values())/len(number_triangles)\n",
    "        number_clique = nx.graph_clique_number(graph) # a float Returns the number of maximal cliques in the graph.\n",
    "        number_connected_components = nx.number_connected_components(graph) # int Returns the number of connected components.\n",
    "        # avg_shortest_path_len = nx.average_shortest_path_length(graph) # float Return the average shortest path length; The average shortest path length is the sum of path lengths d(u,v) between all pairs of nodes (assuming the length is zero if v is not reachable from v) normalized by n*(n-1) where n is the number of nodes in G.\n",
    "        # diameter = nx.distance_measures.diameter(graph) # int The diameter is the maximum eccentricity.\n",
    "        return {'node_number': node_number, 'edge_number': edge_number, 'centrality': centrality, 'degrees': degrees,\n",
    "                'density': density, 'clustring_coef': clustring_coef, 'closeness_centrality': closeness_centrality,\n",
    "                'number_triangles': number_triangles, 'number_clique': number_clique,\n",
    "                'number_connected_components': number_connected_components,\n",
    "                'avg_shortest_path_len': 0, 'diameter': 0}\n",
    "    except:\n",
    "        return {'node_number': 1, 'edge_number': 1, 'centrality': 0, 'degrees': 0,\n",
    "                'density': 0, 'clustring_coef': 0, 'closeness_centrality': 0,\n",
    "                'number_triangles': 0, 'number_clique': 0,\n",
    "                'number_connected_components': 0,\n",
    "                'avg_shortest_path_len': 0, 'diameter': 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aa18cb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop_fun1(data_loader, model, optimizer, device, scheduler=None):\n",
    "    '''optimized function for Hi-BERT'''\n",
    "\n",
    "    model.train()\n",
    "    t0 = time.time()\n",
    "    losses = []\n",
    "    #import pdb;pdb.set_trace()\n",
    "\n",
    "    graph_features = []\n",
    "    for batch_idx, batch in enumerate(data_loader):\n",
    "\n",
    "        ids = [data[\"ids\"] for data in batch] # size of 8\n",
    "        mask = [data[\"mask\"] for data in batch]\n",
    "        token_type_ids = [data[\"token_type_ids\"] for data in batch]\n",
    "        targets = [data[\"targets\"] for data in batch] # length: 8\n",
    "        length = [data['len'] for data in batch] # [tensor([3]), tensor([7]), tensor([2]), tensor([4]), tensor([2]), tensor([4]), tensor([2]), tensor([3])]\n",
    "\n",
    "\n",
    "        'cat is not working for hi-bert'\n",
    "        # ids = torch.cat(ids)\n",
    "        # mask = torch.cat(mask)\n",
    "        # token_type_ids = torch.cat(token_type_ids)\n",
    "        # targets = torch.cat(targets)\n",
    "        # length = torch.cat(length)\n",
    "\n",
    "\n",
    "        # ids = ids.to(device, dtype=torch.long)\n",
    "        # mask = mask.to(device, dtype=torch.long)\n",
    "        # token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
    "        # targets = targets.to(device, dtype=torch.long)\n",
    "\n",
    "        target_labels = torch.stack([x[0] for x in targets]).long().to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # measure time\n",
    "        start = timeit.timeit()\n",
    "        outputs,adj_graph = model(ids=ids, mask=mask, token_type_ids=token_type_ids)\n",
    "        end = timeit.timeit()\n",
    "        model_time = end - start\n",
    "\n",
    "\n",
    "        loss = loss_fun(outputs, target_labels)\n",
    "        loss.backward()\n",
    "        model.float()\n",
    "        optimizer.step()\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "        losses.append(loss.item())\n",
    "        if batch_idx % 50 == 0:\n",
    "            print(\n",
    "                f\"___ batch index = {batch_idx} / {len(data_loader)} ({100*batch_idx / len(data_loader):.2f}%), loss = {np.mean(losses[-10:]):.4f}, time = {time.time()-t0:.2f} secondes ___\")\n",
    "            t0 = time.time()\n",
    "\n",
    "        graph_features.append(get_graph_features(adj_graph))\n",
    "\n",
    "\n",
    "    stats_mean = graph_feature_stats(graph_features)\n",
    "    pprint.pprint(stats_mean)\n",
    "    \n",
    "    return losses\n",
    "\n",
    "def eval_loop_fun1(data_loader, model, device):\n",
    "    model.eval()\n",
    "    fin_targets = []\n",
    "    fin_outputs = []\n",
    "    losses = []\n",
    "    for batch_idx, batch in enumerate(data_loader):\n",
    "        ids = [data[\"ids\"] for data in batch]  # size of 8\n",
    "        mask = [data[\"mask\"] for data in batch]\n",
    "        token_type_ids = [data[\"token_type_ids\"] for data in batch]\n",
    "        targets = [data[\"targets\"] for data in batch]  # length: 8\n",
    "\n",
    "        with torch.no_grad():\n",
    "            target_labels = torch.stack([x[0] for x in targets]).long().to(device)\n",
    "            outputs, _ = model(ids=ids, mask=mask, token_type_ids=token_type_ids)\n",
    "            loss = loss_fun(outputs, target_labels)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "        fin_targets.append(target_labels.cpu().detach().numpy())\n",
    "        fin_outputs.append(torch.softmax(outputs, dim=1).cpu().detach().numpy())\n",
    "    return np.concatenate(fin_outputs), np.concatenate(fin_targets), losses\n",
    "\n",
    "def evaluate(target, predicted):\n",
    "    true_label_mask = [1 if (np.argmax(x)-target[i]) ==\n",
    "                       0 else 0 for i, x in enumerate(predicted)]\n",
    "    nb_prediction = len(true_label_mask)\n",
    "    true_prediction = sum(true_label_mask)\n",
    "    false_prediction = nb_prediction-true_prediction\n",
    "    accuracy = true_prediction/nb_prediction\n",
    "    return{\n",
    "        \"accuracy\": accuracy,\n",
    "        \"nb exemple\": len(target),\n",
    "        \"true_prediction\": true_prediction,\n",
    "        \"false_prediction\": false_prediction,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "408fd0b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/smanduru/.local/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "optimizer=AdamW(model.parameters(), lr=lr)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                        num_warmup_steps = 0,\n",
    "                                        num_training_steps = num_training_steps)\n",
    "val_losses=[]\n",
    "batches_losses=[]\n",
    "val_acc=[]\n",
    "avg_running_time = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7fffd01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bc3fb001",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=============== EPOCH 1 / 20 ===============\n",
      "\n",
      "___ batch index = 0 / 500 (0.00%), loss = 2.8830, time = 5.29 secondes ___\n",
      "___ batch index = 50 / 500 (10.00%), loss = 1.3764, time = 87.36 secondes ___\n",
      "___ batch index = 100 / 500 (20.00%), loss = 1.3936, time = 86.37 secondes ___\n",
      "___ batch index = 150 / 500 (30.00%), loss = 1.3435, time = 86.27 secondes ___\n",
      "___ batch index = 200 / 500 (40.00%), loss = 1.3259, time = 85.90 secondes ___\n",
      "___ batch index = 250 / 500 (50.00%), loss = 1.2892, time = 86.19 secondes ___\n",
      "___ batch index = 300 / 500 (60.00%), loss = 1.2264, time = 86.65 secondes ___\n",
      "___ batch index = 350 / 500 (70.00%), loss = 1.2494, time = 87.19 secondes ___\n",
      "___ batch index = 400 / 500 (80.00%), loss = 1.2515, time = 86.33 secondes ___\n",
      "___ batch index = 450 / 500 (90.00%), loss = 1.1185, time = 86.60 secondes ___\n",
      "{'avg_shortest_path_len': 0.0,\n",
      " 'centrality': 0.8333333333333284,\n",
      " 'closeness_centrality': 0.7293447293447305,\n",
      " 'clustring_coef': 0.7883597883597948,\n",
      " 'degrees': 2.0,\n",
      " 'density': 0.8333333333333284,\n",
      " 'diameter': 0.0,\n",
      " 'edge_number': 30.0,\n",
      " 'node_number': 9.0,\n",
      " 'number_clique': 4.0,\n",
      " 'number_connected_components': 1.0,\n",
      " 'number_triangles': 6.333333333333368}\n",
      "\n",
      " ******** Running time this step.. 868.401504278183\n",
      "\n",
      "*** avg_loss : 1.33, time : ~14.0 min (868.40 sec) ***\n",
      "\n",
      "==> evaluation : avg_loss = 1.14, time : 57.95 sec\n",
      "\n",
      "=====>\t{'accuracy': 0.501, 'nb exemple': 1000, 'true_prediction': 501, 'false_prediction': 499}\n",
      "\t§§ model has been saved §§\n",
      "\n",
      "=============== EPOCH 2 / 20 ===============\n",
      "\n",
      "___ batch index = 0 / 500 (0.00%), loss = 1.2659, time = 1.73 secondes ___\n",
      "___ batch index = 50 / 500 (10.00%), loss = 1.1039, time = 86.48 secondes ___\n",
      "___ batch index = 100 / 500 (20.00%), loss = 1.1568, time = 86.43 secondes ___\n",
      "___ batch index = 150 / 500 (30.00%), loss = 1.0600, time = 85.81 secondes ___\n",
      "___ batch index = 200 / 500 (40.00%), loss = 1.0235, time = 86.76 secondes ___\n",
      "___ batch index = 250 / 500 (50.00%), loss = 1.1000, time = 86.99 secondes ___\n",
      "___ batch index = 300 / 500 (60.00%), loss = 1.0530, time = 85.32 secondes ___\n",
      "___ batch index = 350 / 500 (70.00%), loss = 1.0361, time = 86.15 secondes ___\n",
      "___ batch index = 400 / 500 (80.00%), loss = 1.0166, time = 85.87 secondes ___\n",
      "___ batch index = 450 / 500 (90.00%), loss = 1.0565, time = 86.38 secondes ___\n",
      "{'avg_shortest_path_len': 0.0,\n",
      " 'centrality': 0.8333333333333284,\n",
      " 'closeness_centrality': 0.7293447293447305,\n",
      " 'clustring_coef': 0.7883597883597948,\n",
      " 'degrees': 2.0,\n",
      " 'density': 0.8333333333333284,\n",
      " 'diameter': 0.0,\n",
      " 'edge_number': 30.0,\n",
      " 'node_number': 9.0,\n",
      " 'number_clique': 4.0,\n",
      " 'number_connected_components': 1.0,\n",
      " 'number_triangles': 6.333333333333368}\n",
      "\n",
      " ******** Running time this step.. 863.245335817337\n",
      "\n",
      "*** avg_loss : 1.07, time : ~14.0 min (863.25 sec) ***\n",
      "\n",
      "==> evaluation : avg_loss = 1.00, time : 57.83 sec\n",
      "\n",
      "=====>\t{'accuracy': 0.579, 'nb exemple': 1000, 'true_prediction': 579, 'false_prediction': 421}\n",
      "\t§§ model has been saved §§\n",
      "\n",
      "=============== EPOCH 3 / 20 ===============\n",
      "\n",
      "___ batch index = 0 / 500 (0.00%), loss = 0.6216, time = 1.83 secondes ___\n",
      "___ batch index = 50 / 500 (10.00%), loss = 0.8677, time = 86.41 secondes ___\n",
      "___ batch index = 100 / 500 (20.00%), loss = 0.9400, time = 86.95 secondes ___\n",
      "___ batch index = 150 / 500 (30.00%), loss = 0.7822, time = 86.38 secondes ___\n",
      "___ batch index = 200 / 500 (40.00%), loss = 0.8852, time = 86.19 secondes ___\n",
      "___ batch index = 250 / 500 (50.00%), loss = 0.8878, time = 86.33 secondes ___\n",
      "___ batch index = 300 / 500 (60.00%), loss = 0.9587, time = 85.69 secondes ___\n",
      "___ batch index = 350 / 500 (70.00%), loss = 0.8454, time = 86.82 secondes ___\n",
      "___ batch index = 400 / 500 (80.00%), loss = 0.8730, time = 85.86 secondes ___\n",
      "___ batch index = 450 / 500 (90.00%), loss = 0.9059, time = 86.38 secondes ___\n",
      "{'avg_shortest_path_len': 0.0,\n",
      " 'centrality': 0.8333333333333284,\n",
      " 'closeness_centrality': 0.7293447293447305,\n",
      " 'clustring_coef': 0.7883597883597948,\n",
      " 'degrees': 2.0,\n",
      " 'density': 0.8333333333333284,\n",
      " 'diameter': 0.0,\n",
      " 'edge_number': 30.0,\n",
      " 'node_number': 9.0,\n",
      " 'number_clique': 4.0,\n",
      " 'number_connected_components': 1.0,\n",
      " 'number_triangles': 6.333333333333368}\n",
      "\n",
      " ******** Running time this step.. 863.6022531986237\n",
      "\n",
      "*** avg_loss : 0.92, time : ~14.0 min (863.60 sec) ***\n",
      "\n",
      "==> evaluation : avg_loss = 0.99, time : 57.89 sec\n",
      "\n",
      "=====>\t{'accuracy': 0.56, 'nb exemple': 1000, 'true_prediction': 560, 'false_prediction': 440}\n",
      "\t§§ model has been saved §§\n",
      "\n",
      "=============== EPOCH 4 / 20 ===============\n",
      "\n",
      "___ batch index = 0 / 500 (0.00%), loss = 0.7440, time = 1.77 secondes ___\n",
      "___ batch index = 50 / 500 (10.00%), loss = 0.8002, time = 86.75 secondes ___\n",
      "___ batch index = 100 / 500 (20.00%), loss = 0.7688, time = 85.96 secondes ___\n",
      "___ batch index = 150 / 500 (30.00%), loss = 0.7335, time = 86.34 secondes ___\n",
      "___ batch index = 200 / 500 (40.00%), loss = 0.7463, time = 86.51 secondes ___\n",
      "___ batch index = 250 / 500 (50.00%), loss = 0.7154, time = 86.44 secondes ___\n",
      "___ batch index = 300 / 500 (60.00%), loss = 0.8408, time = 86.39 secondes ___\n",
      "___ batch index = 350 / 500 (70.00%), loss = 0.7967, time = 86.27 secondes ___\n",
      "___ batch index = 400 / 500 (80.00%), loss = 0.7524, time = 86.05 secondes ___\n",
      "___ batch index = 450 / 500 (90.00%), loss = 0.7374, time = 86.22 secondes ___\n",
      "{'avg_shortest_path_len': 0.0,\n",
      " 'centrality': 0.8333333333333284,\n",
      " 'closeness_centrality': 0.7293447293447305,\n",
      " 'clustring_coef': 0.7883597883597948,\n",
      " 'degrees': 2.0,\n",
      " 'density': 0.8333333333333284,\n",
      " 'diameter': 0.0,\n",
      " 'edge_number': 30.0,\n",
      " 'node_number': 9.0,\n",
      " 'number_clique': 4.0,\n",
      " 'number_connected_components': 1.0,\n",
      " 'number_triangles': 6.333333333333368}\n",
      "\n",
      " ******** Running time this step.. 863.4848022460938\n",
      "\n",
      "*** avg_loss : 0.80, time : ~14.0 min (863.48 sec) ***\n",
      "\n",
      "==> evaluation : avg_loss = 1.02, time : 58.04 sec\n",
      "\n",
      "=====>\t{'accuracy': 0.577, 'nb exemple': 1000, 'true_prediction': 577, 'false_prediction': 423}\n",
      "\t§§ model has been saved §§\n",
      "\n",
      "=============== EPOCH 5 / 20 ===============\n",
      "\n",
      "___ batch index = 0 / 500 (0.00%), loss = 0.5811, time = 1.75 secondes ___\n",
      "___ batch index = 50 / 500 (10.00%), loss = 0.8193, time = 86.31 secondes ___\n",
      "___ batch index = 100 / 500 (20.00%), loss = 0.7463, time = 86.15 secondes ___\n",
      "___ batch index = 150 / 500 (30.00%), loss = 0.6848, time = 86.18 secondes ___\n",
      "___ batch index = 200 / 500 (40.00%), loss = 0.7478, time = 86.93 secondes ___\n",
      "___ batch index = 250 / 500 (50.00%), loss = 0.5381, time = 86.12 secondes ___\n",
      "___ batch index = 300 / 500 (60.00%), loss = 0.6732, time = 86.24 secondes ___\n",
      "___ batch index = 350 / 500 (70.00%), loss = 0.7534, time = 86.16 secondes ___\n",
      "___ batch index = 400 / 500 (80.00%), loss = 0.6219, time = 86.29 secondes ___\n",
      "___ batch index = 450 / 500 (90.00%), loss = 0.5876, time = 86.73 secondes ___\n",
      "{'avg_shortest_path_len': 0.0,\n",
      " 'centrality': 0.8333333333333284,\n",
      " 'closeness_centrality': 0.7293447293447305,\n",
      " 'clustring_coef': 0.7883597883597948,\n",
      " 'degrees': 2.0,\n",
      " 'density': 0.8333333333333284,\n",
      " 'diameter': 0.0,\n",
      " 'edge_number': 30.0,\n",
      " 'node_number': 9.0,\n",
      " 'number_clique': 4.0,\n",
      " 'number_connected_components': 1.0,\n",
      " 'number_triangles': 6.333333333333368}\n",
      "\n",
      " ******** Running time this step.. 863.5064430236816\n",
      "\n",
      "*** avg_loss : 0.66, time : ~14.0 min (863.51 sec) ***\n",
      "\n",
      "==> evaluation : avg_loss = 1.11, time : 58.17 sec\n",
      "\n",
      "=====>\t{'accuracy': 0.574, 'nb exemple': 1000, 'true_prediction': 574, 'false_prediction': 426}\n",
      "\t§§ model has been saved §§\n",
      "\n",
      "=============== EPOCH 6 / 20 ===============\n",
      "\n",
      "___ batch index = 0 / 500 (0.00%), loss = 0.4933, time = 1.68 secondes ___\n",
      "___ batch index = 50 / 500 (10.00%), loss = 0.4384, time = 86.70 secondes ___\n",
      "___ batch index = 100 / 500 (20.00%), loss = 0.5286, time = 86.50 secondes ___\n",
      "___ batch index = 150 / 500 (30.00%), loss = 0.4272, time = 85.77 secondes ___\n",
      "___ batch index = 200 / 500 (40.00%), loss = 0.4928, time = 85.66 secondes ___\n",
      "___ batch index = 250 / 500 (50.00%), loss = 0.5482, time = 85.69 secondes ___\n",
      "___ batch index = 300 / 500 (60.00%), loss = 0.4889, time = 87.04 secondes ___\n",
      "___ batch index = 350 / 500 (70.00%), loss = 0.4767, time = 86.59 secondes ___\n",
      "___ batch index = 400 / 500 (80.00%), loss = 0.5110, time = 86.07 secondes ___\n",
      "___ batch index = 450 / 500 (90.00%), loss = 0.6370, time = 87.38 secondes ___\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'avg_shortest_path_len': 0.0,\n",
      " 'centrality': 0.8333333333333284,\n",
      " 'closeness_centrality': 0.7293447293447305,\n",
      " 'clustring_coef': 0.7883597883597948,\n",
      " 'degrees': 2.0,\n",
      " 'density': 0.8333333333333284,\n",
      " 'diameter': 0.0,\n",
      " 'edge_number': 30.0,\n",
      " 'node_number': 9.0,\n",
      " 'number_clique': 4.0,\n",
      " 'number_connected_components': 1.0,\n",
      " 'number_triangles': 6.333333333333368}\n",
      "\n",
      " ******** Running time this step.. 864.0345997810364\n",
      "\n",
      "*** avg_loss : 0.53, time : ~14.0 min (864.03 sec) ***\n",
      "\n",
      "==> evaluation : avg_loss = 1.23, time : 57.88 sec\n",
      "\n",
      "=====>\t{'accuracy': 0.566, 'nb exemple': 1000, 'true_prediction': 566, 'false_prediction': 434}\n",
      "\t§§ model has been saved §§\n",
      "\n",
      "=============== EPOCH 7 / 20 ===============\n",
      "\n",
      "___ batch index = 0 / 500 (0.00%), loss = 0.5376, time = 1.83 secondes ___\n",
      "___ batch index = 50 / 500 (10.00%), loss = 0.4511, time = 86.00 secondes ___\n",
      "___ batch index = 100 / 500 (20.00%), loss = 0.3670, time = 86.26 secondes ___\n",
      "___ batch index = 150 / 500 (30.00%), loss = 0.5090, time = 86.31 secondes ___\n",
      "___ batch index = 200 / 500 (40.00%), loss = 0.4276, time = 87.37 secondes ___\n",
      "___ batch index = 250 / 500 (50.00%), loss = 0.3730, time = 85.85 secondes ___\n",
      "___ batch index = 300 / 500 (60.00%), loss = 0.4640, time = 85.94 secondes ___\n",
      "___ batch index = 350 / 500 (70.00%), loss = 0.4541, time = 86.08 secondes ___\n",
      "___ batch index = 400 / 500 (80.00%), loss = 0.3822, time = 86.49 secondes ___\n",
      "___ batch index = 450 / 500 (90.00%), loss = 0.3563, time = 86.70 secondes ___\n",
      "{'avg_shortest_path_len': 0.0,\n",
      " 'centrality': 0.8333333333333284,\n",
      " 'closeness_centrality': 0.7293447293447305,\n",
      " 'clustring_coef': 0.7883597883597948,\n",
      " 'degrees': 2.0,\n",
      " 'density': 0.8333333333333284,\n",
      " 'diameter': 0.0,\n",
      " 'edge_number': 30.0,\n",
      " 'node_number': 9.0,\n",
      " 'number_clique': 4.0,\n",
      " 'number_connected_components': 1.0,\n",
      " 'number_triangles': 6.333333333333368}\n",
      "\n",
      " ******** Running time this step.. 863.3874197006226\n",
      "\n",
      "*** avg_loss : 0.41, time : ~14.0 min (863.39 sec) ***\n",
      "\n",
      "==> evaluation : avg_loss = 1.34, time : 57.93 sec\n",
      "\n",
      "=====>\t{'accuracy': 0.575, 'nb exemple': 1000, 'true_prediction': 575, 'false_prediction': 425}\n",
      "\t§§ model has been saved §§\n",
      "\n",
      "=============== EPOCH 8 / 20 ===============\n",
      "\n",
      "___ batch index = 0 / 500 (0.00%), loss = 0.2391, time = 1.75 secondes ___\n",
      "___ batch index = 50 / 500 (10.00%), loss = 0.2671, time = 85.97 secondes ___\n",
      "___ batch index = 100 / 500 (20.00%), loss = 0.3503, time = 86.81 secondes ___\n",
      "___ batch index = 150 / 500 (30.00%), loss = 0.3560, time = 86.14 secondes ___\n",
      "___ batch index = 200 / 500 (40.00%), loss = 0.3955, time = 86.14 secondes ___\n",
      "___ batch index = 250 / 500 (50.00%), loss = 0.2630, time = 85.75 secondes ___\n",
      "___ batch index = 300 / 500 (60.00%), loss = 0.3805, time = 86.10 secondes ___\n",
      "___ batch index = 350 / 500 (70.00%), loss = 0.3722, time = 86.50 secondes ___\n",
      "___ batch index = 400 / 500 (80.00%), loss = 0.2997, time = 86.10 secondes ___\n",
      "___ batch index = 450 / 500 (90.00%), loss = 0.4417, time = 86.69 secondes ___\n",
      "{'avg_shortest_path_len': 0.0,\n",
      " 'centrality': 0.8333333333333284,\n",
      " 'closeness_centrality': 0.7293447293447305,\n",
      " 'clustring_coef': 0.7883597883597948,\n",
      " 'degrees': 2.0,\n",
      " 'density': 0.8333333333333284,\n",
      " 'diameter': 0.0,\n",
      " 'edge_number': 30.0,\n",
      " 'node_number': 9.0,\n",
      " 'number_clique': 4.0,\n",
      " 'number_connected_components': 1.0,\n",
      " 'number_triangles': 6.333333333333368}\n",
      "\n",
      " ******** Running time this step.. 862.6178250312805\n",
      "\n",
      "*** avg_loss : 0.33, time : ~14.0 min (862.62 sec) ***\n",
      "\n",
      "==> evaluation : avg_loss = 1.54, time : 58.09 sec\n",
      "\n",
      "=====>\t{'accuracy': 0.549, 'nb exemple': 1000, 'true_prediction': 549, 'false_prediction': 451}\n",
      "\t§§ model has been saved §§\n",
      "\n",
      "=============== EPOCH 9 / 20 ===============\n",
      "\n",
      "___ batch index = 0 / 500 (0.00%), loss = 0.1042, time = 1.68 secondes ___\n",
      "___ batch index = 50 / 500 (10.00%), loss = 0.2870, time = 86.35 secondes ___\n",
      "___ batch index = 100 / 500 (20.00%), loss = 0.2860, time = 86.36 secondes ___\n",
      "___ batch index = 150 / 500 (30.00%), loss = 0.1905, time = 86.15 secondes ___\n",
      "___ batch index = 200 / 500 (40.00%), loss = 0.2128, time = 85.87 secondes ___\n",
      "___ batch index = 250 / 500 (50.00%), loss = 0.3448, time = 86.87 secondes ___\n",
      "___ batch index = 300 / 500 (60.00%), loss = 0.2578, time = 86.32 secondes ___\n",
      "___ batch index = 350 / 500 (70.00%), loss = 0.2822, time = 86.22 secondes ___\n",
      "___ batch index = 400 / 500 (80.00%), loss = 0.3358, time = 86.39 secondes ___\n",
      "___ batch index = 450 / 500 (90.00%), loss = 0.3213, time = 86.15 secondes ___\n",
      "{'avg_shortest_path_len': 0.0,\n",
      " 'centrality': 0.8333333333333284,\n",
      " 'closeness_centrality': 0.7293447293447305,\n",
      " 'clustring_coef': 0.7883597883597948,\n",
      " 'degrees': 2.0,\n",
      " 'density': 0.8333333333333284,\n",
      " 'diameter': 0.0,\n",
      " 'edge_number': 30.0,\n",
      " 'node_number': 9.0,\n",
      " 'number_clique': 4.0,\n",
      " 'number_connected_components': 1.0,\n",
      " 'number_triangles': 6.333333333333368}\n",
      "\n",
      " ******** Running time this step.. 863.3637180328369\n",
      "\n",
      "*** avg_loss : 0.25, time : ~14.0 min (863.36 sec) ***\n",
      "\n",
      "==> evaluation : avg_loss = 1.68, time : 57.84 sec\n",
      "\n",
      "=====>\t{'accuracy': 0.559, 'nb exemple': 1000, 'true_prediction': 559, 'false_prediction': 441}\n",
      "\t§§ model has been saved §§\n",
      "\n",
      "=============== EPOCH 10 / 20 ===============\n",
      "\n",
      "___ batch index = 0 / 500 (0.00%), loss = 0.0747, time = 1.73 secondes ___\n",
      "___ batch index = 50 / 500 (10.00%), loss = 0.2699, time = 86.01 secondes ___\n",
      "___ batch index = 100 / 500 (20.00%), loss = 0.2013, time = 86.18 secondes ___\n",
      "___ batch index = 150 / 500 (30.00%), loss = 0.1838, time = 86.44 secondes ___\n",
      "___ batch index = 200 / 500 (40.00%), loss = 0.2174, time = 86.30 secondes ___\n",
      "___ batch index = 250 / 500 (50.00%), loss = 0.3046, time = 86.56 secondes ___\n",
      "___ batch index = 300 / 500 (60.00%), loss = 0.1164, time = 86.27 secondes ___\n",
      "___ batch index = 350 / 500 (70.00%), loss = 0.2151, time = 86.42 secondes ___\n",
      "___ batch index = 400 / 500 (80.00%), loss = 0.2245, time = 86.24 secondes ___\n",
      "___ batch index = 450 / 500 (90.00%), loss = 0.2190, time = 85.27 secondes ___\n",
      "{'avg_shortest_path_len': 0.0,\n",
      " 'centrality': 0.8333333333333284,\n",
      " 'closeness_centrality': 0.7293447293447305,\n",
      " 'clustring_coef': 0.7883597883597948,\n",
      " 'degrees': 2.0,\n",
      " 'density': 0.8333333333333284,\n",
      " 'diameter': 0.0,\n",
      " 'edge_number': 30.0,\n",
      " 'node_number': 9.0,\n",
      " 'number_clique': 4.0,\n",
      " 'number_connected_components': 1.0,\n",
      " 'number_triangles': 6.333333333333368}\n",
      "\n",
      " ******** Running time this step.. 861.6333742141724\n",
      "\n",
      "*** avg_loss : 0.22, time : ~14.0 min (861.63 sec) ***\n",
      "\n",
      "==> evaluation : avg_loss = 1.79, time : 57.82 sec\n",
      "\n",
      "=====>\t{'accuracy': 0.563, 'nb exemple': 1000, 'true_prediction': 563, 'false_prediction': 437}\n",
      "\t§§ model has been saved §§\n",
      "\n",
      "=============== EPOCH 11 / 20 ===============\n",
      "\n",
      "___ batch index = 0 / 500 (0.00%), loss = 0.3002, time = 1.70 secondes ___\n",
      "___ batch index = 50 / 500 (10.00%), loss = 0.1494, time = 85.44 secondes ___\n",
      "___ batch index = 100 / 500 (20.00%), loss = 0.1987, time = 86.68 secondes ___\n",
      "___ batch index = 150 / 500 (30.00%), loss = 0.1935, time = 87.85 secondes ___\n",
      "___ batch index = 200 / 500 (40.00%), loss = 0.1858, time = 85.91 secondes ___\n",
      "___ batch index = 250 / 500 (50.00%), loss = 0.2343, time = 107.75 secondes ___\n",
      "___ batch index = 300 / 500 (60.00%), loss = 0.1936, time = 92.25 secondes ___\n",
      "___ batch index = 350 / 500 (70.00%), loss = 0.1351, time = 85.98 secondes ___\n",
      "___ batch index = 400 / 500 (80.00%), loss = 0.1726, time = 85.35 secondes ___\n",
      "___ batch index = 450 / 500 (90.00%), loss = 0.1702, time = 86.06 secondes ___\n",
      "{'avg_shortest_path_len': 0.0,\n",
      " 'centrality': 0.8333333333333284,\n",
      " 'closeness_centrality': 0.7293447293447305,\n",
      " 'clustring_coef': 0.7883597883597948,\n",
      " 'degrees': 2.0,\n",
      " 'density': 0.8333333333333284,\n",
      " 'diameter': 0.0,\n",
      " 'edge_number': 30.0,\n",
      " 'node_number': 9.0,\n",
      " 'number_clique': 4.0,\n",
      " 'number_connected_components': 1.0,\n",
      " 'number_triangles': 6.333333333333368}\n",
      "\n",
      " ******** Running time this step.. 889.4651675224304\n",
      "\n",
      "*** avg_loss : 0.18, time : ~14.0 min (889.47 sec) ***\n",
      "\n",
      "==> evaluation : avg_loss = 1.85, time : 57.79 sec\n",
      "\n",
      "=====>\t{'accuracy': 0.553, 'nb exemple': 1000, 'true_prediction': 553, 'false_prediction': 447}\n",
      "\t§§ model has been saved §§\n",
      "\n",
      "=============== EPOCH 12 / 20 ===============\n",
      "\n",
      "___ batch index = 0 / 500 (0.00%), loss = 0.0860, time = 1.64 secondes ___\n",
      "___ batch index = 50 / 500 (10.00%), loss = 0.2137, time = 85.69 secondes ___\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "___ batch index = 100 / 500 (20.00%), loss = 0.1374, time = 86.02 secondes ___\n",
      "___ batch index = 150 / 500 (30.00%), loss = 0.1332, time = 85.58 secondes ___\n",
      "___ batch index = 200 / 500 (40.00%), loss = 0.1520, time = 86.70 secondes ___\n",
      "___ batch index = 250 / 500 (50.00%), loss = 0.1025, time = 86.56 secondes ___\n",
      "___ batch index = 300 / 500 (60.00%), loss = 0.1209, time = 85.86 secondes ___\n",
      "___ batch index = 350 / 500 (70.00%), loss = 0.2191, time = 85.08 secondes ___\n",
      "___ batch index = 400 / 500 (80.00%), loss = 0.1108, time = 86.22 secondes ___\n",
      "___ batch index = 450 / 500 (90.00%), loss = 0.2019, time = 85.43 secondes ___\n",
      "{'avg_shortest_path_len': 0.0,\n",
      " 'centrality': 0.8333333333333284,\n",
      " 'closeness_centrality': 0.7293447293447305,\n",
      " 'clustring_coef': 0.7883597883597948,\n",
      " 'degrees': 2.0,\n",
      " 'density': 0.8333333333333284,\n",
      " 'diameter': 0.0,\n",
      " 'edge_number': 30.0,\n",
      " 'node_number': 9.0,\n",
      " 'number_clique': 4.0,\n",
      " 'number_connected_components': 1.0,\n",
      " 'number_triangles': 6.333333333333368}\n",
      "\n",
      " ******** Running time this step.. 859.0801072120667\n",
      "\n",
      "*** avg_loss : 0.15, time : ~14.0 min (859.08 sec) ***\n",
      "\n",
      "==> evaluation : avg_loss = 2.08, time : 57.82 sec\n",
      "\n",
      "=====>\t{'accuracy': 0.539, 'nb exemple': 1000, 'true_prediction': 539, 'false_prediction': 461}\n",
      "\t§§ model has been saved §§\n",
      "\n",
      "=============== EPOCH 13 / 20 ===============\n",
      "\n",
      "___ batch index = 0 / 500 (0.00%), loss = 0.0163, time = 1.64 secondes ___\n",
      "___ batch index = 50 / 500 (10.00%), loss = 0.2287, time = 86.01 secondes ___\n",
      "___ batch index = 100 / 500 (20.00%), loss = 0.1450, time = 86.09 secondes ___\n",
      "___ batch index = 150 / 500 (30.00%), loss = 0.0993, time = 86.39 secondes ___\n",
      "___ batch index = 200 / 500 (40.00%), loss = 0.0895, time = 85.95 secondes ___\n",
      "___ batch index = 250 / 500 (50.00%), loss = 0.1393, time = 85.28 secondes ___\n",
      "___ batch index = 300 / 500 (60.00%), loss = 0.1669, time = 85.37 secondes ___\n",
      "___ batch index = 350 / 500 (70.00%), loss = 0.1828, time = 85.65 secondes ___\n",
      "___ batch index = 400 / 500 (80.00%), loss = 0.2091, time = 86.10 secondes ___\n",
      "___ batch index = 450 / 500 (90.00%), loss = 0.1269, time = 86.05 secondes ___\n",
      "{'avg_shortest_path_len': 0.0,\n",
      " 'centrality': 0.8333333333333284,\n",
      " 'closeness_centrality': 0.7293447293447305,\n",
      " 'clustring_coef': 0.7883597883597948,\n",
      " 'degrees': 2.0,\n",
      " 'density': 0.8333333333333284,\n",
      " 'diameter': 0.0,\n",
      " 'edge_number': 30.0,\n",
      " 'node_number': 9.0,\n",
      " 'number_clique': 4.0,\n",
      " 'number_connected_components': 1.0,\n",
      " 'number_triangles': 6.333333333333368}\n",
      "\n",
      " ******** Running time this step.. 859.0661623477936\n",
      "\n",
      "*** avg_loss : 0.14, time : ~14.0 min (859.07 sec) ***\n",
      "\n",
      "==> evaluation : avg_loss = 2.23, time : 57.83 sec\n",
      "\n",
      "=====>\t{'accuracy': 0.541, 'nb exemple': 1000, 'true_prediction': 541, 'false_prediction': 459}\n",
      "\t§§ model has been saved §§\n",
      "\n",
      "=============== EPOCH 14 / 20 ===============\n",
      "\n",
      "___ batch index = 0 / 500 (0.00%), loss = 0.0464, time = 1.66 secondes ___\n",
      "___ batch index = 50 / 500 (10.00%), loss = 0.0769, time = 86.00 secondes ___\n",
      "___ batch index = 100 / 500 (20.00%), loss = 0.0843, time = 85.14 secondes ___\n",
      "___ batch index = 150 / 500 (30.00%), loss = 0.1180, time = 85.52 secondes ___\n",
      "___ batch index = 200 / 500 (40.00%), loss = 0.1164, time = 86.08 secondes ___\n",
      "___ batch index = 250 / 500 (50.00%), loss = 0.2512, time = 85.51 secondes ___\n",
      "___ batch index = 300 / 500 (60.00%), loss = 0.0905, time = 86.44 secondes ___\n",
      "___ batch index = 350 / 500 (70.00%), loss = 0.1014, time = 86.24 secondes ___\n",
      "___ batch index = 400 / 500 (80.00%), loss = 0.1203, time = 86.35 secondes ___\n",
      "___ batch index = 450 / 500 (90.00%), loss = 0.0668, time = 85.80 secondes ___\n",
      "{'avg_shortest_path_len': 0.0,\n",
      " 'centrality': 0.8333333333333284,\n",
      " 'closeness_centrality': 0.7293447293447305,\n",
      " 'clustring_coef': 0.7883597883597948,\n",
      " 'degrees': 2.0,\n",
      " 'density': 0.8333333333333284,\n",
      " 'diameter': 0.0,\n",
      " 'edge_number': 30.0,\n",
      " 'node_number': 9.0,\n",
      " 'number_clique': 4.0,\n",
      " 'number_connected_components': 1.0,\n",
      " 'number_triangles': 6.333333333333368}\n",
      "\n",
      " ******** Running time this step.. 858.7503945827484\n",
      "\n",
      "*** avg_loss : 0.12, time : ~14.0 min (858.75 sec) ***\n",
      "\n",
      "==> evaluation : avg_loss = 2.46, time : 57.81 sec\n",
      "\n",
      "=====>\t{'accuracy': 0.543, 'nb exemple': 1000, 'true_prediction': 543, 'false_prediction': 457}\n",
      "\t§§ model has been saved §§\n",
      "\n",
      "=============== EPOCH 15 / 20 ===============\n",
      "\n",
      "___ batch index = 0 / 500 (0.00%), loss = 0.2989, time = 1.79 secondes ___\n",
      "___ batch index = 50 / 500 (10.00%), loss = 0.1458, time = 85.68 secondes ___\n",
      "___ batch index = 100 / 500 (20.00%), loss = 0.0881, time = 86.33 secondes ___\n",
      "___ batch index = 150 / 500 (30.00%), loss = 0.0909, time = 86.03 secondes ___\n",
      "___ batch index = 200 / 500 (40.00%), loss = 0.0748, time = 85.45 secondes ___\n",
      "___ batch index = 250 / 500 (50.00%), loss = 0.1471, time = 86.12 secondes ___\n",
      "___ batch index = 300 / 500 (60.00%), loss = 0.1054, time = 85.96 secondes ___\n",
      "___ batch index = 350 / 500 (70.00%), loss = 0.1330, time = 85.67 secondes ___\n",
      "___ batch index = 400 / 500 (80.00%), loss = 0.1190, time = 86.39 secondes ___\n",
      "___ batch index = 450 / 500 (90.00%), loss = 0.1830, time = 85.41 secondes ___\n",
      "{'avg_shortest_path_len': 0.0,\n",
      " 'centrality': 0.8333333333333284,\n",
      " 'closeness_centrality': 0.7293447293447305,\n",
      " 'clustring_coef': 0.7883597883597948,\n",
      " 'degrees': 2.0,\n",
      " 'density': 0.8333333333333284,\n",
      " 'diameter': 0.0,\n",
      " 'edge_number': 30.0,\n",
      " 'node_number': 9.0,\n",
      " 'number_clique': 4.0,\n",
      " 'number_connected_components': 1.0,\n",
      " 'number_triangles': 6.333333333333368}\n",
      "\n",
      " ******** Running time this step.. 858.8832061290741\n",
      "\n",
      "*** avg_loss : 0.11, time : ~14.0 min (858.88 sec) ***\n",
      "\n",
      "==> evaluation : avg_loss = 2.21, time : 57.79 sec\n",
      "\n",
      "=====>\t{'accuracy': 0.57, 'nb exemple': 1000, 'true_prediction': 570, 'false_prediction': 430}\n",
      "\t§§ model has been saved §§\n",
      "\n",
      "=============== EPOCH 16 / 20 ===============\n",
      "\n",
      "___ batch index = 0 / 500 (0.00%), loss = 0.0389, time = 1.76 secondes ___\n",
      "___ batch index = 50 / 500 (10.00%), loss = 0.0860, time = 85.72 secondes ___\n",
      "___ batch index = 100 / 500 (20.00%), loss = 0.0795, time = 86.28 secondes ___\n",
      "___ batch index = 150 / 500 (30.00%), loss = 0.1554, time = 86.25 secondes ___\n",
      "___ batch index = 200 / 500 (40.00%), loss = 0.0951, time = 85.65 secondes ___\n",
      "___ batch index = 250 / 500 (50.00%), loss = 0.0657, time = 86.36 secondes ___\n",
      "___ batch index = 300 / 500 (60.00%), loss = 0.0892, time = 85.57 secondes ___\n",
      "___ batch index = 350 / 500 (70.00%), loss = 0.1297, time = 85.72 secondes ___\n",
      "___ batch index = 400 / 500 (80.00%), loss = 0.0610, time = 85.34 secondes ___\n",
      "___ batch index = 450 / 500 (90.00%), loss = 0.0521, time = 85.74 secondes ___\n",
      "{'avg_shortest_path_len': 0.0,\n",
      " 'centrality': 0.8333333333333284,\n",
      " 'closeness_centrality': 0.7293447293447305,\n",
      " 'clustring_coef': 0.7883597883597948,\n",
      " 'degrees': 2.0,\n",
      " 'density': 0.8333333333333284,\n",
      " 'diameter': 0.0,\n",
      " 'edge_number': 30.0,\n",
      " 'node_number': 9.0,\n",
      " 'number_clique': 4.0,\n",
      " 'number_connected_components': 1.0,\n",
      " 'number_triangles': 6.333333333333368}\n",
      "\n",
      " ******** Running time this step.. 858.7779412269592\n",
      "\n",
      "*** avg_loss : 0.09, time : ~14.0 min (858.78 sec) ***\n",
      "\n",
      "==> evaluation : avg_loss = 2.33, time : 57.80 sec\n",
      "\n",
      "=====>\t{'accuracy': 0.58, 'nb exemple': 1000, 'true_prediction': 580, 'false_prediction': 420}\n",
      "\t§§ model has been saved §§\n",
      "\n",
      "=============== EPOCH 17 / 20 ===============\n",
      "\n",
      "___ batch index = 0 / 500 (0.00%), loss = 0.0150, time = 1.74 secondes ___\n",
      "___ batch index = 50 / 500 (10.00%), loss = 0.0443, time = 85.82 secondes ___\n",
      "___ batch index = 100 / 500 (20.00%), loss = 0.0642, time = 85.60 secondes ___\n",
      "___ batch index = 150 / 500 (30.00%), loss = 0.0649, time = 85.43 secondes ___\n",
      "___ batch index = 200 / 500 (40.00%), loss = 0.0470, time = 86.23 secondes ___\n",
      "___ batch index = 250 / 500 (50.00%), loss = 0.0795, time = 86.47 secondes ___\n",
      "___ batch index = 300 / 500 (60.00%), loss = 0.0908, time = 86.10 secondes ___\n",
      "___ batch index = 350 / 500 (70.00%), loss = 0.0404, time = 85.53 secondes ___\n",
      "___ batch index = 400 / 500 (80.00%), loss = 0.0903, time = 85.92 secondes ___\n",
      "___ batch index = 450 / 500 (90.00%), loss = 0.0254, time = 85.65 secondes ___\n",
      "{'avg_shortest_path_len': 0.0,\n",
      " 'centrality': 0.8333333333333284,\n",
      " 'closeness_centrality': 0.7293447293447305,\n",
      " 'clustring_coef': 0.7883597883597948,\n",
      " 'degrees': 2.0,\n",
      " 'density': 0.8333333333333284,\n",
      " 'diameter': 0.0,\n",
      " 'edge_number': 30.0,\n",
      " 'node_number': 9.0,\n",
      " 'number_clique': 4.0,\n",
      " 'number_connected_components': 1.0,\n",
      " 'number_triangles': 6.333333333333368}\n",
      "\n",
      " ******** Running time this step.. 858.7228424549103\n",
      "\n",
      "*** avg_loss : 0.07, time : ~14.0 min (858.72 sec) ***\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> evaluation : avg_loss = 2.59, time : 57.79 sec\n",
      "\n",
      "=====>\t{'accuracy': 0.571, 'nb exemple': 1000, 'true_prediction': 571, 'false_prediction': 429}\n",
      "\t§§ model has been saved §§\n",
      "\n",
      "=============== EPOCH 18 / 20 ===============\n",
      "\n",
      "___ batch index = 0 / 500 (0.00%), loss = 0.1411, time = 1.80 secondes ___\n",
      "___ batch index = 50 / 500 (10.00%), loss = 0.0869, time = 85.92 secondes ___\n",
      "___ batch index = 100 / 500 (20.00%), loss = 0.0786, time = 85.74 secondes ___\n",
      "___ batch index = 150 / 500 (30.00%), loss = 0.0611, time = 86.14 secondes ___\n",
      "___ batch index = 200 / 500 (40.00%), loss = 0.0567, time = 85.27 secondes ___\n",
      "___ batch index = 250 / 500 (50.00%), loss = 0.0594, time = 85.54 secondes ___\n",
      "___ batch index = 300 / 500 (60.00%), loss = 0.1250, time = 86.06 secondes ___\n",
      "___ batch index = 350 / 500 (70.00%), loss = 0.0270, time = 86.33 secondes ___\n",
      "___ batch index = 400 / 500 (80.00%), loss = 0.0665, time = 86.06 secondes ___\n",
      "___ batch index = 450 / 500 (90.00%), loss = 0.0814, time = 85.74 secondes ___\n",
      "{'avg_shortest_path_len': 0.0,\n",
      " 'centrality': 0.8333333333333284,\n",
      " 'closeness_centrality': 0.7293447293447305,\n",
      " 'clustring_coef': 0.7883597883597948,\n",
      " 'degrees': 2.0,\n",
      " 'density': 0.8333333333333284,\n",
      " 'diameter': 0.0,\n",
      " 'edge_number': 30.0,\n",
      " 'node_number': 9.0,\n",
      " 'number_clique': 4.0,\n",
      " 'number_connected_components': 1.0,\n",
      " 'number_triangles': 6.333333333333368}\n",
      "\n",
      " ******** Running time this step.. 858.7083580493927\n",
      "\n",
      "*** avg_loss : 0.07, time : ~14.0 min (858.71 sec) ***\n",
      "\n",
      "==> evaluation : avg_loss = 2.68, time : 57.79 sec\n",
      "\n",
      "=====>\t{'accuracy': 0.559, 'nb exemple': 1000, 'true_prediction': 559, 'false_prediction': 441}\n",
      "\t§§ model has been saved §§\n",
      "\n",
      "=============== EPOCH 19 / 20 ===============\n",
      "\n",
      "___ batch index = 0 / 500 (0.00%), loss = 0.0258, time = 1.90 secondes ___\n",
      "___ batch index = 50 / 500 (10.00%), loss = 0.0831, time = 85.83 secondes ___\n",
      "___ batch index = 100 / 500 (20.00%), loss = 0.1206, time = 85.42 secondes ___\n",
      "___ batch index = 150 / 500 (30.00%), loss = 0.0422, time = 85.83 secondes ___\n",
      "___ batch index = 200 / 500 (40.00%), loss = 0.1425, time = 85.44 secondes ___\n",
      "___ batch index = 250 / 500 (50.00%), loss = 0.0917, time = 86.23 secondes ___\n",
      "___ batch index = 300 / 500 (60.00%), loss = 0.0777, time = 85.67 secondes ___\n",
      "___ batch index = 350 / 500 (70.00%), loss = 0.0491, time = 85.95 secondes ___\n",
      "___ batch index = 400 / 500 (80.00%), loss = 0.0645, time = 85.80 secondes ___\n",
      "___ batch index = 450 / 500 (90.00%), loss = 0.0584, time = 85.71 secondes ___\n",
      "{'avg_shortest_path_len': 0.0,\n",
      " 'centrality': 0.8333333333333284,\n",
      " 'closeness_centrality': 0.7293447293447305,\n",
      " 'clustring_coef': 0.7883597883597948,\n",
      " 'degrees': 2.0,\n",
      " 'density': 0.8333333333333284,\n",
      " 'diameter': 0.0,\n",
      " 'edge_number': 30.0,\n",
      " 'node_number': 9.0,\n",
      " 'number_clique': 4.0,\n",
      " 'number_connected_components': 1.0,\n",
      " 'number_triangles': 6.333333333333368}\n",
      "\n",
      " ******** Running time this step.. 858.8746552467346\n",
      "\n",
      "*** avg_loss : 0.08, time : ~14.0 min (858.87 sec) ***\n",
      "\n",
      "==> evaluation : avg_loss = 2.76, time : 57.83 sec\n",
      "\n",
      "=====>\t{'accuracy': 0.551, 'nb exemple': 1000, 'true_prediction': 551, 'false_prediction': 449}\n",
      "\t§§ model has been saved §§\n",
      "\n",
      "=============== EPOCH 20 / 20 ===============\n",
      "\n",
      "___ batch index = 0 / 500 (0.00%), loss = 0.1690, time = 1.69 secondes ___\n",
      "___ batch index = 50 / 500 (10.00%), loss = 0.0436, time = 86.08 secondes ___\n",
      "___ batch index = 100 / 500 (20.00%), loss = 0.0405, time = 86.32 secondes ___\n",
      "___ batch index = 150 / 500 (30.00%), loss = 0.0351, time = 85.87 secondes ___\n",
      "___ batch index = 200 / 500 (40.00%), loss = 0.0320, time = 86.25 secondes ___\n",
      "___ batch index = 250 / 500 (50.00%), loss = 0.0239, time = 85.77 secondes ___\n",
      "___ batch index = 300 / 500 (60.00%), loss = 0.0621, time = 85.53 secondes ___\n",
      "___ batch index = 350 / 500 (70.00%), loss = 0.0811, time = 85.74 secondes ___\n",
      "___ batch index = 400 / 500 (80.00%), loss = 0.1255, time = 86.00 secondes ___\n",
      "___ batch index = 450 / 500 (90.00%), loss = 0.0424, time = 85.43 secondes ___\n",
      "{'avg_shortest_path_len': 0.0,\n",
      " 'centrality': 0.8333333333333284,\n",
      " 'closeness_centrality': 0.7293447293447305,\n",
      " 'clustring_coef': 0.7883597883597948,\n",
      " 'degrees': 2.0,\n",
      " 'density': 0.8333333333333284,\n",
      " 'diameter': 0.0,\n",
      " 'edge_number': 30.0,\n",
      " 'node_number': 9.0,\n",
      " 'number_clique': 4.0,\n",
      " 'number_connected_components': 1.0,\n",
      " 'number_triangles': 6.333333333333368}\n",
      "\n",
      " ******** Running time this step.. 858.7812101840973\n",
      "\n",
      "*** avg_loss : 0.07, time : ~14.0 min (858.78 sec) ***\n",
      "\n",
      "==> evaluation : avg_loss = 2.96, time : 57.75 sec\n",
      "\n",
      "=====>\t{'accuracy': 0.523, 'nb exemple': 1000, 'true_prediction': 523, 'false_prediction': 477}\n",
      "\t§§ model has been saved §§\n",
      "\n",
      "\n",
      "$$$$ average running time per epoch (sec).. 862.8198683738708\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCH):\n",
    "\n",
    "    t0 = time.time()\n",
    "    print(f\"\\n=============== EPOCH {epoch+1} / {EPOCH} ===============\\n\")\n",
    "    batches_losses_tmp=train_loop_fun1(train_loader, model, optimizer, device)\n",
    "    epoch_loss=np.mean(batches_losses_tmp)\n",
    "    print (\"\\n ******** Running time this step..\",time.time()-t0)\n",
    "    avg_running_time.append(time.time()-t0)\n",
    "    print(f\"\\n*** avg_loss : {epoch_loss:.2f}, time : ~{(time.time()-t0)//60} min ({time.time()-t0:.2f} sec) ***\\n\")\n",
    "    t1=time.time()\n",
    "    output, target, val_losses_tmp=eval_loop_fun1(valid_loader, model, device)\n",
    "    print(f\"==> evaluation : avg_loss = {np.mean(val_losses_tmp):.2f}, time : {time.time()-t1:.2f} sec\\n\")\n",
    "    tmp_evaluate=evaluate(target.reshape(-1), output)\n",
    "    print(f\"=====>\\t{tmp_evaluate}\")\n",
    "    val_acc.append(tmp_evaluate['accuracy'])\n",
    "    val_losses.append(val_losses_tmp)\n",
    "    batches_losses.append(batches_losses_tmp)\n",
    "    print(\"\\t§§ model has been saved §§\")\n",
    "\n",
    "print(\"\\n\\n$$$$ average running time per epoch (sec)..\", sum(avg_running_time)/len(avg_running_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7ba38dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, '/scratch/smanduru/NLP/project/saved_models' + '/graphSage_20eps.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a860d77f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ORC)",
   "language": "python",
   "name": "sys_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
