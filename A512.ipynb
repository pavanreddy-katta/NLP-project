{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f2cd8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import networkx as nx\n",
    "from torch_geometric.utils import erdos_renyi_graph, to_networkx, from_networkx\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "import math\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import transformers\n",
    "from transformers import RobertaTokenizer, BertTokenizer, RobertaModel, BertModel, AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "import pprint\n",
    "import time\n",
    "import timeit\n",
    "\n",
    "from graphModels import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5caf19d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 1024\n",
    "CHUNK_LEN = 200\n",
    "OVERLAP_LEN = int(CHUNK_LEN/2)\n",
    "\n",
    "TRAIN_BATCH_SIZE = 32\n",
    "EPOCH = 1\n",
    "lr=1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82b1daa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocess:\n",
    "    \n",
    "    def __init__(self, trainPath, devPath, testPath):\n",
    "        self.train_df = pd.read_csv(trainPath, sep = '\\t', header=0)\n",
    "        self.train_df['review'] = self.train_df['headline'].str.cat(self.train_df['text'], sep=' ')\n",
    "        \n",
    "        self.valid_df = pd.read_csv(devPath, sep = '\\t', header=0)\n",
    "        self.valid_df['review'] = self.valid_df['headline'].str.cat(self.valid_df['text'], sep=' ')\n",
    "        \n",
    "        self.test_df = pd.read_csv(testPath, sep = '\\t', header=0)\n",
    "        self.test_df['review'] = self.test_df['headline'].str.cat(self.test_df['text'], sep=' ')\n",
    "        \n",
    "    \n",
    "    def clean_text(self, sentence):\n",
    "        cleaned_sentence = re.sub(r'[^a-zA-Z0-9\\s]', ' ', sentence)\n",
    "        cleaned_sentence = re.sub(r'\\s+', ' ', cleaned_sentence).strip()\n",
    "        return cleaned_sentence.lower()\n",
    "        \n",
    "    def get_clean(self):\n",
    "        self.train_df['cleaned_text'] = self.train_df['review'].apply(self.clean_text)\n",
    "        self.valid_df['cleaned_text'] = self.valid_df['review'].apply(self.clean_text)\n",
    "        self.test_df['cleaned_text'] = self.test_df['review'].apply(self.clean_text)\n",
    "        return self.train_df[['cleaned_text', 'label']], self.valid_df[['cleaned_text', 'label']], self.test_df[['cleaned_text', 'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dea279e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pr = Preprocess(\"/scratch/smanduru/NLP/project/data/amazon_512/amazon-books-512-train.tsv\",\n",
    "               \"/scratch/smanduru/NLP/project/data/amazon_512/amazon-books-512-dev.tsv\",\n",
    "               \"/scratch/smanduru/NLP/project/data/amazon_512/amazon-books-512-test.tsv\")\n",
    "\n",
    "train, valid, test = pr.get_clean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90041d68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((52628, 2), (52626, 2), (421015, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid.shape, test.shape, train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da6d8df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# roberta_tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79ba3e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, tokenizer, max_len, df, chunk_len=200, overlap_len=50, approach=\"all\", max_size_dataset=None, min_len=249):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.overlap_len = overlap_len\n",
    "        self.chunk_len = chunk_len\n",
    "        self.approach = approach\n",
    "        self.min_len = min_len\n",
    "        self.max_size_dataset = max_size_dataset\n",
    "        self.data, self.label = self.process_data(df)\n",
    "        \n",
    "    def process_data(self, df):\n",
    "        self.num_class = len(set(df['label'].values))\n",
    "        return df['cleaned_text'].values, df['label'].values\n",
    "    \n",
    "    def long_terms_tokenizer(self, data_tokenize, targets):\n",
    "        long_terms_token = []\n",
    "        input_ids_list = []\n",
    "        attention_mask_list = []\n",
    "        token_type_ids_list = []\n",
    "        targets_list = []\n",
    "\n",
    "        previous_input_ids = data_tokenize[\"input_ids\"].reshape(-1)\n",
    "        previous_attention_mask = data_tokenize[\"attention_mask\"].reshape(-1)\n",
    "        previous_token_type_ids = data_tokenize[\"token_type_ids\"].reshape(-1)\n",
    "        remain = data_tokenize.get(\"overflowing_tokens\")\n",
    "        targets = torch.tensor(targets, dtype=torch.int)\n",
    "        \n",
    "        start_token = torch.tensor([101], dtype=torch.long)\n",
    "        end_token = torch.tensor([102], dtype=torch.long)\n",
    "\n",
    "        total_token = len(previous_input_ids) -2 # remove head 101, tail 102\n",
    "        stride = self.overlap_len - 2\n",
    "        number_chunks = math.floor(total_token/stride)\n",
    "\n",
    "        mask_list = torch.ones(self.chunk_len, dtype=torch.long)\n",
    "        type_list = torch.zeros(self.chunk_len, dtype=torch.long)\n",
    "        \n",
    "        for current in range(number_chunks-1):\n",
    "            input_ids = previous_input_ids[current*stride:current*stride+self.chunk_len-2]\n",
    "            input_ids = torch.cat((start_token, input_ids, end_token))\n",
    "            input_ids_list.append(input_ids)\n",
    "\n",
    "            attention_mask_list.append(mask_list)\n",
    "            token_type_ids_list.append(type_list)\n",
    "            targets_list.append(targets)\n",
    "\n",
    "        if len(input_ids_list) == 0:\n",
    "            input_ids = torch.ones(self.chunk_len-2, dtype=torch.long)\n",
    "            input_ids = torch.cat((start_token, input_ids, end_token))\n",
    "            input_ids_list.append(input_ids)\n",
    "\n",
    "            attention_mask_list.append(mask_list)\n",
    "            token_type_ids_list.append(type_list)\n",
    "            targets_list.append(targets)\n",
    "\n",
    "        return({\n",
    "            'ids': input_ids_list,\n",
    "            'mask': attention_mask_list,\n",
    "            'token_type_ids': token_type_ids_list,\n",
    "            'targets': targets_list,\n",
    "            'len': [torch.tensor(len(targets_list), dtype=torch.long)]\n",
    "        })\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        review = str(self.data[idx])\n",
    "        targets = int(self.label[idx])\n",
    "        data = self.tokenizer.encode_plus(\n",
    "            review,\n",
    "            max_length=self.max_len,\n",
    "            pad_to_max_length=False,\n",
    "            add_special_tokens=True,\n",
    "            return_attention_mask=True,\n",
    "            return_token_type_ids=True,\n",
    "            return_overflowing_tokens=True,\n",
    "            return_tensors='pt')\n",
    "        \n",
    "        long_token = self.long_terms_tokenizer(data, targets)\n",
    "        return long_token\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.label.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3648515a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(\n",
    "    tokenizer = bert_tokenizer,\n",
    "    max_len = MAX_LEN,\n",
    "    chunk_len = CHUNK_LEN,\n",
    "    overlap_len = OVERLAP_LEN,\n",
    "    df = train)\n",
    "\n",
    "\n",
    "valid_dataset = CustomDataset(\n",
    "    tokenizer = bert_tokenizer,\n",
    "    max_len = MAX_LEN,\n",
    "    chunk_len = CHUNK_LEN,\n",
    "    overlap_len = OVERLAP_LEN,\n",
    "    df = valid)\n",
    "\n",
    "    \n",
    "test_dataset = CustomDataset(\n",
    "    tokenizer = bert_tokenizer,\n",
    "    max_len = MAX_LEN,\n",
    "    chunk_len = CHUNK_LEN,\n",
    "    overlap_len = OVERLAP_LEN,\n",
    "    df = test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f61d2c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_collate1(batches):\n",
    "    return [{key: torch.stack(value) for key, value in batch.items()} for batch in batches]\n",
    "\n",
    "train_loader = DataLoader(train_dataset,\n",
    "                          batch_size = TRAIN_BATCH_SIZE, \n",
    "                          shuffle = True, \n",
    "                          collate_fn = my_collate1)\n",
    "\n",
    "valid_loader = DataLoader(valid_dataset,\n",
    "                          batch_size = 32, \n",
    "                          shuffle = False, \n",
    "                          collate_fn = my_collate1)\n",
    "\n",
    "test_loader = DataLoader(test_dataset,\n",
    "                          batch_size = 32, \n",
    "                          shuffle = False, \n",
    "                          collate_fn = my_collate1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e2ce60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking Data Loading\n",
    "\n",
    "# for batch_idx, batch in enumerate(test_loader):\n",
    "#     ids = batch[batch_idx]['ids']\n",
    "#     mask = batch[batch_idx]['mask']\n",
    "#     token_type_ids = batch[batch_idx]['token_type_ids']\n",
    "#     targets = batch[batch_idx]['targets']\n",
    "#     length = batch[batch_idx]['len']\n",
    "\n",
    "#     # Now you can print or process these items as needed\n",
    "#     print(f\"Batch {batch_idx + 1} IDs: {ids}\")\n",
    "#     print(f\"Batch {batch_idx + 1} Mask: {mask}\")\n",
    "#     print(f\"Batch {batch_idx + 1} Token Type IDs: {token_type_ids}\")\n",
    "#     print(f\"Batch {batch_idx + 1} Targets: {targets}\")\n",
    "#     print(f\"Batch {batch_idx + 1} Length: {length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb30f2ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8057ad20",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_training_steps = int(len(train_dataset) / TRAIN_BATCH_SIZE * EPOCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d00aae18",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hi_Bert_Classification_Model_GCN(nn.Module):\n",
    "    \n",
    "    \"\"\" A Model for bert fine tuning, put an lstm on top of BERT encoding \"\"\"\n",
    "\n",
    "    def __init__(self, graph_type, num_class, device, adj_method, pooling_method='mean'):\n",
    "        super(Hi_Bert_Classification_Model_GCN, self).__init__()\n",
    "        self.graph_type = graph_type\n",
    "        self.bert_path = 'bert-base-uncased'\n",
    "        self.bert = transformers.BertModel.from_pretrained(self.bert_path)\n",
    "        \n",
    "        # self.roberta = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "\n",
    "        self.lstm_layer_number = 2\n",
    "        'default 128 and 32'\n",
    "        self.lstm_hidden_size = 128\n",
    "        self.hidden_dim = 32\n",
    "        \n",
    "        # self.bert_lstm = nn.Linear(768, self.lstm_hidden_size)\n",
    "        self.device = device\n",
    "        self.pooling_method=pooling_method\n",
    "\n",
    "        self.mapping = nn.Linear(768, self.lstm_hidden_size).to(device)\n",
    "\n",
    "        'start GCN'\n",
    "        if self.graph_type == 'gcn':\n",
    "            self.gcn = GCN(input_dim=self.lstm_hidden_size, hidden_dim=32, output_dim=num_class).to(device)\n",
    "        elif self.graph_type == 'gat':\n",
    "            self.gcn = GAT(input_dim=self.lstm_hidden_size, hidden_dim=32, output_dim=num_class).to(device)\n",
    "        elif self.graph_type == 'graphsage':\n",
    "            self.gcn = GraphSAGE(input_dim=self.lstm_hidden_size, hidden_dim=32, output_dim=num_class).to(device)\n",
    "        elif self.graph_type == 'linear':\n",
    "            self.gcn = LinearFirst(input_dim=self.lstm_hidden_size, hidden_dim=32, output_dim=num_class).to(device)\n",
    "        elif self.graph_type == 'rank':\n",
    "            self.gcn = SimpleRank(input_dim=self.lstm_hidden_size, hidden_dim=32, output_dim=num_class).to(device)\n",
    "        elif self.graph_type == 'diffpool':\n",
    "            self.gcn = DiffPool(self.device,max_nodes=10,input_dim=self.lstm_hidden_size, hidden_dim=32, output_dim=num_class).to(device)\n",
    "        elif self.graph_type == 'hipool':\n",
    "            self.gcn = HiPool(self.device,input_dim=self.lstm_hidden_size, hidden_dim=32, output_dim=num_class).to(device)\n",
    "            \n",
    "        self.adj_method = adj_method\n",
    "\n",
    "\n",
    "    def forward(self, ids, mask, token_type_ids):\n",
    "\n",
    "        # import pdb;pdb.set_trace()\n",
    "        'encode bert'\n",
    "        bert_ids = pad_sequence(ids).permute(1, 0, 2).long().to(self.device)\n",
    "        bert_mask = pad_sequence(mask).permute(1, 0, 2).long().to(self.device)\n",
    "        bert_token_type_ids = pad_sequence(token_type_ids).permute(1, 0, 2).long().to(self.device)\n",
    "        batch_bert = []\n",
    "        for emb_pool, emb_mask, emb_token_type_ids in zip(bert_ids, bert_mask, bert_token_type_ids):\n",
    "            results = self.bert(emb_pool, attention_mask=emb_mask, token_type_ids=emb_token_type_ids)\n",
    "            batch_bert.append(results[1])\n",
    "\n",
    "        sent_bert = torch.stack(batch_bert, 0)\n",
    "        'GCN starts'\n",
    "        sent_bert = self.mapping(sent_bert)\n",
    "        node_number = sent_bert.shape[1]\n",
    "        \n",
    "\n",
    "        'random, using networkx'\n",
    "        if self.adj_method == 'random':\n",
    "            generated_adj = nx.dense_gnm_random_graph(node_number, node_number)\n",
    "        elif self.adj_method == 'er':\n",
    "            generated_adj = nx.erdos_renyi_graph(node_number, node_number)\n",
    "        elif self.adj_method == 'binom':\n",
    "            generated_adj = nx.binomial_graph(node_number, p=0.5)\n",
    "        elif self.adj_method == 'path':\n",
    "            generated_adj = nx.path_graph(node_number)\n",
    "        elif self.adj_method == 'complete':\n",
    "            generated_adj = nx.complete_graph(node_number)\n",
    "        elif self.adj_method == 'kk':\n",
    "            generated_adj = kronecker_generator(node_number)\n",
    "        elif self.adj_method == 'watts':\n",
    "            if node_number-1 > 0:\n",
    "                generated_adj = nx.watts_strogatz_graph(node_number, k=node_number-1, p=0.5)\n",
    "            else:\n",
    "                generated_adj = nx.watts_strogatz_graph(node_number, k=node_number, p=0.5)\n",
    "        elif self.adj_method == 'ba':\n",
    "            if node_number - 1>0:\n",
    "                generated_adj = nx.barabasi_albert_graph(node_number, m=node_number-1)\n",
    "            else:\n",
    "                generated_adj = nx.barabasi_albert_graph(node_number, m=node_number)\n",
    "        elif self.adj_method == 'bigbird':\n",
    "\n",
    "            # following are attention edges\n",
    "            attention_adj = np.zeros((node_number, node_number))\n",
    "            global_attention_step = 2\n",
    "            attention_adj[:, :global_attention_step] = 1\n",
    "            attention_adj[:global_attention_step, :] = 1\n",
    "            np.fill_diagonal(attention_adj,1) # fill diagonal with 1\n",
    "            half_sliding_window_size = 1\n",
    "            np.fill_diagonal(attention_adj[:,half_sliding_window_size:], 1)\n",
    "            np.fill_diagonal(attention_adj[half_sliding_window_size:, :], 1)\n",
    "            generated_adj = nx.from_numpy_matrix(attention_adj)\n",
    "\n",
    "        else:\n",
    "            generated_adj = nx.dense_gnm_random_graph(node_number, node_number)\n",
    "\n",
    "\n",
    "        nx_adj = from_networkx(generated_adj)\n",
    "        adj = nx_adj['edge_index'].to(self.device)\n",
    "\n",
    "        'combine starts'\n",
    "        # generated_adj2 = nx.dense_gnm_random_graph(node_number,node_number)\n",
    "        # nx_adj = from_networkx(generated_adj)\n",
    "        # adj = nx_adj['edge_index'].to(self.device)\n",
    "        # nx_adj2 = from_networkx(generated_adj2)\n",
    "        # adj2 = nx_adj2['edge_index'].to(self.device)\n",
    "        # adj = torch.cat([adj2, adj], 1)\n",
    "        'combine ends'\n",
    "\n",
    "        if self.adj_method == 'complete':\n",
    "            'complete connected'\n",
    "            adj = torch.ones((node_number,node_number)).to_sparse().indices().to(self.device)\n",
    "\n",
    "        if self.graph_type.endswith('pool'):\n",
    "            'diffpool only accepts dense adj'\n",
    "            adj_matrix = nx.adjacency_matrix(generated_adj).todense()\n",
    "            adj_matrix = torch.from_numpy(np.asarray(adj_matrix)).to(self.device)\n",
    "            adj = (adj,adj_matrix)\n",
    "        # if self.args.graph_type == 'hipool':\n",
    "\n",
    "        # sent_bert shape torch.Size([batch_size, 3, 768])\n",
    "        gcn_output_batch = []\n",
    "        for node_feature in sent_bert:\n",
    "            # import pdb;pdb.set_trace()\n",
    "\n",
    "            gcn_output=self.gcn(node_feature, adj)\n",
    "\n",
    "            'graph-level read out, summation'\n",
    "            gcn_output = torch.sum(gcn_output,0)\n",
    "            gcn_output_batch.append(gcn_output)\n",
    "\n",
    "        # import pdb;\n",
    "        # pdb.set_trace()\n",
    "\n",
    "        gcn_output_batch = torch.stack(gcn_output_batch, 0)\n",
    "\n",
    "        'GCN ends'\n",
    "\n",
    "        # import pdb;\n",
    "        # pdb.set_trace()\n",
    "        return gcn_output_batch,generated_adj # (batch_size, class_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3e1ae0b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = Hi_Bert_Classification_Model_GCN(graph_type = 'graphsage',\n",
    "                                       num_class=train_dataset.num_class,\n",
    "                                       device=device,\n",
    "                                       adj_method='bigbird').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "46728279",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fun(outputs, targets):\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    return loss(outputs, targets)\n",
    "\n",
    "def graph_feature_stats(graph_feature_list):\n",
    "    total_number = len(graph_feature_list)\n",
    "    stats = {k:[] for k in graph_feature_list[0].keys()}\n",
    "    for feature_dict in graph_feature_list:\n",
    "        for key in stats.keys():\n",
    "            stats[key].append(feature_dict[key])\n",
    "    'get mean'\n",
    "    stats_mean = {k:sum(v)/len(v) for (k,v) in stats.items()}\n",
    "    return stats_mean\n",
    "\n",
    "def get_graph_features(graph):\n",
    "    'more https://networkx.org/documentation/stable/reference/algorithms/approximation.html'\n",
    "    try:\n",
    "\n",
    "        # import pdb;pdb.set_trace()\n",
    "        node_number = nx.number_of_nodes(graph)  # int\n",
    "        centrality = nx.degree_centrality(graph) # a dictionary\n",
    "        centrality = sum(centrality.values())/node_number\n",
    "        edge_number = nx.number_of_edges(graph) # int\n",
    "        degrees = dict(graph.degree) # a dictionary\n",
    "        degrees = sum(degrees.values()) /edge_number\n",
    "        density = nx.density(graph) # a float\n",
    "        clustring_coef = nx.average_clustering(graph) # a float Compute the average clustering coefficient for the graph G.\n",
    "        closeness_centrality = nx.closeness_centrality(graph) # dict\n",
    "        closeness_centrality = sum(closeness_centrality.values())/len(closeness_centrality)\n",
    "        number_triangles = nx.triangles(graph) # dict\n",
    "        number_triangles = sum(number_triangles.values())/len(number_triangles)\n",
    "        number_clique = nx.graph_clique_number(graph) # a float Returns the number of maximal cliques in the graph.\n",
    "        number_connected_components = nx.number_connected_components(graph) # int Returns the number of connected components.\n",
    "        # avg_shortest_path_len = nx.average_shortest_path_length(graph) # float Return the average shortest path length; The average shortest path length is the sum of path lengths d(u,v) between all pairs of nodes (assuming the length is zero if v is not reachable from v) normalized by n*(n-1) where n is the number of nodes in G.\n",
    "        # diameter = nx.distance_measures.diameter(graph) # int The diameter is the maximum eccentricity.\n",
    "        return {'node_number': node_number, 'edge_number': edge_number, 'centrality': centrality, 'degrees': degrees,\n",
    "                'density': density, 'clustring_coef': clustring_coef, 'closeness_centrality': closeness_centrality,\n",
    "                'number_triangles': number_triangles, 'number_clique': number_clique,\n",
    "                'number_connected_components': number_connected_components,\n",
    "                'avg_shortest_path_len': 0, 'diameter': 0}\n",
    "    except:\n",
    "        return {'node_number': 1, 'edge_number': 1, 'centrality': 0, 'degrees': 0,\n",
    "                'density': 0, 'clustring_coef': 0, 'closeness_centrality': 0,\n",
    "                'number_triangles': 0, 'number_clique': 0,\n",
    "                'number_connected_components': 0,\n",
    "                'avg_shortest_path_len': 0, 'diameter': 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aa18cb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop_fun1(data_loader, model, optimizer, device, scheduler=None):\n",
    "    '''optimized function for Hi-BERT'''\n",
    "\n",
    "    model.train()\n",
    "    t0 = time.time()\n",
    "    losses = []\n",
    "    #import pdb;pdb.set_trace()\n",
    "\n",
    "    graph_features = []\n",
    "    for batch_idx, batch in enumerate(data_loader):\n",
    "\n",
    "        ids = [data[\"ids\"] for data in batch] # size of 8\n",
    "        mask = [data[\"mask\"] for data in batch]\n",
    "        token_type_ids = [data[\"token_type_ids\"] for data in batch]\n",
    "        targets = [data[\"targets\"] for data in batch] # length: 8\n",
    "        length = [data['len'] for data in batch] # [tensor([3]), tensor([7]), tensor([2]), tensor([4]), tensor([2]), tensor([4]), tensor([2]), tensor([3])]\n",
    "\n",
    "\n",
    "        'cat is not working for hi-bert'\n",
    "        # ids = torch.cat(ids)\n",
    "        # mask = torch.cat(mask)\n",
    "        # token_type_ids = torch.cat(token_type_ids)\n",
    "        # targets = torch.cat(targets)\n",
    "        # length = torch.cat(length)\n",
    "\n",
    "\n",
    "        # ids = ids.to(device, dtype=torch.long)\n",
    "        # mask = mask.to(device, dtype=torch.long)\n",
    "        # token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
    "        # targets = targets.to(device, dtype=torch.long)\n",
    "\n",
    "        target_labels = torch.stack([x[0] for x in targets]).long().to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # measure time\n",
    "        start = timeit.timeit()\n",
    "        outputs,adj_graph = model(ids=ids, mask=mask, token_type_ids=token_type_ids)\n",
    "        end = timeit.timeit()\n",
    "        model_time = end - start\n",
    "\n",
    "\n",
    "        loss = loss_fun(outputs, target_labels)\n",
    "        loss.backward()\n",
    "        model.float()\n",
    "        optimizer.step()\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "        losses.append(loss.item())\n",
    "        if batch_idx % 50 == 0:\n",
    "            print(\n",
    "                f\"___ batch index = {batch_idx} / {len(data_loader)} ({100*batch_idx / len(data_loader):.2f}%), loss = {np.mean(losses[-10:]):.4f}, time = {time.time()-t0:.2f} secondes ___\")\n",
    "            t0 = time.time()\n",
    "\n",
    "        graph_features.append(get_graph_features(adj_graph))\n",
    "\n",
    "\n",
    "    stats_mean = graph_feature_stats(graph_features)\n",
    "    pprint.pprint(stats_mean)\n",
    "    \n",
    "    return losses\n",
    "\n",
    "def eval_loop_fun1(data_loader, model, device):\n",
    "    model.eval()\n",
    "    fin_targets = []\n",
    "    fin_outputs = []\n",
    "    losses = []\n",
    "    for batch_idx, batch in enumerate(data_loader):\n",
    "        ids = [data[\"ids\"] for data in batch]  # size of 8\n",
    "        mask = [data[\"mask\"] for data in batch]\n",
    "        token_type_ids = [data[\"token_type_ids\"] for data in batch]\n",
    "        targets = [data[\"targets\"] for data in batch]  # length: 8\n",
    "\n",
    "        with torch.no_grad():\n",
    "            target_labels = torch.stack([x[0] for x in targets]).long().to(device)\n",
    "            outputs, _ = model(ids=ids, mask=mask, token_type_ids=token_type_ids)\n",
    "            loss = loss_fun(outputs, target_labels)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "        fin_targets.append(target_labels.cpu().detach().numpy())\n",
    "        fin_outputs.append(torch.softmax(outputs, dim=1).cpu().detach().numpy())\n",
    "    return np.concatenate(fin_outputs), np.concatenate(fin_targets), losses\n",
    "\n",
    "def evaluate(target, predicted):\n",
    "    true_label_mask = [1 if (np.argmax(x)-target[i]) ==\n",
    "                       0 else 0 for i, x in enumerate(predicted)]\n",
    "    nb_prediction = len(true_label_mask)\n",
    "    true_prediction = sum(true_label_mask)\n",
    "    false_prediction = nb_prediction-true_prediction\n",
    "    accuracy = true_prediction/nb_prediction\n",
    "    return{\n",
    "        \"accuracy\": accuracy,\n",
    "        \"nb exemple\": len(target),\n",
    "        \"true_prediction\": true_prediction,\n",
    "        \"false_prediction\": false_prediction,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "408fd0b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/smanduru/.local/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "optimizer=AdamW(model.parameters(), lr=lr)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                        num_warmup_steps = 0,\n",
    "                                        num_training_steps = num_training_steps)\n",
    "val_losses=[]\n",
    "batches_losses=[]\n",
    "val_acc=[]\n",
    "avg_running_time = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7fffd01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3fb001",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=============== EPOCH 1 / 1 ===============\n",
      "\n",
      "___ batch index = 0 / 13157 (0.00%), loss = 3.4173, time = 4.82 secondes ___\n",
      "___ batch index = 50 / 13157 (0.38%), loss = 1.3500, time = 135.31 secondes ___\n",
      "___ batch index = 100 / 13157 (0.76%), loss = 1.2990, time = 135.43 secondes ___\n",
      "___ batch index = 150 / 13157 (1.14%), loss = 1.2851, time = 135.92 secondes ___\n",
      "___ batch index = 200 / 13157 (1.52%), loss = 1.1795, time = 135.06 secondes ___\n",
      "___ batch index = 250 / 13157 (1.90%), loss = 1.2422, time = 134.52 secondes ___\n",
      "___ batch index = 300 / 13157 (2.28%), loss = 1.0553, time = 135.44 secondes ___\n",
      "___ batch index = 350 / 13157 (2.66%), loss = 1.1177, time = 135.40 secondes ___\n",
      "___ batch index = 400 / 13157 (3.04%), loss = 1.0795, time = 134.92 secondes ___\n",
      "___ batch index = 450 / 13157 (3.42%), loss = 1.1177, time = 135.12 secondes ___\n",
      "___ batch index = 500 / 13157 (3.80%), loss = 1.0607, time = 134.17 secondes ___\n",
      "___ batch index = 550 / 13157 (4.18%), loss = 0.9405, time = 135.40 secondes ___\n",
      "___ batch index = 600 / 13157 (4.56%), loss = 0.9165, time = 134.61 secondes ___\n",
      "___ batch index = 650 / 13157 (4.94%), loss = 1.0631, time = 135.29 secondes ___\n",
      "___ batch index = 700 / 13157 (5.32%), loss = 0.8730, time = 135.50 secondes ___\n",
      "___ batch index = 750 / 13157 (5.70%), loss = 0.9571, time = 135.25 secondes ___\n",
      "___ batch index = 800 / 13157 (6.08%), loss = 0.9844, time = 135.00 secondes ___\n",
      "___ batch index = 850 / 13157 (6.46%), loss = 0.9579, time = 135.24 secondes ___\n",
      "___ batch index = 900 / 13157 (6.84%), loss = 0.9242, time = 134.91 secondes ___\n",
      "___ batch index = 950 / 13157 (7.22%), loss = 0.9781, time = 135.46 secondes ___\n",
      "___ batch index = 1000 / 13157 (7.60%), loss = 0.8949, time = 135.14 secondes ___\n",
      "___ batch index = 1050 / 13157 (7.98%), loss = 0.8797, time = 135.54 secondes ___\n",
      "___ batch index = 1100 / 13157 (8.36%), loss = 0.9441, time = 135.61 secondes ___\n",
      "___ batch index = 1150 / 13157 (8.74%), loss = 0.8830, time = 134.92 secondes ___\n",
      "___ batch index = 1200 / 13157 (9.12%), loss = 0.9848, time = 135.29 secondes ___\n",
      "___ batch index = 1250 / 13157 (9.50%), loss = 0.8435, time = 135.43 secondes ___\n",
      "___ batch index = 1300 / 13157 (9.88%), loss = 0.8342, time = 135.32 secondes ___\n",
      "___ batch index = 1350 / 13157 (10.26%), loss = 0.9147, time = 135.23 secondes ___\n",
      "___ batch index = 1400 / 13157 (10.64%), loss = 0.8456, time = 135.29 secondes ___\n",
      "___ batch index = 1450 / 13157 (11.02%), loss = 0.7595, time = 134.48 secondes ___\n",
      "___ batch index = 1500 / 13157 (11.40%), loss = 0.8722, time = 135.12 secondes ___\n",
      "___ batch index = 1550 / 13157 (11.78%), loss = 0.8966, time = 134.82 secondes ___\n",
      "___ batch index = 1600 / 13157 (12.16%), loss = 0.8708, time = 135.01 secondes ___\n",
      "___ batch index = 1650 / 13157 (12.54%), loss = 0.9669, time = 135.06 secondes ___\n",
      "___ batch index = 1700 / 13157 (12.92%), loss = 0.9376, time = 134.97 secondes ___\n",
      "___ batch index = 1750 / 13157 (13.30%), loss = 0.9377, time = 135.15 secondes ___\n",
      "___ batch index = 1800 / 13157 (13.68%), loss = 0.9906, time = 135.32 secondes ___\n",
      "___ batch index = 1850 / 13157 (14.06%), loss = 0.8480, time = 137.83 secondes ___\n",
      "___ batch index = 1900 / 13157 (14.44%), loss = 0.7942, time = 135.19 secondes ___\n",
      "___ batch index = 1950 / 13157 (14.82%), loss = 0.7943, time = 135.14 secondes ___\n",
      "___ batch index = 2000 / 13157 (15.20%), loss = 0.8449, time = 135.36 secondes ___\n",
      "___ batch index = 2050 / 13157 (15.58%), loss = 0.8572, time = 134.89 secondes ___\n",
      "___ batch index = 2100 / 13157 (15.96%), loss = 0.8726, time = 135.41 secondes ___\n",
      "___ batch index = 2150 / 13157 (16.34%), loss = 0.8729, time = 134.66 secondes ___\n",
      "___ batch index = 2200 / 13157 (16.72%), loss = 0.9146, time = 135.33 secondes ___\n",
      "___ batch index = 2250 / 13157 (17.10%), loss = 0.8562, time = 135.67 secondes ___\n",
      "___ batch index = 2300 / 13157 (17.48%), loss = 0.8955, time = 134.68 secondes ___\n",
      "___ batch index = 2350 / 13157 (17.86%), loss = 0.8694, time = 135.07 secondes ___\n",
      "___ batch index = 2400 / 13157 (18.24%), loss = 0.8517, time = 135.16 secondes ___\n",
      "___ batch index = 2450 / 13157 (18.62%), loss = 0.8109, time = 135.05 secondes ___\n",
      "___ batch index = 2500 / 13157 (19.00%), loss = 0.8974, time = 134.69 secondes ___\n",
      "___ batch index = 2550 / 13157 (19.38%), loss = 0.8721, time = 135.08 secondes ___\n",
      "___ batch index = 2600 / 13157 (19.76%), loss = 0.8148, time = 135.13 secondes ___\n",
      "___ batch index = 2650 / 13157 (20.14%), loss = 0.8013, time = 134.61 secondes ___\n",
      "___ batch index = 2700 / 13157 (20.52%), loss = 0.9042, time = 135.05 secondes ___\n",
      "___ batch index = 2750 / 13157 (20.90%), loss = 0.8262, time = 134.78 secondes ___\n",
      "___ batch index = 2800 / 13157 (21.28%), loss = 0.9210, time = 135.18 secondes ___\n",
      "___ batch index = 2850 / 13157 (21.66%), loss = 0.7669, time = 135.02 secondes ___\n",
      "___ batch index = 2900 / 13157 (22.04%), loss = 0.7473, time = 134.97 secondes ___\n",
      "___ batch index = 2950 / 13157 (22.42%), loss = 0.8761, time = 135.11 secondes ___\n",
      "___ batch index = 3000 / 13157 (22.80%), loss = 0.8228, time = 134.83 secondes ___\n",
      "___ batch index = 3050 / 13157 (23.18%), loss = 0.8362, time = 135.16 secondes ___\n",
      "___ batch index = 3100 / 13157 (23.56%), loss = 0.8204, time = 134.93 secondes ___\n",
      "___ batch index = 3150 / 13157 (23.94%), loss = 0.7201, time = 134.88 secondes ___\n",
      "___ batch index = 3200 / 13157 (24.32%), loss = 0.8110, time = 139.43 secondes ___\n",
      "___ batch index = 3250 / 13157 (24.70%), loss = 0.8499, time = 135.21 secondes ___\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCH):\n",
    "\n",
    "    t0 = time.time()\n",
    "    print(f\"\\n=============== EPOCH {epoch+1} / {EPOCH} ===============\\n\")\n",
    "    batches_losses_tmp=train_loop_fun1(train_loader, model, optimizer, device)\n",
    "    epoch_loss=np.mean(batches_losses_tmp)\n",
    "    print (\"\\n ******** Running time this step..\",time.time()-t0)\n",
    "    avg_running_time.append(time.time()-t0)\n",
    "    print(f\"\\n*** avg_loss : {epoch_loss:.2f}, time : ~{(time.time()-t0)//60} min ({time.time()-t0:.2f} sec) ***\\n\")\n",
    "    t1=time.time()\n",
    "    output, target, val_losses_tmp=eval_loop_fun1(valid_loader, model, device)\n",
    "    print(f\"==> evaluation : avg_loss = {np.mean(val_losses_tmp):.2f}, time : {time.time()-t1:.2f} sec\\n\")\n",
    "    tmp_evaluate=evaluate(target.reshape(-1), output)\n",
    "    print(f\"=====>\\t{tmp_evaluate}\")\n",
    "    val_acc.append(tmp_evaluate['accuracy'])\n",
    "    val_losses.append(val_losses_tmp)\n",
    "    batches_losses.append(batches_losses_tmp)\n",
    "    print(\"\\t§§ model has been saved §§\")\n",
    "\n",
    "print(\"\\n\\n$$$$ average running time per epoch (sec)..\", sum(avg_running_time)/len(avg_running_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba38dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, '/scratch/smanduru/NLP/project/saved_models/A512' + '/graphSage_20eps.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a860d77f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ORC)",
   "language": "python",
   "name": "sys_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
