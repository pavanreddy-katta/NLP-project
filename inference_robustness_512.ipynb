{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b6f917b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import networkx as nx\n",
    "\n",
    "from graphModels import *\n",
    "import torch_geometric\n",
    "from torch_geometric.utils import erdos_renyi_graph, to_networkx, from_networkx\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "import math\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import transformers\n",
    "from transformers import RobertaTokenizer, BertTokenizer, RobertaModel, BertModel, AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "import pprint\n",
    "import time\n",
    "import timeit\n",
    "\n",
    "\n",
    "import random\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8a6dbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hi_Bert_Classification_Model_GCN(nn.Module):\n",
    "    \n",
    "    \"\"\" A Model for bert fine tuning, put an lstm on top of BERT encoding \"\"\"\n",
    "\n",
    "    def __init__(self, graph_type, num_class, device, adj_method, pooling_method='mean'):\n",
    "        super(Hi_Bert_Classification_Model_GCN, self).__init__()\n",
    "        self.graph_type = graph_type\n",
    "        self.bert_path = 'bert-base-uncased'\n",
    "        self.bert = transformers.BertModel.from_pretrained(self.bert_path)\n",
    "        \n",
    "        # self.roberta = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "\n",
    "        self.lstm_layer_number = 2\n",
    "        'default 128 and 32'\n",
    "        self.lstm_hidden_size = 128\n",
    "        self.hidden_dim = 32\n",
    "        \n",
    "        # self.bert_lstm = nn.Linear(768, self.lstm_hidden_size)\n",
    "        self.device = device\n",
    "        self.pooling_method=pooling_method\n",
    "\n",
    "        self.mapping = nn.Linear(768, self.lstm_hidden_size).to(device)\n",
    "\n",
    "        'start GCN'\n",
    "        if self.graph_type == 'gcn':\n",
    "            self.gcn = GCN(input_dim=self.lstm_hidden_size, hidden_dim=32, output_dim=num_class).to(device)\n",
    "        elif self.graph_type == 'gat':\n",
    "            self.gcn = GAT(input_dim=self.lstm_hidden_size, hidden_dim=32, output_dim=num_class).to(device)\n",
    "        elif self.graph_type == 'graphsage':\n",
    "            self.gcn = GraphSAGE(input_dim=self.lstm_hidden_size, hidden_dim=32, output_dim=num_class).to(device)\n",
    "        elif self.graph_type == 'linear':\n",
    "            self.gcn = LinearFirst(input_dim=self.lstm_hidden_size, hidden_dim=32, output_dim=num_class).to(device)\n",
    "        elif self.graph_type == 'rank':\n",
    "            self.gcn = SimpleRank(input_dim=self.lstm_hidden_size, hidden_dim=32, output_dim=num_class).to(device)\n",
    "        elif self.graph_type == 'diffpool':\n",
    "            self.gcn = DiffPool(self.device,max_nodes=10,input_dim=self.lstm_hidden_size, hidden_dim=32, output_dim=num_class).to(device)\n",
    "        elif self.graph_type == 'hipool':\n",
    "            self.gcn = HiPool(self.device,input_dim=self.lstm_hidden_size, hidden_dim=32, output_dim=num_class).to(device)\n",
    "            \n",
    "        self.adj_method = adj_method\n",
    "\n",
    "\n",
    "    def forward(self, ids, mask, token_type_ids):\n",
    "\n",
    "        # import pdb;pdb.set_trace()\n",
    "        'encode bert'\n",
    "        bert_ids = pad_sequence(ids).permute(1, 0, 2).long().to(self.device)\n",
    "        bert_mask = pad_sequence(mask).permute(1, 0, 2).long().to(self.device)\n",
    "        bert_token_type_ids = pad_sequence(token_type_ids).permute(1, 0, 2).long().to(self.device)\n",
    "        batch_bert = []\n",
    "        for emb_pool, emb_mask, emb_token_type_ids in zip(bert_ids, bert_mask, bert_token_type_ids):\n",
    "            results = self.bert(emb_pool, attention_mask=emb_mask, token_type_ids=emb_token_type_ids)\n",
    "            batch_bert.append(results[1])\n",
    "\n",
    "        sent_bert = torch.stack(batch_bert, 0)\n",
    "        'GCN starts'\n",
    "        sent_bert = self.mapping(sent_bert)\n",
    "        node_number = sent_bert.shape[1]\n",
    "        \n",
    "\n",
    "        'random, using networkx'\n",
    "        if self.adj_method == 'random':\n",
    "            generated_adj = nx.dense_gnm_random_graph(node_number, node_number)\n",
    "        elif self.adj_method == 'er':\n",
    "            generated_adj = nx.erdos_renyi_graph(node_number, node_number)\n",
    "        elif self.adj_method == 'binom':\n",
    "            generated_adj = nx.binomial_graph(node_number, p=0.5)\n",
    "        elif self.adj_method == 'path':\n",
    "            generated_adj = nx.path_graph(node_number)\n",
    "        elif self.adj_method == 'complete':\n",
    "            generated_adj = nx.complete_graph(node_number)\n",
    "        elif self.adj_method == 'kk':\n",
    "            generated_adj = kronecker_generator(node_number)\n",
    "        elif self.adj_method == 'watts':\n",
    "            if node_number-1 > 0:\n",
    "                generated_adj = nx.watts_strogatz_graph(node_number, k=node_number-1, p=0.5)\n",
    "            else:\n",
    "                generated_adj = nx.watts_strogatz_graph(node_number, k=node_number, p=0.5)\n",
    "        elif self.adj_method == 'ba':\n",
    "            if node_number - 1>0:\n",
    "                generated_adj = nx.barabasi_albert_graph(node_number, m=node_number-1)\n",
    "            else:\n",
    "                generated_adj = nx.barabasi_albert_graph(node_number, m=node_number)\n",
    "        elif self.adj_method == 'bigbird':\n",
    "\n",
    "            # following are attention edges\n",
    "            attention_adj = np.zeros((node_number, node_number))\n",
    "            global_attention_step = 2\n",
    "            attention_adj[:, :global_attention_step] = 1\n",
    "            attention_adj[:global_attention_step, :] = 1\n",
    "            np.fill_diagonal(attention_adj,1) # fill diagonal with 1\n",
    "            half_sliding_window_size = 1\n",
    "            np.fill_diagonal(attention_adj[:,half_sliding_window_size:], 1)\n",
    "            np.fill_diagonal(attention_adj[half_sliding_window_size:, :], 1)\n",
    "            generated_adj = nx.from_numpy_matrix(attention_adj)\n",
    "\n",
    "        else:\n",
    "            generated_adj = nx.dense_gnm_random_graph(node_number, node_number)\n",
    "\n",
    "\n",
    "        nx_adj = from_networkx(generated_adj)\n",
    "        adj = nx_adj['edge_index'].to(self.device)\n",
    "\n",
    "        'combine starts'\n",
    "        # generated_adj2 = nx.dense_gnm_random_graph(node_number,node_number)\n",
    "        # nx_adj = from_networkx(generated_adj)\n",
    "        # adj = nx_adj['edge_index'].to(self.device)\n",
    "        # nx_adj2 = from_networkx(generated_adj2)\n",
    "        # adj2 = nx_adj2['edge_index'].to(self.device)\n",
    "        # adj = torch.cat([adj2, adj], 1)\n",
    "        'combine ends'\n",
    "\n",
    "        if self.adj_method == 'complete':\n",
    "            'complete connected'\n",
    "            adj = torch.ones((node_number,node_number)).to_sparse().indices().to(self.device)\n",
    "\n",
    "        if self.graph_type.endswith('pool'):\n",
    "            'diffpool only accepts dense adj'\n",
    "            adj_matrix = nx.adjacency_matrix(generated_adj).todense()\n",
    "            adj_matrix = torch.from_numpy(np.asarray(adj_matrix)).to(self.device)\n",
    "            adj = (adj,adj_matrix)\n",
    "        # if self.args.graph_type == 'hipool':\n",
    "\n",
    "        # sent_bert shape torch.Size([batch_size, 3, 768])\n",
    "        gcn_output_batch = []\n",
    "        for node_feature in sent_bert:\n",
    "            # import pdb;pdb.set_trace()\n",
    "\n",
    "            gcn_output=self.gcn(node_feature, adj)\n",
    "\n",
    "            'graph-level read out, summation'\n",
    "            gcn_output = torch.sum(gcn_output,0)\n",
    "            gcn_output_batch.append(gcn_output)\n",
    "\n",
    "        # import pdb;\n",
    "        # pdb.set_trace()\n",
    "\n",
    "        gcn_output_batch = torch.stack(gcn_output_batch, 0)\n",
    "\n",
    "        'GCN ends'\n",
    "\n",
    "        # import pdb;\n",
    "        # pdb.set_trace()\n",
    "        return gcn_output_batch,generated_adj # (batch_size, class_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e34ff9c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Hi_Bert_Classification_Model_GCN(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (mapping): Linear(in_features=768, out_features=128, bias=True)\n",
       "  (gcn): HiPool(\n",
       "    (conv1): DenseGCNConv(128, 32)\n",
       "    (conv2): DenseGCNConv(32, 32)\n",
       "    (linear1): Linear(in_features=32, out_features=5, bias=True)\n",
       "    (reversed_conv1): DenseGCNConv(128, 32)\n",
       "    (multihead_attn_l1): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_path = '/scratch/smanduru/NLP/project/saved_models/A512' + '/hipool_10eps.pth'\n",
    "\n",
    "# Load the entire model\n",
    "model = torch.load(load_path)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55e0de2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocess:\n",
    "    \n",
    "    def __init__(self, testPath):\n",
    "        \n",
    "        self.test_df = pd.read_csv(testPath, sep = '\\t', header=0)\n",
    "        self.test_df['review'] = self.test_df['headline'].str.cat(self.test_df['text'], sep=' ')\n",
    "        \n",
    "    \n",
    "    def clean_text(self, sentence):\n",
    "        cleaned_sentence = re.sub(r'[^a-zA-Z0-9\\s]', ' ', sentence)\n",
    "        cleaned_sentence = re.sub(r'\\s+', ' ', cleaned_sentence).strip()\n",
    "        return cleaned_sentence.lower()\n",
    "        \n",
    "    def get_clean(self):\n",
    "        \n",
    "        self.test_df['cleaned_text'] = self.test_df['review'].apply(self.clean_text)\n",
    "        return self.test_df[['cleaned_text', 'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10c047c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pr = Preprocess(\"/scratch/smanduru/NLP/project/data/amazon_512/amazon-books-512-test.tsv\")\n",
    "\n",
    "test = pr.get_clean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30b0305c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, tokenizer, max_len, df, chunk_len=200, overlap_len=50, approach=\"all\", max_size_dataset=None, min_len=249):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.overlap_len = overlap_len\n",
    "        self.chunk_len = chunk_len\n",
    "        self.approach = approach\n",
    "        self.min_len = min_len\n",
    "        self.max_size_dataset = max_size_dataset\n",
    "        self.data, self.label = self.process_data(df)\n",
    "        \n",
    "    def process_data(self, df):\n",
    "        self.num_class = len(set(df['label'].values))\n",
    "        return df['cleaned_text'].values, df['label'].values\n",
    "    \n",
    "    def long_terms_tokenizer(self, data_tokenize, targets):\n",
    "        long_terms_token = []\n",
    "        input_ids_list = []\n",
    "        attention_mask_list = []\n",
    "        token_type_ids_list = []\n",
    "        targets_list = []\n",
    "\n",
    "        previous_input_ids = data_tokenize[\"input_ids\"].reshape(-1)\n",
    "        previous_attention_mask = data_tokenize[\"attention_mask\"].reshape(-1)\n",
    "        previous_token_type_ids = data_tokenize[\"token_type_ids\"].reshape(-1)\n",
    "        remain = data_tokenize.get(\"overflowing_tokens\")\n",
    "        targets = torch.tensor(targets, dtype=torch.int)\n",
    "        \n",
    "        start_token = torch.tensor([101], dtype=torch.long)\n",
    "        end_token = torch.tensor([102], dtype=torch.long)\n",
    "\n",
    "        total_token = len(previous_input_ids) -2 # remove head 101, tail 102\n",
    "        stride = self.overlap_len - 2\n",
    "        number_chunks = math.floor(total_token/stride)\n",
    "\n",
    "        mask_list = torch.ones(self.chunk_len, dtype=torch.long)\n",
    "        type_list = torch.zeros(self.chunk_len, dtype=torch.long)\n",
    "        \n",
    "        for current in range(number_chunks-1):\n",
    "            input_ids = previous_input_ids[current*stride:current*stride+self.chunk_len-2]\n",
    "            input_ids = torch.cat((start_token, input_ids, end_token))\n",
    "            input_ids_list.append(input_ids)\n",
    "\n",
    "            attention_mask_list.append(mask_list)\n",
    "            token_type_ids_list.append(type_list)\n",
    "            targets_list.append(targets)\n",
    "\n",
    "        if len(input_ids_list) == 0:\n",
    "            input_ids = torch.ones(self.chunk_len-2, dtype=torch.long)\n",
    "            input_ids = torch.cat((start_token, input_ids, end_token))\n",
    "            input_ids_list.append(input_ids)\n",
    "\n",
    "            attention_mask_list.append(mask_list)\n",
    "            token_type_ids_list.append(type_list)\n",
    "            targets_list.append(targets)\n",
    "\n",
    "        return({\n",
    "            'ids': input_ids_list,\n",
    "            'mask': attention_mask_list,\n",
    "            'token_type_ids': token_type_ids_list,\n",
    "            'targets': targets_list,\n",
    "            'len': [torch.tensor(len(targets_list), dtype=torch.long)]\n",
    "        })\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        review = str(self.data[idx])\n",
    "        targets = int(self.label[idx])\n",
    "        data = self.tokenizer.encode_plus(\n",
    "            review,\n",
    "            max_length=self.max_len,\n",
    "            pad_to_max_length=False,\n",
    "            add_special_tokens=True,\n",
    "            return_attention_mask=True,\n",
    "            return_token_type_ids=True,\n",
    "            return_overflowing_tokens=True,\n",
    "            return_tensors='pt')\n",
    "        \n",
    "        long_token = self.long_terms_tokenizer(data, targets)\n",
    "        return long_token\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.label.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ba81048",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_collate1(batches):\n",
    "    return [{key: torch.stack(value) for key, value in batch.items()} for batch in batches]\n",
    "\n",
    "MAX_LEN = 1024\n",
    "CHUNK_LEN = 200\n",
    "OVERLAP_LEN = int(CHUNK_LEN/2)\n",
    "\n",
    "# TRAIN_BATCH_SIZE = 16\n",
    "# EPOCH = 20\n",
    "# lr=1e-5\n",
    "\n",
    "test_dataset = CustomDataset(\n",
    "    tokenizer = bert_tokenizer,\n",
    "    max_len = MAX_LEN,\n",
    "    chunk_len = CHUNK_LEN,\n",
    "    overlap_len = OVERLAP_LEN,\n",
    "    df = test)\n",
    "\n",
    "\n",
    "test_loader = DataLoader(test_dataset,\n",
    "                          batch_size = 32, \n",
    "                          shuffle = False, \n",
    "                          collate_fn = my_collate1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4887180b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fun(outputs, targets):\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    return loss(outputs, targets)\n",
    "\n",
    "def eval_loop_fun1(data_loader, model, device):\n",
    "    model.eval()\n",
    "    fin_targets = []\n",
    "    fin_outputs = []\n",
    "    losses = []\n",
    "    for batch_idx, batch in enumerate(data_loader):\n",
    "        ids = [data[\"ids\"] for data in batch]  # size of 8\n",
    "        mask = [data[\"mask\"] for data in batch]\n",
    "        token_type_ids = [data[\"token_type_ids\"] for data in batch]\n",
    "        targets = [data[\"targets\"] for data in batch]  # length: 8\n",
    "\n",
    "        with torch.no_grad():\n",
    "            target_labels = torch.stack([x[0] for x in targets]).long().to(device)\n",
    "            outputs, _ = model(ids=ids, mask=mask, token_type_ids=token_type_ids)\n",
    "            loss = loss_fun(outputs, target_labels)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "        fin_targets.append(target_labels.cpu().detach().numpy())\n",
    "        fin_outputs.append(torch.softmax(outputs, dim=1).cpu().detach().numpy())\n",
    "    return np.concatenate(fin_outputs), np.concatenate(fin_targets), losses\n",
    "\n",
    "def evaluate(target, predicted):\n",
    "    true_label_mask = [1 if (np.argmax(x)-target[i]) ==\n",
    "                       0 else 0 for i, x in enumerate(predicted)]\n",
    "    nb_prediction = len(true_label_mask)\n",
    "    true_prediction = sum(true_label_mask)\n",
    "    false_prediction = nb_prediction-true_prediction\n",
    "    accuracy = true_prediction/nb_prediction\n",
    "    return{\n",
    "        \"accuracy\": accuracy,\n",
    "        \"nb exemple\": len(target),\n",
    "        \"true_prediction\": true_prediction,\n",
    "        \"false_prediction\": false_prediction,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d504c060",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Hi_Bert_Classification_Model_GCN(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (mapping): Linear(in_features=768, out_features=128, bias=True)\n",
       "  (gcn): HiPool(\n",
       "    (conv1): DenseGCNConv(128, 32)\n",
       "    (conv2): DenseGCNConv(32, 32)\n",
       "    (linear1): Linear(in_features=32, out_features=5, bias=True)\n",
       "    (reversed_conv1): DenseGCNConv(128, 32)\n",
       "    (multihead_attn_l1): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0bf8a8b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/tmp/ipykernel_1961477/1720383461.py:120: FutureWarning: adjacency_matrix will return a scipy.sparse array instead of a matrix in Networkx 3.0.\n",
      "  adj_matrix = nx.adjacency_matrix(generated_adj).todense()\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the test set\n",
    "predicted_probs, true_labels, losses = eval_loop_fun1(test_loader, model, device)\n",
    "\n",
    "# Evaluate accuracy\n",
    "evaluation_result = evaluate(true_labels, predicted_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b59d291",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.6454224147759663,\n",
       " 'nb exemple': 52626,\n",
       " 'true_prediction': 33966,\n",
       " 'false_prediction': 18660}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ca2c6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "misclassified_examples = []\n",
    "classified_examples = []\n",
    "for i, (true_label, predicted_prob) in enumerate(zip(true_labels, predicted_probs)):\n",
    "    predicted_label = np.argmax(predicted_prob)\n",
    "    if predicted_label != true_label:\n",
    "        misclassified_examples.append({\n",
    "            'Example Index': i,\n",
    "            'True Label': true_label,\n",
    "            'Predicted Label': predicted_label,\n",
    "            'Predicted Probabilities': predicted_prob.tolist(),\n",
    "            'Raw Input': test.iloc[i]['cleaned_text']\n",
    "        })\n",
    "    else:\n",
    "        classified_examples.append({\n",
    "            'Example Index': i,\n",
    "            'True Label': true_label,\n",
    "            'Predicted Label': predicted_label,\n",
    "            'Predicted Probabilities': predicted_prob.tolist(),\n",
    "            'Raw Input': test.iloc[i]['cleaned_text']\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "89053c0f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Index: 52172\n",
      "True Label: 0, Predicted Label: 3\n",
      "Raw Input Sentence: this book is pioneering in one way i mean it s pretty unusual to find a reactionary science fiction writer even an old one since i ve just had to read a page screed in the form of a novel i hope it s all right for me to respond to this book in purely argumentative terms the story is the same ok thriller crichton produces every time he sneezes before trusting crichton s competence to gauge the state of science it s worth glancing back at the things he has considered plausible likely decimating viruses brought to earth from space cloned dinosaurs and time travel of course it s much safer predicting what s not going to happen which is the track crichton has opted for in this novel his science though has always been fragmentary and slipshod yes he went to harvard but there s a reason he proceeded from there into publishing his politics are less reliable remember the japanese takeover of the american economy the battles for computer materials in the congo the female usurpation of corporate boardrooms in this book he makes the strikingly counterintuitive argument that environmentalists despite all evidence to the contrary are on the verge of achieving huge political breakthroughs the fact that environmentalism continually polls around the bottom rung of american voters concerns seems to belie this belief as does the nearly utter absence of it from the issues discussed in the last presidential election well anyway what about the science the facts of global warming are not difficult to grasp burning gasoline creates carbon dioxide and carbon dioxide holds heat in the atmosphere see venus it is true that the feedback effects of this make it impossible to predict global warming with certainty but the general physics of this equation are simple so taken on average we should expect that the more carbon dioxide we put in the atmosphere the more the temperature will go up though we may be wrong about how the effects will actually play out we must remember that we will not be able to reverse these trends if they become far advanced one prediction of global warming is that the himalayan glaciers will disappear these provide the water for the crops which feed about of the world s population this is not a thing we want to be wrong about though a wealthy and aged writer in america can regard this possibility with a great deal of philosophical detachment in any case we do know that gasoline is a limited resource and we know that it forces the united states to support countries we find politically objectionable the money i spent on gas today might for instance help fund the iranian attempt at the nuclear bomb it d be better if america tried something like sweden which has announced an attempt to eliminate oil use within fifteen years sic the technology and social change involved is exciting even visionary the sort of bold and intelligent thing one would hope to find anticipated by a good science fiction writer\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Shuffle misclassified examples\n",
    "np.random.shuffle(misclassified_examples)\n",
    "\n",
    "# Print 5 randomly selected misclassified examples\n",
    "for example in misclassified_examples[:5]:\n",
    "    print(f\"Example Index: {example['Example Index']}\")\n",
    "    print(f\"True Label: {example['True Label']}, Predicted Label: {example['Predicted Label']}\")\n",
    "    print(f\"Raw Input Sentence: {example['Raw Input']}\")\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67679de7",
   "metadata": {},
   "source": [
    "# Robustness - Jumbling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e992cb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_examples = random.sample(classified_examples, min(10, len(classified_examples)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "35afd01a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[41993, 20921, 39847, 35522, 27660, 23345, 40922, 35234, 30719, 19054]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexes = [] # [587, 417, 364, 82, 311, 642, 872, 152, 287, 314] [944, 535, 909, 912, 55, 186, 297, 830, 541, 871]\n",
    "for each in selected_examples:\n",
    "    indexes.append(each['Example Index'])\n",
    "\n",
    "indexes\n",
    "\n",
    "# [7, 120, 586, 769, 63, 931, 322, 371, 51, 105] - Graph Sage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5b0d086b",
   "metadata": {},
   "outputs": [],
   "source": [
    "robust_df = pd.DataFrame(selected_examples)\n",
    "\n",
    "columns_to_drop = ['Predicted Label', 'Predicted Probabilities']\n",
    "robust_df = robust_df.drop(columns=columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "48eb29ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jumbling function\n",
    "def jumble_sentence(sentence):\n",
    "    # Split the sentence into words\n",
    "    words = sentence.split()\n",
    "    \n",
    "    # Jumble the words\n",
    "    jumbled_words = random.sample(words, len(words))\n",
    "    \n",
    "    # Join the jumbled words back into a sentence\n",
    "    jumbled_sentence = ' '.join(jumbled_words)\n",
    "    \n",
    "    return jumbled_sentence\n",
    "\n",
    "robust_df['jumbled_sentence'] = robust_df['Raw Input'].apply(jumble_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "857cda23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Example Index', 'True Label', 'Raw Input', 'jumbled_sentence'], dtype='object')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "robust_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "50a8ac63",
   "metadata": {},
   "outputs": [],
   "source": [
    "robust_df = robust_df.rename(columns={'True Label': 'label'})\n",
    "robust_df = robust_df.rename(columns={'jumbled_sentence': 'cleaned_text'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "966518d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "robust_dataset = CustomDataset(\n",
    "    tokenizer = bert_tokenizer,\n",
    "    max_len = MAX_LEN,\n",
    "    chunk_len = CHUNK_LEN,\n",
    "    overlap_len = OVERLAP_LEN,\n",
    "    df = robust_df[['cleaned_text', 'label']])\n",
    "\n",
    "\n",
    "robust_loader = DataLoader(robust_dataset,\n",
    "                          batch_size = 2, \n",
    "                          shuffle = False, \n",
    "                          collate_fn = my_collate1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c4ec4c79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1961477/1720383461.py:120: FutureWarning: adjacency_matrix will return a scipy.sparse array instead of a matrix in Networkx 3.0.\n",
      "  adj_matrix = nx.adjacency_matrix(generated_adj).todense()\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the test set\n",
    "predicted_probs, true_labels, losses = eval_loop_fun1(robust_loader, model, device)\n",
    "\n",
    "# Evaluate accuracy\n",
    "evaluation_result = evaluate(true_labels, predicted_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ed058dbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.6,\n",
       " 'nb exemple': 10,\n",
       " 'true_prediction': 6,\n",
       " 'false_prediction': 4}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4b8f9243",
   "metadata": {},
   "outputs": [],
   "source": [
    "robust_misclassified_examples = []\n",
    "robust_classified_examples = []\n",
    "for i, (true_label, predicted_prob) in enumerate(zip(true_labels, predicted_probs)):\n",
    "    predicted_label = np.argmax(predicted_prob)\n",
    "    if predicted_label != true_label:\n",
    "        robust_misclassified_examples.append({\n",
    "            'Example Index': i,\n",
    "            'True Label': true_label,\n",
    "            'Predicted Label': predicted_label,\n",
    "            'Predicted Probabilities': predicted_prob.tolist(),\n",
    "            'Raw Input': test.iloc[i]['cleaned_text']\n",
    "        })\n",
    "    else:\n",
    "        robust_classified_examples.append({\n",
    "            'Example Index': i,\n",
    "            'True Label': true_label,\n",
    "            'Predicted Label': predicted_label,\n",
    "            'Predicted Probabilities': predicted_prob.tolist(),\n",
    "            'Raw Input': test.iloc[i]['cleaned_text']\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "99e2085e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Example Index': 0,\n",
       "  'True Label': 4,\n",
       "  'Predicted Label': 3,\n",
       "  'Predicted Probabilities': [0.02595958299934864,\n",
       "   0.02595958299934864,\n",
       "   0.10178646445274353,\n",
       "   0.4291187822818756,\n",
       "   0.4171755015850067],\n",
       "  'Raw Input': 'jobs for everyone senator paul simon of illinois wrote this book in he died in the basic premise of the book is that simon s proposed guaranteed job opportunity program will help reduce unemployment and stem poverty by gainfully employing in public works type projects those without jobs who could and would work simon compares and contrasts his proposal with the works progress administration the comprehensive employment and training act ceta and the jobs training partnership act jtpa the workforce investment act wia the one we have now came about after simon s book was written so of course it was not included in his book wia took effect in and is still the current law of the land as of what you may learn from simon s book is some of the history of public job creation programs since the depression years simon had some very good ideas and many of them have already been incorporated into wia and similar state programs the tenor of all these latter day job and training programs is that of compromise and hybridism between the political left and the political right roosevelt s wpa started the ball rolling and when unemployment rose alarmingly again a few decades later ceta was enacted but the political right did not like ceta it was too much government intervention and not enough obedience to free market forces thus we pass through jtpa a hybrid type program with something for private and public sectors to wia wia doesn t provide jobs except to the employees of the wia work source centers these centers have different names in each participating state the employees of the centers may install banks of computers hold job preparation classes and provide job leads so that unemployed people can on their own find jobs hopefully in the private sector without subsidy however there are still many subsidies social security s ticket to work program is another example of how simon s ideas have morphed into the st century social security recipients are encouraged to work and are given some leeway in how much they make until they reach a certain criterion level these are examples of the compromises that both republicans and democrats finally bought and got through both houses to become laws so far some of it seems to be working i work at one of the wia sites in california this organization previously provided jtpa training the organization also sponsors a ticket to work program it seems to be an ongoing cycle what comes next one important factor that has changed since simon wrote this book immigration from outside the u s immigrants have taken many of the jobs that simon was proposing to create when simon wrote african american poverty was widely cited and rued today at least in places where immigrants have made homes there is an entirely different battle going on centering around the question are illegal immigrants taking jobs from legal americans or are they doing jobs no american would stoop to do simon s book has several vignettes of struggling unemployed people mostly from the early s so the book is outdated but still useful for some purposes diximus'},\n",
       " {'Example Index': 2,\n",
       "  'True Label': 4,\n",
       "  'Predicted Label': 3,\n",
       "  'Predicted Probabilities': [0.009080256335437298,\n",
       "   0.009080256335437298,\n",
       "   0.12242667376995087,\n",
       "   0.8056092858314514,\n",
       "   0.053803540766239166],\n",
       "  'Raw Input': 'the normandy invasion i realized that the greatest drama here is not the event but the raw and frightening uncertainty for everyone involved so states jeff shaara in his introduction to the second volume in his wwii trilogy and wow did he ever come through though the events of this horrendous war have been chewed over for more than half a century shaara brings a sense of immediacy to his recounting that stimulates genuine anxiety in the reader he s definitely hit his stride as historical novelist bringing together painstaking research with his considerable skills at dramatization the steel wave covers the period from january to september beginning with the planning of d day and ending with the allies success at driving the germans from france shaara alternates between action scenes involving the fighting men and their generals and strategic sessions involving the higher ups most notably churchill eisenhower montgomery and patton for the allies and rommel for the nazis even hitler makes the occasional appearance the author s admiration for rommel who fought with honor for his country while despising hitler is obvious and probably justified similarly shaara illustrates the delicate balancing act performed by eisenhower as he coordinated american and british staff officers and juggled american and british political demands having watched all the old wwii movies it s easy to shrug off the normandy invasion as perfectly planned and executed but having read this book your comfortable assumptions are blown to bits very little went as planned and the men who fought this campaign carried on in the face of mind boggling unforeseen obstacles what shaara does best is encapsulate the experience of the soldiers as his father did in the killer angels in which provided the most incredible description of an artillery bombardment that i ve ever read the son now does for omaha beach and its grueling aftermath screaming wails the air above them ripped and shattered the shells began to thunder above them jolting him the men tumbling again more dust the concrete shaking deafening blasts he lay flat held his helmet to his head curled his legs in tight felt himself bouncing on the concrete his hands hard on his ears his brain screaming into the roar of fire the terror grabbing him pulling him into a complete and perfect hell when these guys those who survived the landing hit the beach they were soaking wet and really scared and it s been said that men were dying at the rate of one every six seconds and yet they prevailed in the air on the sea and on the land in the face of everything rommel could throw at them the steel wave makes the battles and those who fought them come alive no dry facts maps and figures here we must not ever forget those who won this war and those who read this book are in no danger of that and shattered the shells began to thunder above them jolting him the men tumbling again more dust the concrete shaking deafening blasts he lay flat held his helmet to his head curled his legs in tight felt himself bouncing on the concrete his hands hard on his ears his brain screaming into the roar of fire the terror grabbing him pulling him into a complete and perfect hell when these guys those who survived the landing hit the beach they were soaking wet and really scared and it s been said that men were dying at the rate of one every six seconds and yet they prevailed in the air on the sea and on the land in the face of everything rommel could throw at them the steel wave makes the battles and those who fought them come alive no dry facts maps and figures here we must not ever forget those who won this war and those who read this book are in no danger of that'},\n",
       " {'Example Index': 3,\n",
       "  'True Label': 4,\n",
       "  'Predicted Label': 2,\n",
       "  'Predicted Probabilities': [0.13074031472206116,\n",
       "   0.1674039363861084,\n",
       "   0.3492603898048401,\n",
       "   0.2218550145626068,\n",
       "   0.13074031472206116],\n",
       "  'Raw Input': 'a spiritual memoir for punks there are many books available about buddhism from all the various schools theravada zen nichiren vietnamese tibetan there are books about buddhist ethics about meditation about dealing with grief anger and depression through buddhism but i doubt that there are many books like this describing the intersection between the rebellious ethos of punk rock and the transformational spiritual practice of buddhism there are as levine points out many differences between the two but they share more in common than one might think and most important for levine and for anyone who can relate to his story the two are not mutually exclusive both can be valid healthy ways of expressing oneself one important thing to keep in mind is that dharma punx is first and foremost a memoir it is not a book about buddhism or about punk rock levine writes very honestly or at least with a willingness to talk about his faults about his life giving us all the raw details the good and the bad he also makes it clear that there is more to him than either punk rock or buddhism he enjoys other forms of music takes influences from other spiritual paths but he also talks about more than music and religion he also discusses his family life experiences with his friends his jobs and the depths of drug addiction and violence that he had spiraled into during his early and mid teens levine makes an interesting point when he talks about the trajectory of his pre buddhist life that when his days were spent getting high or stealing beating someone up getting beat up or getting arrested looking for the next fix he wasn t connected anymore to his punk rock lifestyle he had stopped caring about the music and the identity and most other punkers scorned him when he got to that point the book unfolds in a series of short chapters each taking a title that often sounds inspired by song titles the events described in the chapter usually having some ironic relation tothe title examples who killed bambi sex pistols meditate and destroy from metallica s seek and destroy die die my darling the misfits levine starts by showing him at this lowest point locked up in a padded cell after a suicide attempt wishing for death finally realizing the pain he had inflicted on himself and others by his ways wanting to get out of it but not seeing any other solution than death he half heartedly tries to follow his father s stephen levine a spiritual teacher in his own right meditation instructions it helps a little and becomes the seed for levine s spiritual development but there is a hard road ahead for him but before he goes into his recovery he tells us how he got there in another startling anecdote he then goes into being five years old holding a knife to his stomach and wanting to die while his mother and stepfather scream at each other his parental situation as a child while not the most horrifying one could imagine is not exactly optimal either his mother always seems to end up with abusive men and his father and stepmother while loving seem a bit distant and wrapped up in their own lives to give levine the attention he rightly or wrongly craved while levine does seem to hold his parents accountable to an extent he later also makes clear that he had to realize that he was the one to choose to follow the path that he did and that his circumstances were not as horrible as many in the world have to live with most of all it took him to be able to realize that his circumstances do not justify his actions nor do his past actions prevent him from changing the twelve step addiction recovery program as well as buddhism and spiritual practice were instrumental in him coming to these realizations from contemplating suicide at five he went on to become an angry and rebellious youth and when he found punk rock it seemed to fit that perfectly he found others into the same music and way of being and by the age of ten they were drinking alcohol smoking pot and worse he moved back and forth between his mother and father between california and new mexico due to his continual run ins with authority getting into trouble at school getting caught with drugs vandalizing etc he attended many punk shows shaved his head wore boots and studded leather and brandished the logos of his favorite bands on stickers shirts and tattoos but as i alluded to earlier the anger and frustration with not just authority but life itself of never feeling satisfied led to his total immersion in drugs violence and sex the supposed hallmarks of a rock n roll lifestyle yet the very things that can disconnect people from the music they love even in the worst moments though levine keeps a sense of humor and humility pointing out the ridiculousness of his behavior though maintaining a serious enough tone to not lose the impact i found myself stopping at various points when he was describing his early life and thinking wow that s messed up like in one episode him and his friend break into another friend s house steal the vcr and jewelry then sell the vcr for coke and weed they meet up with a guy who takes them around in his car and drink and they end up crashing the car flips and catches on fire levine escapes by kicking out a window but makes sure to grab the beer and stolen jewelry before getting out they walk away from the upside down burning car drinking their beers one of the most inspiring things about this book is hearing how low levine got in his life and was yet able to become a respected spiritual teacher counseling people dealing with addiction working with aids patients and helping his community any way he can so for those who think that there is no hope for reform you can look at levine s story and say that if he could do so can i recovery and discipline in one s life whether you re dealing with addiction illness tragedy or just plain old anger confusion and fear happen only with practice it takes time it takes pain and it takes a lot of humility levine outlines how he spent years going to the many people he stole from or hurt physically and mentally and making amends but buddhism s draw for many people is that it gives them a structure and purposeful way of achieving this just like the step plan which is why it s so effective though he at first resisted the spiritual elements of recovery trying to just ween himself from drugs and alcohol without the religious trappings his relapses and frustrations push him toward spirituality particularly buddhism as a way of living a better life ironically enough one of his first teachers who did not align himself with any particular religion though drawing heavily from eastern ones ended up getting married to a woman breaking a vow of celibacy who caused much pain and suffering to his disciples manipulating him into being a power hungry guru levine could have regressed to his old ways from that and though it took him a long time to deal with it he moved on to other teachers and to his own investigations this perhaps underscores the way in which punk rock can connect with buddhism both have a sort of anti establishment anti authority ethos they express this in different ways while buddhists lay or monk nun often follow teachers the emphasis is on what actually works to transform them and break them from the fetters of suffering from samsara in punk this takes expression in music looks and acts that differ from the norm though a lot of these have been commercialized and normalized now and in anger and rebellion towards those in power who cause widespread suffering by their greed apathy and malice both call one to resist the ignorance that leads to unawareness about suffering about its causes and the ways to resist it levine remarks that when he first met monks in asia he felt they were the most punk people he had met they had truly resisted and transcended the system to live a totally radical way of life one of the disappointments i found with levine s description of his spiritual development is that he doesn t really explain the buddhist concepts and practices that have affected his life so much he mentions karma the eightfold path the four noble truths etc but doesn t explain what these are i have investigated buddhism enough to know what these mean to me but i think it would have been interesting to hear levine explain how he interprets them and relates them to his life and for someone not as familiar with buddhism i think it would have been helpful also he goes into his travels to asia where he encounters monks and teachers of both the theravada and mahayana traditions which can also be sort of confusing i also found it a bit odd that he mentions trying to be as sensitive to the ecological ramifications of his actions as possible sticking to a strict vegan diet out of compassion for example and yet he seems to travel a lot with many international flights which have a devastating environmental impact but he never mentions this i m not saying it s wrong just that you would think he would be a little more conscious of this i do give levine credit though for not showing himself as some perfect spiritual being in contrast to his previous drugged out destructive self he points out various flaws in himself that being sober and dedicated to a spiritual life have not necessarily stamped out pride anger vanity even doubt and despair this only serves to make his story sound more human and sincere i get put off by spiritual writers who give me the feel of having a superior attitude with little awareness of it levine does feel superior at times but the fact that he admits it makes those moments less irritating to me and makes me feel less distant from some elevated untouchable guru onevery interesting chapter in his spiritual life is when he takes on his father s a year to live practice living his life as if he really only had a year to live he goes through periods of confusion and fear actually getting into the mindset of believing that he would soon die it also turns out to be a very liberating experience for him helping him to overcome some of his attachments and fears so in conclusion if you are looking for a spiritual oriented memoir especially one in which the protagonist comes from a punk rock background you will enjoy this book if you re looking to learn about buddhism or punk rock this is not the place to start it does show how both can affect a person s life i give it stars only for the slight flaws i mentioned but it s a very powerful book that i highly recommend'},\n",
       " {'Example Index': 4,\n",
       "  'True Label': 3,\n",
       "  'Predicted Label': 2,\n",
       "  'Predicted Probabilities': [0.0016689262120053172,\n",
       "   0.031439539045095444,\n",
       "   0.9247117638587952,\n",
       "   0.04051078110933304,\n",
       "   0.0016689262120053172],\n",
       "  'Raw Input': 'it s ideal dr ellie sullivan has completed her residency in the er at st vincent s hospital and is planning to move on to where she s not sure a few days before her final one at the hospital she decides to go jogging at a nearby park and runs into an fbi sting ellie unfortunately is now at risk since she may be able to identify the man who shot one of the agents she also gets the attention of agent max daniels who was on the scene and accompanied the felled agent to the hospital where ellie performed the surgery to save his life additionally ellie s is the focus of a stalker from her childhood who almost killed her when she was eleven years old she s led an unconventional life since attempting to move on with her life under a cloud of anonymity and secrecy to further complicate matters ellie is a prodigy having graduated from college at sixteen both ellie and max experience an undercurrent of attraction for each other but are unwilling to acknowledge or pursue it he s not looking to be in a relationship and she cannot afford the commitment given the circumstances of her life however when the fbi determines ellie needs protection until they can capture the shooter who is wanted for other crimes max inexplicably decides to assume that responsibility after ellie has returned home to winston falls south carolina for her sister s wedding the relationship between ellie and max has all the typical elements of a garwood romance max is the sexy good looking commitment phobic hero to ellie s disarming qualities she has every reason to be fragile but has resiliency and grit under her fair skin and is an appealing heroine while max doesn t exactly fit the image she s created as her ideal man her attraction to him is beyond her resistance both have their share of baggage and it adds another layer of tension as they find their feelings for each other deepening max s protection of ellie from the park shooter and the threat of the stalker s return underscore this entire story providing added tension to the developing relationship between the two and that of ellie s family ellie and her sister ava are at odds her father is over protective and her mother is in denial all this unfolds during the week prior to ava s wedding with all the drama associated with a high stress event this is an interesting story with well developed likeable characters ava being the one exception you become invested in max and ellie and root for their relationship my only criticism is many aspects of the story wrap too hurriedly towards the end as if there was a need for immediate closure it was however a pleasure to see a reference to one of the buchanan men from the buchanan renard series let s hope that s a sign that there s another book coming in that series if you re a fan of that one you ll like this story s return underscore this entire story providing added tension to the developing relationship between the two and that of ellie s family ellie and her sister ava are at odds her father is over protective and her mother is in denial all this unfolds during the week prior to ava s wedding with all the drama associated with a high stress event this is an interesting story with well developed likeable characters ava being the one exception you become invested in max and ellie and root for their relationship my only criticism is many aspects of the story wrap too hurriedly towards the end as if there was a need for immediate closure it was however a pleasure to see a reference to one of the buchanan men from the buchanan renard series let s hope that s a sign that there s another book coming in that series if you re a fan of that one you ll like this story'}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "robust_misclassified_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc3ac97",
   "metadata": {},
   "source": [
    "# Robustness - Drop Some Words Randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e57efba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "robust_df = robust_df[['Example Index', 'label', 'Raw Input']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "02fea5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_words(sentence):\n",
    "    words = sentence.split()\n",
    "    \n",
    "    # Randomly choose a percentage of words to drop (adjust as needed)\n",
    "    percentage_to_drop = 0.3  # 30% of words will be dropped\n",
    "    num_words_to_drop = int(len(words) * percentage_to_drop)\n",
    "    \n",
    "    # Randomly select words to drop\n",
    "    words_to_drop = random.sample(words, num_words_to_drop)\n",
    "    \n",
    "    # Create a new sentence without the dropped words\n",
    "    new_sentence = ' '.join(word for word in words if word not in words_to_drop)\n",
    "    \n",
    "    return new_sentence\n",
    "\n",
    "# Create a new column 'Robust Input' by applying the drop_words function\n",
    "robust_df['cleaned_text'] = robust_df['Raw Input'].apply(drop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1a73a603",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1961477/1720383461.py:120: FutureWarning: adjacency_matrix will return a scipy.sparse array instead of a matrix in Networkx 3.0.\n",
      "  adj_matrix = nx.adjacency_matrix(generated_adj).todense()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8,\n",
       " 'nb exemple': 10,\n",
       " 'true_prediction': 8,\n",
       " 'false_prediction': 2}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "robust_dataset = CustomDataset(\n",
    "    tokenizer = bert_tokenizer,\n",
    "    max_len = MAX_LEN,\n",
    "    chunk_len = CHUNK_LEN,\n",
    "    overlap_len = OVERLAP_LEN,\n",
    "    df = robust_df[['cleaned_text', 'label']])\n",
    "\n",
    "\n",
    "robust_loader = DataLoader(robust_dataset,\n",
    "                          batch_size = 2, \n",
    "                          shuffle = False, \n",
    "                          collate_fn = my_collate1)\n",
    "\n",
    "# Evaluate the test set\n",
    "predicted_probs, true_labels, losses = eval_loop_fun1(robust_loader, model, device)\n",
    "\n",
    "# Evaluate accuracy\n",
    "evaluation_result = evaluate(true_labels, predicted_probs)\n",
    "\n",
    "evaluation_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3797a053",
   "metadata": {},
   "outputs": [],
   "source": [
    "robust_misclassified_examples = []\n",
    "robust_classified_examples = []\n",
    "for i, (true_label, predicted_prob) in enumerate(zip(true_labels, predicted_probs)):\n",
    "    predicted_label = np.argmax(predicted_prob)\n",
    "    if predicted_label != true_label:\n",
    "        robust_misclassified_examples.append({\n",
    "            'Example Index': i,\n",
    "            'True Label': true_label,\n",
    "            'Predicted Label': predicted_label,\n",
    "            'Predicted Probabilities': predicted_prob.tolist(),\n",
    "            'Raw Input': test.iloc[i]['cleaned_text']\n",
    "        })\n",
    "    else:\n",
    "        robust_classified_examples.append({\n",
    "            'Example Index': i,\n",
    "            'True Label': true_label,\n",
    "            'Predicted Label': predicted_label,\n",
    "            'Predicted Probabilities': predicted_prob.tolist(),\n",
    "            'Raw Input': test.iloc[i]['cleaned_text']\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "46807643",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Example Index': 6,\n",
       "  'True Label': 3,\n",
       "  'Predicted Label': 4,\n",
       "  'Predicted Probabilities': [0.0012362416600808501,\n",
       "   0.0012362416600808501,\n",
       "   0.004063094034790993,\n",
       "   0.01332862488925457,\n",
       "   0.9801357984542847],\n",
       "  'Raw Input': 'changing misconceptions this book is written from the perspective that the church has missed out on the meaning of the term people of bread found in the bible as an image of god s people in vondey s mind the church has completely missed the point of the lord s supper and we have taken the analogy of being people of the bread and cast it to the wayside this book is his response to this problem his response is to take the term people of bread and look at it from many different angles to see exactly what it would mean in different contexts to do this he looks first at the idea of images and suggests that the church has lost its ability to imagine it is his viewpoint that imagery was much more prolific in biblical times and that by allowing it to become a lost art we have lost our ability to interpret some of what god is telling us he then looks at the actual idea of breaking the bread and the connotations such an action might have in it he finds lessons regarding hospitality freedom duty and responsibility for the church i ll be honest my initial response to this book was fairly negative i felt it was an excellent example of what happens when someone takes an analogy too far i then read his conclusion people of bread has been an exercise in reviving the theological imagination as an ecclesiological task this exercise has attempted to liberate the image of bread from the exclusive context of the lord s supper and to paint a vivid picture of the social moral missiological sacramental ecumenical and eschatological nature of the church its at that point that i had to stand back and say well done vondey accomplished his mission he took the idea of people of bread and looked at it from every angle he wasn t necessarily trying to prove a point but to point out a problem that the church has i think he did that quite well i will say this though it is not a light book to be read by the faint of heart its not a devotional book this book has amazing insights in it but it takes commitment to find them if you re up to the challenge then by all means read the book'},\n",
       " {'Example Index': 8,\n",
       "  'True Label': 3,\n",
       "  'Predicted Label': 4,\n",
       "  'Predicted Probabilities': [9.12141113076359e-05,\n",
       "   9.12141113076359e-05,\n",
       "   0.0004436119052115828,\n",
       "   0.002344597363844514,\n",
       "   0.9970294237136841],\n",
       "  'Raw Input': 'and stars fairly entertaining installment in the bride series cat racing really perhaps it was an interesting way to begin a romance and though it was actually something done historically in england it seemed so fanciful to me i put the book down and started another however since i committed to read this among many others for my best irish romances list i eventually picked it up again i have to say while i could have done without the cat racing it turned out to be a fairly entertaining read set in this is th in the sherbrooke series see list below and the only one of coulter s romances set in ireland and really only the last part both the hero and heroine are english but they do end up in ireland at a castle called pendragon having not read others in the series it was clear that i was coming into the middle of things i do think it can be enjoyed as a stand alone but particularly at the beginning the names won t mean as much the story tells of meggie sherbrooke chief cat racing trainer and daughter of a vicar tysen sherbrooke also a titled lord since she was meggie thought herself in love with jeremy at her coming out she meets jeremy again and hopes he will see her as bride material but jeremy soon weds another on the rebound meggie meets thomas malcombe young earl of lancaster and marries him neither professes love he wants her because she makes him laugh and he hasn t laughed in a very long time coulter gets full marks for very witty dialog and the wonderful heroine who delivers it though her sentence construction had me tripping over passages at times there were several things that detracted in this romance first the conflict between jeremy and meggie was a bit contrived why would he be angry at her for a girlhood crush please she married him after all it made thomas look petty not the tall dark strong man we first thought him to be when he decided to be cruel on their wedding night i hated him this from a guy who is supposed to love her her capitulation after that was a bit of a letdown and who takes his bride home to meet his mother on his honeymoon after that i just never warmed to the man and it was a bit hard to believe they eventually loved each other though the latter part of the book takes place in ireland there isn t much of an irish feel to it actually it could have taken place in england i decided not to include the book on my best irish historical romances list if you want to read more in the bride series sherbrooke here s the list the sherbrooke bridethe hellion bridethe heiress bridemad jackthe courtshipthe scottish bridependragonthe sherbrooke twinslyon s gatewizards daughterthe prince of ravenscaro love her her capitulation after that was a bit of a letdown and who takes his bride home to meet his mother on his honeymoon after that i just never warmed to the man and it was a bit hard to believe they eventually loved each other though the latter part of the book takes place in ireland there isn t much of an irish feel to it actually it could have taken place in england i decided not to include the book on my best irish historical romances list if you want to read more in the bride series sherbrooke here s the list the sherbrooke bridethe hellion bridethe heiress bridemad jackthe courtshipthe scottish bridependragonthe sherbrooke twinslyon s gatewizards daughterthe prince of ravenscar'}]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "robust_misclassified_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8330dd00",
   "metadata": {},
   "source": [
    "# Robustness - Misspelled Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8a076ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "robust_df = robust_df[['Example Index', 'label', 'Raw Input']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c416942c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 504\n",
      "22 450\n",
      "22 440\n",
      "23 462\n",
      "31 629\n",
      "22 444\n",
      "21 437\n",
      "39 797\n",
      "19 398\n",
      "28 570\n"
     ]
    }
   ],
   "source": [
    "def introduce_misspellings(sentence):\n",
    "    words = sentence.split()\n",
    "\n",
    "    # Randomly choose a percentage of words to misspell (adjust as needed)\n",
    "    percentage_to_misspell = 0.05  # 20% of words will be misspelled\n",
    "    num_words_to_misspell = int(len(words) * percentage_to_misspell)\n",
    "    print(num_words_to_misspell, len(words))\n",
    "\n",
    "    # Create a set of misspelled versions of the alphabet\n",
    "    misspelled_alphabet = {\n",
    "        'a': 'ae',\n",
    "        'b': 'bf',\n",
    "        'c': 'cd',\n",
    "        'd': 'de',\n",
    "        'e': 'ea',\n",
    "        'f': 'fg',\n",
    "        'g': 'gh',\n",
    "        'h': 'hi',\n",
    "        'i': 'ij',\n",
    "        'j': 'jk',\n",
    "        'k': 'kl',\n",
    "        'l': 'lm',\n",
    "        'm': 'mn',\n",
    "        'n': 'no',\n",
    "        'o': 'op',\n",
    "        'p': 'pq',\n",
    "        'q': 'qr',\n",
    "        'r': 'rs',\n",
    "        's': 'st',\n",
    "        't': 'tu',\n",
    "        'u': 'uv',\n",
    "        'v': 'vw',\n",
    "        'w': 'wx',\n",
    "        'x': 'xy',\n",
    "        'y': 'yz',\n",
    "        'z': 'zx'\n",
    "    }\n",
    "\n",
    "    # Randomly select words to misspell\n",
    "    words_to_misspell = random.sample(words, num_words_to_misspell)\n",
    "\n",
    "    # Replace selected words with misspelled versions\n",
    "    misspelled_words = [misspelled_alphabet.get(word[0], word) + ''.join(random.choice(string.ascii_lowercase) for _ in range(len(word) - 1)) for word in words_to_misspell]\n",
    "\n",
    "    # Create a new sentence with misspelled words\n",
    "    new_sentence = ' '.join(misspelled_alphabet.get(word[0], word) + ''.join(random.choice(string.ascii_lowercase) for _ in range(len(word) - 1)) if word in words_to_misspell else word for word in words)\n",
    "    return new_sentence\n",
    "\n",
    "# Create a new column 'Robust Input' by applying the introduce_misspellings function\n",
    "robust_df['cleaned_text'] = robust_df['Raw Input'].apply(introduce_misspellings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "38731c49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1961477/1720383461.py:120: FutureWarning: adjacency_matrix will return a scipy.sparse array instead of a matrix in Networkx 3.0.\n",
      "  adj_matrix = nx.adjacency_matrix(generated_adj).todense()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8,\n",
       " 'nb exemple': 10,\n",
       " 'true_prediction': 8,\n",
       " 'false_prediction': 2}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "robust_dataset = CustomDataset(\n",
    "    tokenizer = bert_tokenizer,\n",
    "    max_len = MAX_LEN,\n",
    "    chunk_len = CHUNK_LEN,\n",
    "    overlap_len = OVERLAP_LEN,\n",
    "    df = robust_df[['cleaned_text', 'label']])\n",
    "\n",
    "\n",
    "robust_loader = DataLoader(robust_dataset,\n",
    "                          batch_size = 2, \n",
    "                          shuffle = False, \n",
    "                          collate_fn = my_collate1)\n",
    "\n",
    "# Evaluate the test set\n",
    "predicted_probs, true_labels, losses = eval_loop_fun1(robust_loader, model, device)\n",
    "\n",
    "# Evaluate accuracy\n",
    "evaluation_result = evaluate(true_labels, predicted_probs)\n",
    "\n",
    "evaluation_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "77755daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "robust_misclassified_examples = []\n",
    "robust_classified_examples = []\n",
    "for i, (true_label, predicted_prob) in enumerate(zip(true_labels, predicted_probs)):\n",
    "    predicted_label = np.argmax(predicted_prob)\n",
    "    if predicted_label != true_label:\n",
    "        robust_misclassified_examples.append({\n",
    "            'Example Index': i,\n",
    "            'True Label': true_label,\n",
    "            'Predicted Label': predicted_label,\n",
    "            'Predicted Probabilities': predicted_prob.tolist(),\n",
    "            'Raw Input': test.iloc[i]['cleaned_text']\n",
    "        })\n",
    "    else:\n",
    "        robust_classified_examples.append({\n",
    "            'Example Index': i,\n",
    "            'True Label': true_label,\n",
    "            'Predicted Label': predicted_label,\n",
    "            'Predicted Probabilities': predicted_prob.tolist(),\n",
    "            'Raw Input': test.iloc[i]['cleaned_text']\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d0c96644",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Example Index': 4,\n",
       "  'True Label': 3,\n",
       "  'Predicted Label': 4,\n",
       "  'Predicted Probabilities': [0.000305040244711563,\n",
       "   0.000305040244711563,\n",
       "   0.0008944853325374424,\n",
       "   0.10141097009181976,\n",
       "   0.8970845341682434],\n",
       "  'Raw Input': 'it s ideal dr ellie sullivan has completed her residency in the er at st vincent s hospital and is planning to move on to where she s not sure a few days before her final one at the hospital she decides to go jogging at a nearby park and runs into an fbi sting ellie unfortunately is now at risk since she may be able to identify the man who shot one of the agents she also gets the attention of agent max daniels who was on the scene and accompanied the felled agent to the hospital where ellie performed the surgery to save his life additionally ellie s is the focus of a stalker from her childhood who almost killed her when she was eleven years old she s led an unconventional life since attempting to move on with her life under a cloud of anonymity and secrecy to further complicate matters ellie is a prodigy having graduated from college at sixteen both ellie and max experience an undercurrent of attraction for each other but are unwilling to acknowledge or pursue it he s not looking to be in a relationship and she cannot afford the commitment given the circumstances of her life however when the fbi determines ellie needs protection until they can capture the shooter who is wanted for other crimes max inexplicably decides to assume that responsibility after ellie has returned home to winston falls south carolina for her sister s wedding the relationship between ellie and max has all the typical elements of a garwood romance max is the sexy good looking commitment phobic hero to ellie s disarming qualities she has every reason to be fragile but has resiliency and grit under her fair skin and is an appealing heroine while max doesn t exactly fit the image she s created as her ideal man her attraction to him is beyond her resistance both have their share of baggage and it adds another layer of tension as they find their feelings for each other deepening max s protection of ellie from the park shooter and the threat of the stalker s return underscore this entire story providing added tension to the developing relationship between the two and that of ellie s family ellie and her sister ava are at odds her father is over protective and her mother is in denial all this unfolds during the week prior to ava s wedding with all the drama associated with a high stress event this is an interesting story with well developed likeable characters ava being the one exception you become invested in max and ellie and root for their relationship my only criticism is many aspects of the story wrap too hurriedly towards the end as if there was a need for immediate closure it was however a pleasure to see a reference to one of the buchanan men from the buchanan renard series let s hope that s a sign that there s another book coming in that series if you re a fan of that one you ll like this story s return underscore this entire story providing added tension to the developing relationship between the two and that of ellie s family ellie and her sister ava are at odds her father is over protective and her mother is in denial all this unfolds during the week prior to ava s wedding with all the drama associated with a high stress event this is an interesting story with well developed likeable characters ava being the one exception you become invested in max and ellie and root for their relationship my only criticism is many aspects of the story wrap too hurriedly towards the end as if there was a need for immediate closure it was however a pleasure to see a reference to one of the buchanan men from the buchanan renard series let s hope that s a sign that there s another book coming in that series if you re a fan of that one you ll like this story'},\n",
       " {'Example Index': 6,\n",
       "  'True Label': 3,\n",
       "  'Predicted Label': 4,\n",
       "  'Predicted Probabilities': [4.1711009544087574e-05,\n",
       "   4.1711009544087574e-05,\n",
       "   0.00014098426618147641,\n",
       "   0.019844604656100273,\n",
       "   0.9799309968948364],\n",
       "  'Raw Input': 'changing misconceptions this book is written from the perspective that the church has missed out on the meaning of the term people of bread found in the bible as an image of god s people in vondey s mind the church has completely missed the point of the lord s supper and we have taken the analogy of being people of the bread and cast it to the wayside this book is his response to this problem his response is to take the term people of bread and look at it from many different angles to see exactly what it would mean in different contexts to do this he looks first at the idea of images and suggests that the church has lost its ability to imagine it is his viewpoint that imagery was much more prolific in biblical times and that by allowing it to become a lost art we have lost our ability to interpret some of what god is telling us he then looks at the actual idea of breaking the bread and the connotations such an action might have in it he finds lessons regarding hospitality freedom duty and responsibility for the church i ll be honest my initial response to this book was fairly negative i felt it was an excellent example of what happens when someone takes an analogy too far i then read his conclusion people of bread has been an exercise in reviving the theological imagination as an ecclesiological task this exercise has attempted to liberate the image of bread from the exclusive context of the lord s supper and to paint a vivid picture of the social moral missiological sacramental ecumenical and eschatological nature of the church its at that point that i had to stand back and say well done vondey accomplished his mission he took the idea of people of bread and looked at it from every angle he wasn t necessarily trying to prove a point but to point out a problem that the church has i think he did that quite well i will say this though it is not a light book to be read by the faint of heart its not a devotional book this book has amazing insights in it but it takes commitment to find them if you re up to the challenge then by all means read the book'}]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "robust_misclassified_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682ad94a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab4ca17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf3dd88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
