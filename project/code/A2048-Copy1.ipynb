{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f2cd8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import networkx as nx\n",
    "from torch_geometric.utils import erdos_renyi_graph, to_networkx, from_networkx\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "import math\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import transformers\n",
    "from transformers import RobertaTokenizer, BertTokenizer, RobertaModel, BertModel, AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "import pprint\n",
    "import time\n",
    "import timeit\n",
    "\n",
    "from graphModels import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5caf19d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 1024\n",
    "CHUNK_LEN = 200\n",
    "OVERLAP_LEN = int(CHUNK_LEN/2)\n",
    "\n",
    "TRAIN_BATCH_SIZE = 16\n",
    "EPOCH = 20\n",
    "lr=1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82b1daa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocess:\n",
    "    \n",
    "    def __init__(self, trainPath, devPath, testPath):\n",
    "        self.train_df = pd.read_csv(trainPath, sep = '\\t', header=0)\n",
    "        self.train_df['review'] = self.train_df['headline'].str.cat(self.train_df['text'], sep=' ')\n",
    "        \n",
    "        self.valid_df = pd.read_csv(devPath, sep = '\\t', header=0)\n",
    "        self.valid_df['review'] = self.valid_df['headline'].str.cat(self.valid_df['text'], sep=' ')\n",
    "        \n",
    "        self.test_df = pd.read_csv(testPath, sep = '\\t', header=0)\n",
    "        self.test_df['review'] = self.test_df['headline'].str.cat(self.test_df['text'], sep=' ')\n",
    "        \n",
    "    \n",
    "    def clean_text(self, sentence):\n",
    "        cleaned_sentence = re.sub(r'[^a-zA-Z0-9\\s]', ' ', sentence)\n",
    "        cleaned_sentence = re.sub(r'\\s+', ' ', cleaned_sentence).strip()\n",
    "        return cleaned_sentence.lower()\n",
    "        \n",
    "    def get_clean(self):\n",
    "        self.train_df['cleaned_text'] = self.train_df['review'].apply(self.clean_text)\n",
    "        self.valid_df['cleaned_text'] = self.valid_df['review'].apply(self.clean_text)\n",
    "        self.test_df['cleaned_text'] = self.test_df['review'].apply(self.clean_text)\n",
    "        return self.train_df[['cleaned_text', 'label']], self.valid_df[['cleaned_text', 'label']], self.test_df[['cleaned_text', 'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dea279e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pr = Preprocess(\"/scratch/smanduru/NLP/project/data/amazon_2048/amazon-books-2048-train.tsv\",\n",
    "               \"/scratch/smanduru/NLP/project/data/amazon_2048/amazon-books-2048-dev.tsv\",\n",
    "               \"/scratch/smanduru/NLP/project/data/amazon_2048/amazon-books-2048-test.tsv\")\n",
    "\n",
    "train, valid, test = pr.get_clean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90041d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# valid.shape, test.shape, train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da6d8df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# roberta_tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79ba3e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, tokenizer, max_len, df, chunk_len=200, overlap_len=50, approach=\"all\", max_size_dataset=None, min_len=249):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.overlap_len = overlap_len\n",
    "        self.chunk_len = chunk_len\n",
    "        self.approach = approach\n",
    "        self.min_len = min_len\n",
    "        self.max_size_dataset = max_size_dataset\n",
    "        self.data, self.label = self.process_data(df)\n",
    "        \n",
    "    def process_data(self, df):\n",
    "        self.num_class = len(set(df['label'].values))\n",
    "        return df['cleaned_text'].values, df['label'].values\n",
    "    \n",
    "    def long_terms_tokenizer(self, data_tokenize, targets):\n",
    "        long_terms_token = []\n",
    "        input_ids_list = []\n",
    "        attention_mask_list = []\n",
    "        token_type_ids_list = []\n",
    "        targets_list = []\n",
    "\n",
    "        previous_input_ids = data_tokenize[\"input_ids\"].reshape(-1)\n",
    "        previous_attention_mask = data_tokenize[\"attention_mask\"].reshape(-1)\n",
    "        previous_token_type_ids = data_tokenize[\"token_type_ids\"].reshape(-1)\n",
    "        remain = data_tokenize.get(\"overflowing_tokens\")\n",
    "        targets = torch.tensor(targets, dtype=torch.int)\n",
    "        \n",
    "        start_token = torch.tensor([101], dtype=torch.long)\n",
    "        end_token = torch.tensor([102], dtype=torch.long)\n",
    "\n",
    "        total_token = len(previous_input_ids) -2 # remove head 101, tail 102\n",
    "        stride = self.overlap_len - 2\n",
    "        number_chunks = math.floor(total_token/stride)\n",
    "\n",
    "        mask_list = torch.ones(self.chunk_len, dtype=torch.long)\n",
    "        type_list = torch.zeros(self.chunk_len, dtype=torch.long)\n",
    "        \n",
    "        for current in range(number_chunks-1):\n",
    "            input_ids = previous_input_ids[current*stride:current*stride+self.chunk_len-2]\n",
    "            input_ids = torch.cat((start_token, input_ids, end_token))\n",
    "            input_ids_list.append(input_ids)\n",
    "\n",
    "            attention_mask_list.append(mask_list)\n",
    "            token_type_ids_list.append(type_list)\n",
    "            targets_list.append(targets)\n",
    "\n",
    "        if len(input_ids_list) == 0:\n",
    "            input_ids = torch.ones(self.chunk_len-2, dtype=torch.long)\n",
    "            input_ids = torch.cat((start_token, input_ids, end_token))\n",
    "            input_ids_list.append(input_ids)\n",
    "\n",
    "            attention_mask_list.append(mask_list)\n",
    "            token_type_ids_list.append(type_list)\n",
    "            targets_list.append(targets)\n",
    "\n",
    "        return({\n",
    "            'ids': input_ids_list,\n",
    "            'mask': attention_mask_list,\n",
    "            'token_type_ids': token_type_ids_list,\n",
    "            'targets': targets_list,\n",
    "            'len': [torch.tensor(len(targets_list), dtype=torch.long)]\n",
    "        })\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        review = str(self.data[idx])\n",
    "        targets = int(self.label[idx])\n",
    "        data = self.tokenizer.encode_plus(\n",
    "            review,\n",
    "            max_length=self.max_len,\n",
    "            pad_to_max_length=False,\n",
    "            add_special_tokens=True,\n",
    "            return_attention_mask=True,\n",
    "            return_token_type_ids=True,\n",
    "            return_overflowing_tokens=True,\n",
    "            return_tensors='pt')\n",
    "        \n",
    "        long_token = self.long_terms_tokenizer(data, targets)\n",
    "        return long_token\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.label.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3648515a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(\n",
    "    tokenizer = bert_tokenizer,\n",
    "    max_len = MAX_LEN,\n",
    "    chunk_len = CHUNK_LEN,\n",
    "    overlap_len = OVERLAP_LEN,\n",
    "    df = train)\n",
    "\n",
    "\n",
    "valid_dataset = CustomDataset(\n",
    "    tokenizer = bert_tokenizer,\n",
    "    max_len = MAX_LEN,\n",
    "    chunk_len = CHUNK_LEN,\n",
    "    overlap_len = OVERLAP_LEN,\n",
    "    df = valid)\n",
    "\n",
    "    \n",
    "test_dataset = CustomDataset(\n",
    "    tokenizer = bert_tokenizer,\n",
    "    max_len = MAX_LEN,\n",
    "    chunk_len = CHUNK_LEN,\n",
    "    overlap_len = OVERLAP_LEN,\n",
    "    df = test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f61d2c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_collate1(batches):\n",
    "    return [{key: torch.stack(value) for key, value in batch.items()} for batch in batches]\n",
    "\n",
    "train_loader = DataLoader(train_dataset,\n",
    "                          batch_size = TRAIN_BATCH_SIZE, \n",
    "                          shuffle = True, \n",
    "                          collate_fn = my_collate1)\n",
    "\n",
    "valid_loader = DataLoader(valid_dataset,\n",
    "                          batch_size = 32, \n",
    "                          shuffle = False, \n",
    "                          collate_fn = my_collate1)\n",
    "\n",
    "test_loader = DataLoader(test_dataset,\n",
    "                          batch_size = 32, \n",
    "                          shuffle = False, \n",
    "                          collate_fn = my_collate1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e2ce60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking Data Loading\n",
    "\n",
    "# for batch_idx, batch in enumerate(test_loader):\n",
    "#     ids = batch[batch_idx]['ids']\n",
    "#     mask = batch[batch_idx]['mask']\n",
    "#     token_type_ids = batch[batch_idx]['token_type_ids']\n",
    "#     targets = batch[batch_idx]['targets']\n",
    "#     length = batch[batch_idx]['len']\n",
    "\n",
    "#     # Now you can print or process these items as needed\n",
    "#     print(f\"Batch {batch_idx + 1} IDs: {ids}\")\n",
    "#     print(f\"Batch {batch_idx + 1} Mask: {mask}\")\n",
    "#     print(f\"Batch {batch_idx + 1} Token Type IDs: {token_type_ids}\")\n",
    "#     print(f\"Batch {batch_idx + 1} Targets: {targets}\")\n",
    "#     print(f\"Batch {batch_idx + 1} Length: {length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb30f2ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8057ad20",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_training_steps = int(len(train_dataset) / TRAIN_BATCH_SIZE * EPOCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d00aae18",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hi_Bert_Classification_Model_GCN(nn.Module):\n",
    "    \n",
    "    \"\"\" A Model for bert fine tuning, put an lstm on top of BERT encoding \"\"\"\n",
    "\n",
    "    def __init__(self, graph_type, num_class, device, adj_method, pooling_method='mean'):\n",
    "        super(Hi_Bert_Classification_Model_GCN, self).__init__()\n",
    "        self.graph_type = graph_type\n",
    "        self.bert_path = 'bert-base-uncased'\n",
    "        self.bert = transformers.BertModel.from_pretrained(self.bert_path)\n",
    "        \n",
    "        # self.roberta = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "\n",
    "        self.lstm_layer_number = 2\n",
    "        'default 128 and 32'\n",
    "        self.lstm_hidden_size = 128\n",
    "        self.hidden_dim = 32\n",
    "        \n",
    "        # self.bert_lstm = nn.Linear(768, self.lstm_hidden_size)\n",
    "        self.device = device\n",
    "        self.pooling_method=pooling_method\n",
    "\n",
    "        self.mapping = nn.Linear(768, self.lstm_hidden_size).to(device)\n",
    "\n",
    "        'start GCN'\n",
    "        if self.graph_type == 'gcn':\n",
    "            self.gcn = GCN(input_dim=self.lstm_hidden_size, hidden_dim=32, output_dim=num_class).to(device)\n",
    "        elif self.graph_type == 'gat':\n",
    "            self.gcn = GAT(input_dim=self.lstm_hidden_size, hidden_dim=32, output_dim=num_class).to(device)\n",
    "        elif self.graph_type == 'graphsage':\n",
    "            self.gcn = GraphSAGE(input_dim=self.lstm_hidden_size, hidden_dim=32, output_dim=num_class).to(device)\n",
    "        elif self.graph_type == 'linear':\n",
    "            self.gcn = LinearFirst(input_dim=self.lstm_hidden_size, hidden_dim=32, output_dim=num_class).to(device)\n",
    "        elif self.graph_type == 'rank':\n",
    "            self.gcn = SimpleRank(input_dim=self.lstm_hidden_size, hidden_dim=32, output_dim=num_class).to(device)\n",
    "        elif self.graph_type == 'diffpool':\n",
    "            self.gcn = DiffPool(self.device,max_nodes=10,input_dim=self.lstm_hidden_size, hidden_dim=32, output_dim=num_class).to(device)\n",
    "        elif self.graph_type == 'hipool':\n",
    "            self.gcn = HiPool(self.device,input_dim=self.lstm_hidden_size, hidden_dim=32, output_dim=num_class).to(device)\n",
    "            \n",
    "        self.adj_method = adj_method\n",
    "\n",
    "\n",
    "    def forward(self, ids, mask, token_type_ids):\n",
    "\n",
    "        # import pdb;pdb.set_trace()\n",
    "        'encode bert'\n",
    "        bert_ids = pad_sequence(ids).permute(1, 0, 2).long().to(self.device)\n",
    "        bert_mask = pad_sequence(mask).permute(1, 0, 2).long().to(self.device)\n",
    "        bert_token_type_ids = pad_sequence(token_type_ids).permute(1, 0, 2).long().to(self.device)\n",
    "        batch_bert = []\n",
    "        for emb_pool, emb_mask, emb_token_type_ids in zip(bert_ids, bert_mask, bert_token_type_ids):\n",
    "            results = self.bert(emb_pool, attention_mask=emb_mask, token_type_ids=emb_token_type_ids)\n",
    "            batch_bert.append(results[1])\n",
    "\n",
    "        sent_bert = torch.stack(batch_bert, 0)\n",
    "        'GCN starts'\n",
    "        sent_bert = self.mapping(sent_bert)\n",
    "        node_number = sent_bert.shape[1]\n",
    "        \n",
    "\n",
    "        'random, using networkx'\n",
    "        if self.adj_method == 'random':\n",
    "            generated_adj = nx.dense_gnm_random_graph(node_number, node_number)\n",
    "        elif self.adj_method == 'er':\n",
    "            generated_adj = nx.erdos_renyi_graph(node_number, node_number)\n",
    "        elif self.adj_method == 'binom':\n",
    "            generated_adj = nx.binomial_graph(node_number, p=0.5)\n",
    "        elif self.adj_method == 'path':\n",
    "            generated_adj = nx.path_graph(node_number)\n",
    "        elif self.adj_method == 'complete':\n",
    "            generated_adj = nx.complete_graph(node_number)\n",
    "        elif self.adj_method == 'kk':\n",
    "            generated_adj = kronecker_generator(node_number)\n",
    "        elif self.adj_method == 'watts':\n",
    "            if node_number-1 > 0:\n",
    "                generated_adj = nx.watts_strogatz_graph(node_number, k=node_number-1, p=0.5)\n",
    "            else:\n",
    "                generated_adj = nx.watts_strogatz_graph(node_number, k=node_number, p=0.5)\n",
    "        elif self.adj_method == 'ba':\n",
    "            if node_number - 1>0:\n",
    "                generated_adj = nx.barabasi_albert_graph(node_number, m=node_number-1)\n",
    "            else:\n",
    "                generated_adj = nx.barabasi_albert_graph(node_number, m=node_number)\n",
    "        elif self.adj_method == 'bigbird':\n",
    "\n",
    "            # following are attention edges\n",
    "            attention_adj = np.zeros((node_number, node_number))\n",
    "            global_attention_step = 2\n",
    "            attention_adj[:, :global_attention_step] = 1\n",
    "            attention_adj[:global_attention_step, :] = 1\n",
    "            np.fill_diagonal(attention_adj,1) # fill diagonal with 1\n",
    "            half_sliding_window_size = 1\n",
    "            np.fill_diagonal(attention_adj[:,half_sliding_window_size:], 1)\n",
    "            np.fill_diagonal(attention_adj[half_sliding_window_size:, :], 1)\n",
    "            generated_adj = nx.from_numpy_matrix(attention_adj)\n",
    "\n",
    "        else:\n",
    "            generated_adj = nx.dense_gnm_random_graph(node_number, node_number)\n",
    "\n",
    "\n",
    "        nx_adj = from_networkx(generated_adj)\n",
    "        adj = nx_adj['edge_index'].to(self.device)\n",
    "\n",
    "        'combine starts'\n",
    "        # generated_adj2 = nx.dense_gnm_random_graph(node_number,node_number)\n",
    "        # nx_adj = from_networkx(generated_adj)\n",
    "        # adj = nx_adj['edge_index'].to(self.device)\n",
    "        # nx_adj2 = from_networkx(generated_adj2)\n",
    "        # adj2 = nx_adj2['edge_index'].to(self.device)\n",
    "        # adj = torch.cat([adj2, adj], 1)\n",
    "        'combine ends'\n",
    "\n",
    "        if self.adj_method == 'complete':\n",
    "            'complete connected'\n",
    "            adj = torch.ones((node_number,node_number)).to_sparse().indices().to(self.device)\n",
    "\n",
    "        if self.graph_type.endswith('pool'):\n",
    "            'diffpool only accepts dense adj'\n",
    "            adj_matrix = nx.adjacency_matrix(generated_adj).todense()\n",
    "            adj_matrix = torch.from_numpy(np.asarray(adj_matrix)).to(self.device)\n",
    "            adj = (adj,adj_matrix)\n",
    "        # if self.args.graph_type == 'hipool':\n",
    "\n",
    "        # sent_bert shape torch.Size([batch_size, 3, 768])\n",
    "        gcn_output_batch = []\n",
    "        for node_feature in sent_bert:\n",
    "            # import pdb;pdb.set_trace()\n",
    "\n",
    "            gcn_output=self.gcn(node_feature, adj)\n",
    "\n",
    "            'graph-level read out, summation'\n",
    "            gcn_output = torch.sum(gcn_output,0)\n",
    "            gcn_output_batch.append(gcn_output)\n",
    "\n",
    "        # import pdb;\n",
    "        # pdb.set_trace()\n",
    "\n",
    "        gcn_output_batch = torch.stack(gcn_output_batch, 0)\n",
    "\n",
    "        'GCN ends'\n",
    "\n",
    "        # import pdb;\n",
    "        # pdb.set_trace()\n",
    "        return gcn_output_batch,generated_adj # (batch_size, class_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3e1ae0b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = Hi_Bert_Classification_Model_GCN(graph_type = 'hipool',\n",
    "                                       num_class=train_dataset.num_class,\n",
    "                                       device=device,\n",
    "                                       adj_method='bigbird').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "46728279",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fun(outputs, targets):\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    return loss(outputs, targets)\n",
    "\n",
    "def graph_feature_stats(graph_feature_list):\n",
    "    total_number = len(graph_feature_list)\n",
    "    stats = {k:[] for k in graph_feature_list[0].keys()}\n",
    "    for feature_dict in graph_feature_list:\n",
    "        for key in stats.keys():\n",
    "            stats[key].append(feature_dict[key])\n",
    "    'get mean'\n",
    "    stats_mean = {k:sum(v)/len(v) for (k,v) in stats.items()}\n",
    "    return stats_mean\n",
    "\n",
    "def get_graph_features(graph):\n",
    "    'more https://networkx.org/documentation/stable/reference/algorithms/approximation.html'\n",
    "    try:\n",
    "\n",
    "        # import pdb;pdb.set_trace()\n",
    "        node_number = nx.number_of_nodes(graph)  # int\n",
    "        centrality = nx.degree_centrality(graph) # a dictionary\n",
    "        centrality = sum(centrality.values())/node_number\n",
    "        edge_number = nx.number_of_edges(graph) # int\n",
    "        degrees = dict(graph.degree) # a dictionary\n",
    "        degrees = sum(degrees.values()) /edge_number\n",
    "        density = nx.density(graph) # a float\n",
    "        clustring_coef = nx.average_clustering(graph) # a float Compute the average clustering coefficient for the graph G.\n",
    "        closeness_centrality = nx.closeness_centrality(graph) # dict\n",
    "        closeness_centrality = sum(closeness_centrality.values())/len(closeness_centrality)\n",
    "        number_triangles = nx.triangles(graph) # dict\n",
    "        number_triangles = sum(number_triangles.values())/len(number_triangles)\n",
    "        number_clique = nx.graph_clique_number(graph) # a float Returns the number of maximal cliques in the graph.\n",
    "        number_connected_components = nx.number_connected_components(graph) # int Returns the number of connected components.\n",
    "        # avg_shortest_path_len = nx.average_shortest_path_length(graph) # float Return the average shortest path length; The average shortest path length is the sum of path lengths d(u,v) between all pairs of nodes (assuming the length is zero if v is not reachable from v) normalized by n*(n-1) where n is the number of nodes in G.\n",
    "        # diameter = nx.distance_measures.diameter(graph) # int The diameter is the maximum eccentricity.\n",
    "        return {'node_number': node_number, 'edge_number': edge_number, 'centrality': centrality, 'degrees': degrees,\n",
    "                'density': density, 'clustring_coef': clustring_coef, 'closeness_centrality': closeness_centrality,\n",
    "                'number_triangles': number_triangles, 'number_clique': number_clique,\n",
    "                'number_connected_components': number_connected_components,\n",
    "                'avg_shortest_path_len': 0, 'diameter': 0}\n",
    "    except:\n",
    "        return {'node_number': 1, 'edge_number': 1, 'centrality': 0, 'degrees': 0,\n",
    "                'density': 0, 'clustring_coef': 0, 'closeness_centrality': 0,\n",
    "                'number_triangles': 0, 'number_clique': 0,\n",
    "                'number_connected_components': 0,\n",
    "                'avg_shortest_path_len': 0, 'diameter': 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aa18cb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop_fun1(data_loader, model, optimizer, device, scheduler=None):\n",
    "    '''optimized function for Hi-BERT'''\n",
    "\n",
    "    model.train()\n",
    "    t0 = time.time()\n",
    "    losses = []\n",
    "    #import pdb;pdb.set_trace()\n",
    "\n",
    "    graph_features = []\n",
    "    for batch_idx, batch in enumerate(data_loader):\n",
    "\n",
    "        ids = [data[\"ids\"] for data in batch] # size of 8\n",
    "        mask = [data[\"mask\"] for data in batch]\n",
    "        token_type_ids = [data[\"token_type_ids\"] for data in batch]\n",
    "        targets = [data[\"targets\"] for data in batch] # length: 8\n",
    "        length = [data['len'] for data in batch] # [tensor([3]), tensor([7]), tensor([2]), tensor([4]), tensor([2]), tensor([4]), tensor([2]), tensor([3])]\n",
    "\n",
    "\n",
    "        'cat is not working for hi-bert'\n",
    "        # ids = torch.cat(ids)\n",
    "        # mask = torch.cat(mask)\n",
    "        # token_type_ids = torch.cat(token_type_ids)\n",
    "        # targets = torch.cat(targets)\n",
    "        # length = torch.cat(length)\n",
    "\n",
    "\n",
    "        # ids = ids.to(device, dtype=torch.long)\n",
    "        # mask = mask.to(device, dtype=torch.long)\n",
    "        # token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
    "        # targets = targets.to(device, dtype=torch.long)\n",
    "\n",
    "        target_labels = torch.stack([x[0] for x in targets]).long().to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # measure time\n",
    "        start = timeit.timeit()\n",
    "        outputs,adj_graph = model(ids=ids, mask=mask, token_type_ids=token_type_ids)\n",
    "        end = timeit.timeit()\n",
    "        model_time = end - start\n",
    "\n",
    "\n",
    "        loss = loss_fun(outputs, target_labels)\n",
    "        loss.backward()\n",
    "        model.float()\n",
    "        optimizer.step()\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "        losses.append(loss.item())\n",
    "        if batch_idx % 50 == 0:\n",
    "            print(\n",
    "                f\"___ batch index = {batch_idx} / {len(data_loader)} ({100*batch_idx / len(data_loader):.2f}%), loss = {np.mean(losses[-10:]):.4f}, time = {time.time()-t0:.2f} secondes ___\")\n",
    "            t0 = time.time()\n",
    "\n",
    "        graph_features.append(get_graph_features(adj_graph))\n",
    "\n",
    "\n",
    "    stats_mean = graph_feature_stats(graph_features)\n",
    "    pprint.pprint(stats_mean)\n",
    "    \n",
    "    return losses\n",
    "\n",
    "def eval_loop_fun1(data_loader, model, device):\n",
    "    model.eval()\n",
    "    fin_targets = []\n",
    "    fin_outputs = []\n",
    "    losses = []\n",
    "    for batch_idx, batch in enumerate(data_loader):\n",
    "        ids = [data[\"ids\"] for data in batch]  # size of 8\n",
    "        mask = [data[\"mask\"] for data in batch]\n",
    "        token_type_ids = [data[\"token_type_ids\"] for data in batch]\n",
    "        targets = [data[\"targets\"] for data in batch]  # length: 8\n",
    "\n",
    "\n",
    "        with torch.no_grad():\n",
    "            target_labels = torch.stack([x[0] for x in targets]).long().to(device)\n",
    "            outputs, _ = model(ids=ids, mask=mask, token_type_ids=token_type_ids)\n",
    "            loss = loss_fun(outputs, target_labels)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "        fin_targets.append(target_labels.cpu().detach().numpy())\n",
    "        fin_outputs.append(torch.softmax(outputs, dim=1).cpu().detach().numpy())\n",
    "    return np.concatenate(fin_outputs), np.concatenate(fin_targets), losses\n",
    "\n",
    "def evaluate(target, predicted):\n",
    "    true_label_mask = [1 if (np.argmax(x)-target[i]) ==\n",
    "                       0 else 0 for i, x in enumerate(predicted)]\n",
    "    nb_prediction = len(true_label_mask)\n",
    "    true_prediction = sum(true_label_mask)\n",
    "    false_prediction = nb_prediction-true_prediction\n",
    "    accuracy = true_prediction/nb_prediction\n",
    "    return{\n",
    "        \"accuracy\": accuracy,\n",
    "        \"nb exemple\": len(target),\n",
    "        \"true_prediction\": true_prediction,\n",
    "        \"false_prediction\": false_prediction,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "408fd0b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/smanduru/.local/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "optimizer=AdamW(model.parameters(), lr=lr)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                        num_warmup_steps = 0,\n",
    "                                        num_training_steps = num_training_steps)\n",
    "val_losses=[]\n",
    "batches_losses=[]\n",
    "val_acc=[]\n",
    "avg_running_time = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7fffd01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bc3fb001",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=============== EPOCH 1 / 20 ===============\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_823864/1720383461.py:120: FutureWarning: adjacency_matrix will return a scipy.sparse array instead of a matrix in Networkx 3.0.\n",
      "  adj_matrix = nx.adjacency_matrix(generated_adj).todense()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "___ batch index = 0 / 500 (0.00%), loss = 1.5830, time = 2.88 secondes ___\n",
      "___ batch index = 50 / 500 (10.00%), loss = 1.3834, time = 87.55 secondes ___\n",
      "___ batch index = 100 / 500 (20.00%), loss = 1.3166, time = 87.03 secondes ___\n",
      "___ batch index = 150 / 500 (30.00%), loss = 1.2192, time = 87.58 secondes ___\n",
      "___ batch index = 200 / 500 (40.00%), loss = 1.2592, time = 86.49 secondes ___\n",
      "___ batch index = 250 / 500 (50.00%), loss = 1.2572, time = 88.35 secondes ___\n",
      "___ batch index = 300 / 500 (60.00%), loss = 1.1478, time = 87.02 secondes ___\n",
      "___ batch index = 350 / 500 (70.00%), loss = 1.2970, time = 87.17 secondes ___\n",
      "___ batch index = 400 / 500 (80.00%), loss = 1.1839, time = 86.91 secondes ___\n",
      "___ batch index = 450 / 500 (90.00%), loss = 1.2245, time = 87.09 secondes ___\n",
      "{'avg_shortest_path_len': 0.0,\n",
      " 'centrality': 0.8333333333333284,\n",
      " 'closeness_centrality': 0.7293447293447305,\n",
      " 'clustring_coef': 0.7883597883597948,\n",
      " 'degrees': 2.0,\n",
      " 'density': 0.8333333333333284,\n",
      " 'diameter': 0.0,\n",
      " 'edge_number': 30.0,\n",
      " 'node_number': 9.0,\n",
      " 'number_clique': 4.0,\n",
      " 'number_connected_components': 1.0,\n",
      " 'number_triangles': 6.333333333333368}\n",
      "\n",
      " ******** Running time this step.. 872.8022809028625\n",
      "\n",
      "*** avg_loss : 1.29, time : ~14.0 min (872.80 sec) ***\n",
      "\n",
      "==> evaluation : avg_loss = 1.17, time : 58.65 sec\n",
      "\n",
      "=====>\t{'accuracy': 0.496, 'nb exemple': 1000, 'true_prediction': 496, 'false_prediction': 504}\n",
      "\t§§ model has been saved §§\n",
      "\n",
      "=============== EPOCH 2 / 20 ===============\n",
      "\n",
      "___ batch index = 0 / 500 (0.00%), loss = 0.9335, time = 1.65 secondes ___\n",
      "___ batch index = 50 / 500 (10.00%), loss = 1.1528, time = 86.53 secondes ___\n",
      "___ batch index = 100 / 500 (20.00%), loss = 1.2018, time = 86.13 secondes ___\n",
      "___ batch index = 150 / 500 (30.00%), loss = 1.2335, time = 87.25 secondes ___\n",
      "___ batch index = 200 / 500 (40.00%), loss = 0.9848, time = 87.57 secondes ___\n",
      "___ batch index = 250 / 500 (50.00%), loss = 1.2582, time = 87.54 secondes ___\n",
      "___ batch index = 300 / 500 (60.00%), loss = 1.1532, time = 87.18 secondes ___\n",
      "___ batch index = 350 / 500 (70.00%), loss = 0.9650, time = 86.89 secondes ___\n",
      "___ batch index = 400 / 500 (80.00%), loss = 1.1828, time = 87.17 secondes ___\n",
      "___ batch index = 450 / 500 (90.00%), loss = 1.1117, time = 86.28 secondes ___\n",
      "{'avg_shortest_path_len': 0.0,\n",
      " 'centrality': 0.8333333333333284,\n",
      " 'closeness_centrality': 0.7293447293447305,\n",
      " 'clustring_coef': 0.7883597883597948,\n",
      " 'degrees': 2.0,\n",
      " 'density': 0.8333333333333284,\n",
      " 'diameter': 0.0,\n",
      " 'edge_number': 30.0,\n",
      " 'node_number': 9.0,\n",
      " 'number_clique': 4.0,\n",
      " 'number_connected_components': 1.0,\n",
      " 'number_triangles': 6.333333333333368}\n",
      "\n",
      " ******** Running time this step.. 869.2755908966064\n",
      "\n",
      "*** avg_loss : 1.11, time : ~14.0 min (869.28 sec) ***\n",
      "\n",
      "==> evaluation : avg_loss = 1.09, time : 58.59 sec\n",
      "\n",
      "=====>\t{'accuracy': 0.55, 'nb exemple': 1000, 'true_prediction': 550, 'false_prediction': 450}\n",
      "\t§§ model has been saved §§\n",
      "\n",
      "=============== EPOCH 3 / 20 ===============\n",
      "\n",
      "___ batch index = 0 / 500 (0.00%), loss = 0.7127, time = 1.85 secondes ___\n",
      "___ batch index = 50 / 500 (10.00%), loss = 0.9987, time = 87.49 secondes ___\n",
      "___ batch index = 100 / 500 (20.00%), loss = 0.9859, time = 87.47 secondes ___\n",
      "___ batch index = 150 / 500 (30.00%), loss = 1.0222, time = 86.98 secondes ___\n",
      "___ batch index = 200 / 500 (40.00%), loss = 1.0354, time = 86.67 secondes ___\n",
      "___ batch index = 250 / 500 (50.00%), loss = 1.0629, time = 86.90 secondes ___\n",
      "___ batch index = 300 / 500 (60.00%), loss = 0.9032, time = 87.01 secondes ___\n",
      "___ batch index = 350 / 500 (70.00%), loss = 0.9565, time = 86.81 secondes ___\n",
      "___ batch index = 400 / 500 (80.00%), loss = 1.0771, time = 86.10 secondes ___\n",
      "___ batch index = 450 / 500 (90.00%), loss = 1.0327, time = 86.91 secondes ___\n",
      "{'avg_shortest_path_len': 0.0,\n",
      " 'centrality': 0.8333333333333284,\n",
      " 'closeness_centrality': 0.7293447293447305,\n",
      " 'clustring_coef': 0.7883597883597948,\n",
      " 'degrees': 2.0,\n",
      " 'density': 0.8333333333333284,\n",
      " 'diameter': 0.0,\n",
      " 'edge_number': 30.0,\n",
      " 'node_number': 9.0,\n",
      " 'number_clique': 4.0,\n",
      " 'number_connected_components': 1.0,\n",
      " 'number_triangles': 6.333333333333368}\n",
      "\n",
      " ******** Running time this step.. 869.0744779109955\n",
      "\n",
      "*** avg_loss : 0.99, time : ~14.0 min (869.07 sec) ***\n",
      "\n",
      "==> evaluation : avg_loss = 1.06, time : 58.57 sec\n",
      "\n",
      "=====>\t{'accuracy': 0.562, 'nb exemple': 1000, 'true_prediction': 562, 'false_prediction': 438}\n",
      "\t§§ model has been saved §§\n",
      "\n",
      "=============== EPOCH 4 / 20 ===============\n",
      "\n",
      "___ batch index = 0 / 500 (0.00%), loss = 0.7431, time = 1.69 secondes ___\n",
      "___ batch index = 50 / 500 (10.00%), loss = 0.8499, time = 86.96 secondes ___\n",
      "___ batch index = 100 / 500 (20.00%), loss = 0.8164, time = 86.29 secondes ___\n",
      "___ batch index = 150 / 500 (30.00%), loss = 0.8715, time = 86.53 secondes ___\n",
      "___ batch index = 200 / 500 (40.00%), loss = 0.7767, time = 86.86 secondes ___\n",
      "___ batch index = 250 / 500 (50.00%), loss = 0.8854, time = 86.91 secondes ___\n",
      "___ batch index = 300 / 500 (60.00%), loss = 0.9805, time = 86.92 secondes ___\n",
      "___ batch index = 350 / 500 (70.00%), loss = 0.8642, time = 86.36 secondes ___\n",
      "___ batch index = 400 / 500 (80.00%), loss = 0.8246, time = 87.48 secondes ___\n",
      "___ batch index = 450 / 500 (90.00%), loss = 0.8886, time = 92.70 secondes ___\n",
      "{'avg_shortest_path_len': 0.0,\n",
      " 'centrality': 0.8333333333333284,\n",
      " 'closeness_centrality': 0.7293447293447305,\n",
      " 'clustring_coef': 0.7883597883597948,\n",
      " 'degrees': 2.0,\n",
      " 'density': 0.8333333333333284,\n",
      " 'diameter': 0.0,\n",
      " 'edge_number': 30.0,\n",
      " 'node_number': 9.0,\n",
      " 'number_clique': 4.0,\n",
      " 'number_connected_components': 1.0,\n",
      " 'number_triangles': 6.333333333333368}\n",
      "\n",
      " ******** Running time this step.. 874.3525242805481\n",
      "\n",
      "*** avg_loss : 0.88, time : ~14.0 min (874.35 sec) ***\n",
      "\n",
      "==> evaluation : avg_loss = 1.09, time : 58.55 sec\n",
      "\n",
      "=====>\t{'accuracy': 0.558, 'nb exemple': 1000, 'true_prediction': 558, 'false_prediction': 442}\n",
      "\t§§ model has been saved §§\n",
      "\n",
      "=============== EPOCH 5 / 20 ===============\n",
      "\n",
      "___ batch index = 0 / 500 (0.00%), loss = 0.9055, time = 1.66 secondes ___\n",
      "___ batch index = 50 / 500 (10.00%), loss = 0.8657, time = 87.51 secondes ___\n",
      "___ batch index = 100 / 500 (20.00%), loss = 0.7506, time = 86.86 secondes ___\n",
      "___ batch index = 150 / 500 (30.00%), loss = 0.8342, time = 86.13 secondes ___\n",
      "___ batch index = 200 / 500 (40.00%), loss = 0.9914, time = 86.42 secondes ___\n",
      "___ batch index = 250 / 500 (50.00%), loss = 0.7046, time = 86.98 secondes ___\n",
      "___ batch index = 300 / 500 (60.00%), loss = 0.7255, time = 86.88 secondes ___\n",
      "___ batch index = 350 / 500 (70.00%), loss = 0.7733, time = 87.15 secondes ___\n",
      "___ batch index = 400 / 500 (80.00%), loss = 0.6879, time = 86.52 secondes ___\n",
      "___ batch index = 450 / 500 (90.00%), loss = 0.8345, time = 87.14 secondes ___\n",
      "{'avg_shortest_path_len': 0.0,\n",
      " 'centrality': 0.8333333333333284,\n",
      " 'closeness_centrality': 0.7293447293447305,\n",
      " 'clustring_coef': 0.7883597883597948,\n",
      " 'degrees': 2.0,\n",
      " 'density': 0.8333333333333284,\n",
      " 'diameter': 0.0,\n",
      " 'edge_number': 30.0,\n",
      " 'node_number': 9.0,\n",
      " 'number_clique': 4.0,\n",
      " 'number_connected_components': 1.0,\n",
      " 'number_triangles': 6.333333333333368}\n",
      "\n",
      " ******** Running time this step.. 868.1687955856323\n",
      "\n",
      "*** avg_loss : 0.77, time : ~14.0 min (868.17 sec) ***\n",
      "\n",
      "==> evaluation : avg_loss = 1.07, time : 58.60 sec\n",
      "\n",
      "=====>\t{'accuracy': 0.581, 'nb exemple': 1000, 'true_prediction': 581, 'false_prediction': 419}\n",
      "\t§§ model has been saved §§\n",
      "\n",
      "=============== EPOCH 6 / 20 ===============\n",
      "\n",
      "___ batch index = 0 / 500 (0.00%), loss = 1.0033, time = 1.67 secondes ___\n",
      "___ batch index = 50 / 500 (10.00%), loss = 0.5897, time = 86.70 secondes ___\n",
      "___ batch index = 100 / 500 (20.00%), loss = 0.6803, time = 87.01 secondes ___\n",
      "___ batch index = 150 / 500 (30.00%), loss = 0.5337, time = 87.05 secondes ___\n",
      "___ batch index = 200 / 500 (40.00%), loss = 0.6700, time = 86.93 secondes ___\n",
      "___ batch index = 250 / 500 (50.00%), loss = 0.6393, time = 86.36 secondes ___\n",
      "___ batch index = 300 / 500 (60.00%), loss = 0.5249, time = 87.80 secondes ___\n",
      "___ batch index = 350 / 500 (70.00%), loss = 0.5718, time = 86.20 secondes ___\n",
      "___ batch index = 400 / 500 (80.00%), loss = 0.5849, time = 87.30 secondes ___\n",
      "___ batch index = 450 / 500 (90.00%), loss = 0.5898, time = 86.73 secondes ___\n",
      "{'avg_shortest_path_len': 0.0,\n",
      " 'centrality': 0.8333333333333284,\n",
      " 'closeness_centrality': 0.7293447293447305,\n",
      " 'clustring_coef': 0.7883597883597948,\n",
      " 'degrees': 2.0,\n",
      " 'density': 0.8333333333333284,\n",
      " 'diameter': 0.0,\n",
      " 'edge_number': 30.0,\n",
      " 'node_number': 9.0,\n",
      " 'number_clique': 4.0,\n",
      " 'number_connected_components': 1.0,\n",
      " 'number_triangles': 6.333333333333368}\n",
      "\n",
      " ******** Running time this step.. 868.3247311115265\n",
      "\n",
      "*** avg_loss : 0.62, time : ~14.0 min (868.32 sec) ***\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> evaluation : avg_loss = 1.14, time : 58.58 sec\n",
      "\n",
      "=====>\t{'accuracy': 0.565, 'nb exemple': 1000, 'true_prediction': 565, 'false_prediction': 435}\n",
      "\t§§ model has been saved §§\n",
      "\n",
      "=============== EPOCH 7 / 20 ===============\n",
      "\n",
      "___ batch index = 0 / 500 (0.00%), loss = 0.3348, time = 1.75 secondes ___\n",
      "___ batch index = 50 / 500 (10.00%), loss = 0.5005, time = 86.66 secondes ___\n",
      "___ batch index = 100 / 500 (20.00%), loss = 0.5130, time = 86.84 secondes ___\n",
      "___ batch index = 150 / 500 (30.00%), loss = 0.5260, time = 87.10 secondes ___\n",
      "___ batch index = 200 / 500 (40.00%), loss = 0.4926, time = 87.01 secondes ___\n",
      "___ batch index = 250 / 500 (50.00%), loss = 0.3704, time = 87.10 secondes ___\n",
      "___ batch index = 300 / 500 (60.00%), loss = 0.5633, time = 86.81 secondes ___\n",
      "___ batch index = 350 / 500 (70.00%), loss = 0.5814, time = 86.52 secondes ___\n",
      "___ batch index = 400 / 500 (80.00%), loss = 0.4629, time = 86.50 secondes ___\n",
      "___ batch index = 450 / 500 (90.00%), loss = 0.6623, time = 86.40 secondes ___\n",
      "{'avg_shortest_path_len': 0.0,\n",
      " 'centrality': 0.8333333333333284,\n",
      " 'closeness_centrality': 0.7293447293447305,\n",
      " 'clustring_coef': 0.7883597883597948,\n",
      " 'degrees': 2.0,\n",
      " 'density': 0.8333333333333284,\n",
      " 'diameter': 0.0,\n",
      " 'edge_number': 30.0,\n",
      " 'node_number': 9.0,\n",
      " 'number_clique': 4.0,\n",
      " 'number_connected_components': 1.0,\n",
      " 'number_triangles': 6.333333333333368}\n",
      "\n",
      " ******** Running time this step.. 868.3602385520935\n",
      "\n",
      "*** avg_loss : 0.51, time : ~14.0 min (868.36 sec) ***\n",
      "\n",
      "==> evaluation : avg_loss = 1.27, time : 58.59 sec\n",
      "\n",
      "=====>\t{'accuracy': 0.604, 'nb exemple': 1000, 'true_prediction': 604, 'false_prediction': 396}\n",
      "\t§§ model has been saved §§\n",
      "\n",
      "=============== EPOCH 8 / 20 ===============\n",
      "\n",
      "___ batch index = 0 / 500 (0.00%), loss = 0.4351, time = 1.71 secondes ___\n",
      "___ batch index = 50 / 500 (10.00%), loss = 0.4372, time = 86.67 secondes ___\n",
      "___ batch index = 100 / 500 (20.00%), loss = 0.4440, time = 87.31 secondes ___\n",
      "___ batch index = 150 / 500 (30.00%), loss = 0.5105, time = 86.30 secondes ___\n",
      "___ batch index = 200 / 500 (40.00%), loss = 0.4143, time = 87.26 secondes ___\n",
      "___ batch index = 250 / 500 (50.00%), loss = 0.5034, time = 86.98 secondes ___\n",
      "___ batch index = 300 / 500 (60.00%), loss = 0.5275, time = 86.44 secondes ___\n",
      "___ batch index = 350 / 500 (70.00%), loss = 0.6243, time = 86.86 secondes ___\n",
      "___ batch index = 400 / 500 (80.00%), loss = 0.4367, time = 86.54 secondes ___\n",
      "___ batch index = 450 / 500 (90.00%), loss = 0.4979, time = 87.09 secondes ___\n",
      "{'avg_shortest_path_len': 0.0,\n",
      " 'centrality': 0.8333333333333284,\n",
      " 'closeness_centrality': 0.7293447293447305,\n",
      " 'clustring_coef': 0.7883597883597948,\n",
      " 'degrees': 2.0,\n",
      " 'density': 0.8333333333333284,\n",
      " 'diameter': 0.0,\n",
      " 'edge_number': 30.0,\n",
      " 'node_number': 9.0,\n",
      " 'number_clique': 4.0,\n",
      " 'number_connected_components': 1.0,\n",
      " 'number_triangles': 6.333333333333368}\n",
      "\n",
      " ******** Running time this step.. 868.1875700950623\n",
      "\n",
      "*** avg_loss : 0.43, time : ~14.0 min (868.19 sec) ***\n",
      "\n",
      "==> evaluation : avg_loss = 1.42, time : 58.55 sec\n",
      "\n",
      "=====>\t{'accuracy': 0.575, 'nb exemple': 1000, 'true_prediction': 575, 'false_prediction': 425}\n",
      "\t§§ model has been saved §§\n",
      "\n",
      "=============== EPOCH 9 / 20 ===============\n",
      "\n",
      "___ batch index = 0 / 500 (0.00%), loss = 0.3045, time = 1.72 secondes ___\n",
      "___ batch index = 50 / 500 (10.00%), loss = 0.3605, time = 87.20 secondes ___\n",
      "___ batch index = 100 / 500 (20.00%), loss = 0.3511, time = 86.03 secondes ___\n",
      "___ batch index = 150 / 500 (30.00%), loss = 0.3009, time = 86.86 secondes ___\n",
      "___ batch index = 200 / 500 (40.00%), loss = 0.3841, time = 87.01 secondes ___\n",
      "___ batch index = 250 / 500 (50.00%), loss = 0.3065, time = 87.45 secondes ___\n",
      "___ batch index = 300 / 500 (60.00%), loss = 0.3460, time = 86.17 secondes ___\n",
      "___ batch index = 350 / 500 (70.00%), loss = 0.2961, time = 86.79 secondes ___\n",
      "___ batch index = 400 / 500 (80.00%), loss = 0.3848, time = 86.30 secondes ___\n",
      "___ batch index = 450 / 500 (90.00%), loss = 0.4641, time = 87.40 secondes ___\n",
      "{'avg_shortest_path_len': 0.0,\n",
      " 'centrality': 0.8333333333333284,\n",
      " 'closeness_centrality': 0.7293447293447305,\n",
      " 'clustring_coef': 0.7883597883597948,\n",
      " 'degrees': 2.0,\n",
      " 'density': 0.8333333333333284,\n",
      " 'diameter': 0.0,\n",
      " 'edge_number': 30.0,\n",
      " 'node_number': 9.0,\n",
      " 'number_clique': 4.0,\n",
      " 'number_connected_components': 1.0,\n",
      " 'number_triangles': 6.333333333333368}\n",
      "\n",
      " ******** Running time this step.. 867.8855485916138\n",
      "\n",
      "*** avg_loss : 0.37, time : ~14.0 min (867.89 sec) ***\n",
      "\n",
      "==> evaluation : avg_loss = 1.48, time : 58.57 sec\n",
      "\n",
      "=====>\t{'accuracy': 0.546, 'nb exemple': 1000, 'true_prediction': 546, 'false_prediction': 454}\n",
      "\t§§ model has been saved §§\n",
      "\n",
      "=============== EPOCH 10 / 20 ===============\n",
      "\n",
      "___ batch index = 0 / 500 (0.00%), loss = 0.3204, time = 1.78 secondes ___\n",
      "___ batch index = 50 / 500 (10.00%), loss = 0.2956, time = 86.70 secondes ___\n",
      "___ batch index = 100 / 500 (20.00%), loss = 0.3516, time = 86.70 secondes ___\n",
      "___ batch index = 150 / 500 (30.00%), loss = 0.3309, time = 86.61 secondes ___\n",
      "___ batch index = 200 / 500 (40.00%), loss = 0.2848, time = 86.56 secondes ___\n",
      "___ batch index = 250 / 500 (50.00%), loss = 0.2944, time = 86.49 secondes ___\n",
      "___ batch index = 300 / 500 (60.00%), loss = 0.3884, time = 87.10 secondes ___\n",
      "___ batch index = 350 / 500 (70.00%), loss = 0.2520, time = 87.13 secondes ___\n",
      "___ batch index = 400 / 500 (80.00%), loss = 0.3366, time = 86.67 secondes ___\n",
      "___ batch index = 450 / 500 (90.00%), loss = 0.3899, time = 86.81 secondes ___\n",
      "{'avg_shortest_path_len': 0.0,\n",
      " 'centrality': 0.8333333333333284,\n",
      " 'closeness_centrality': 0.7293447293447305,\n",
      " 'clustring_coef': 0.7883597883597948,\n",
      " 'degrees': 2.0,\n",
      " 'density': 0.8333333333333284,\n",
      " 'diameter': 0.0,\n",
      " 'edge_number': 30.0,\n",
      " 'node_number': 9.0,\n",
      " 'number_clique': 4.0,\n",
      " 'number_connected_components': 1.0,\n",
      " 'number_triangles': 6.333333333333368}\n",
      "\n",
      " ******** Running time this step.. 867.6611683368683\n",
      "\n",
      "*** avg_loss : 0.33, time : ~14.0 min (867.66 sec) ***\n",
      "\n",
      "==> evaluation : avg_loss = 1.59, time : 58.47 sec\n",
      "\n",
      "=====>\t{'accuracy': 0.547, 'nb exemple': 1000, 'true_prediction': 547, 'false_prediction': 453}\n",
      "\t§§ model has been saved §§\n",
      "\n",
      "=============== EPOCH 11 / 20 ===============\n",
      "\n",
      "___ batch index = 0 / 500 (0.00%), loss = 0.1757, time = 1.74 secondes ___\n",
      "___ batch index = 50 / 500 (10.00%), loss = 0.2859, time = 87.07 secondes ___\n",
      "___ batch index = 100 / 500 (20.00%), loss = 0.2360, time = 86.20 secondes ___\n",
      "___ batch index = 150 / 500 (30.00%), loss = 0.3032, time = 86.91 secondes ___\n",
      "___ batch index = 200 / 500 (40.00%), loss = 0.2402, time = 86.53 secondes ___\n",
      "___ batch index = 250 / 500 (50.00%), loss = 0.3256, time = 86.41 secondes ___\n",
      "___ batch index = 300 / 500 (60.00%), loss = 0.3734, time = 87.28 secondes ___\n",
      "___ batch index = 350 / 500 (70.00%), loss = 0.3191, time = 86.52 secondes ___\n",
      "___ batch index = 400 / 500 (80.00%), loss = 0.2735, time = 86.17 secondes ___\n",
      "___ batch index = 450 / 500 (90.00%), loss = 0.3071, time = 86.74 secondes ___\n",
      "{'avg_shortest_path_len': 0.0,\n",
      " 'centrality': 0.8333333333333284,\n",
      " 'closeness_centrality': 0.7293447293447305,\n",
      " 'clustring_coef': 0.7883597883597948,\n",
      " 'degrees': 2.0,\n",
      " 'density': 0.8333333333333284,\n",
      " 'diameter': 0.0,\n",
      " 'edge_number': 30.0,\n",
      " 'node_number': 9.0,\n",
      " 'number_clique': 4.0,\n",
      " 'number_connected_components': 1.0,\n",
      " 'number_triangles': 6.333333333333368}\n",
      "\n",
      " ******** Running time this step.. 867.5357315540314\n",
      "\n",
      "*** avg_loss : 0.28, time : ~14.0 min (867.54 sec) ***\n",
      "\n",
      "==> evaluation : avg_loss = 1.68, time : 58.83 sec\n",
      "\n",
      "=====>\t{'accuracy': 0.571, 'nb exemple': 1000, 'true_prediction': 571, 'false_prediction': 429}\n",
      "\t§§ model has been saved §§\n",
      "\n",
      "=============== EPOCH 12 / 20 ===============\n",
      "\n",
      "___ batch index = 0 / 500 (0.00%), loss = 0.1161, time = 1.68 secondes ___\n",
      "___ batch index = 50 / 500 (10.00%), loss = 0.2597, time = 86.23 secondes ___\n",
      "___ batch index = 100 / 500 (20.00%), loss = 0.4540, time = 87.36 secondes ___\n",
      "___ batch index = 150 / 500 (30.00%), loss = 0.3458, time = 86.37 secondes ___\n",
      "___ batch index = 200 / 500 (40.00%), loss = 0.3084, time = 87.06 secondes ___\n",
      "___ batch index = 250 / 500 (50.00%), loss = 0.2547, time = 86.73 secondes ___\n",
      "___ batch index = 300 / 500 (60.00%), loss = 0.3125, time = 86.93 secondes ___\n",
      "___ batch index = 350 / 500 (70.00%), loss = 0.3051, time = 86.61 secondes ___\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "___ batch index = 400 / 500 (80.00%), loss = 0.2725, time = 86.77 secondes ___\n",
      "___ batch index = 450 / 500 (90.00%), loss = 0.2058, time = 87.04 secondes ___\n",
      "{'avg_shortest_path_len': 0.0,\n",
      " 'centrality': 0.8333333333333284,\n",
      " 'closeness_centrality': 0.7293447293447305,\n",
      " 'clustring_coef': 0.7883597883597948,\n",
      " 'degrees': 2.0,\n",
      " 'density': 0.8333333333333284,\n",
      " 'diameter': 0.0,\n",
      " 'edge_number': 30.0,\n",
      " 'node_number': 9.0,\n",
      " 'number_clique': 4.0,\n",
      " 'number_connected_components': 1.0,\n",
      " 'number_triangles': 6.333333333333368}\n",
      "\n",
      " ******** Running time this step.. 867.6695470809937\n",
      "\n",
      "*** avg_loss : 0.25, time : ~14.0 min (867.67 sec) ***\n",
      "\n",
      "==> evaluation : avg_loss = 1.82, time : 58.52 sec\n",
      "\n",
      "=====>\t{'accuracy': 0.585, 'nb exemple': 1000, 'true_prediction': 585, 'false_prediction': 415}\n",
      "\t§§ model has been saved §§\n",
      "\n",
      "=============== EPOCH 13 / 20 ===============\n",
      "\n",
      "___ batch index = 0 / 500 (0.00%), loss = 0.4090, time = 1.76 secondes ___\n",
      "___ batch index = 50 / 500 (10.00%), loss = 0.1681, time = 86.54 secondes ___\n",
      "___ batch index = 100 / 500 (20.00%), loss = 0.1992, time = 86.94 secondes ___\n",
      "___ batch index = 150 / 500 (30.00%), loss = 0.2472, time = 87.25 secondes ___\n",
      "___ batch index = 200 / 500 (40.00%), loss = 0.1673, time = 86.15 secondes ___\n",
      "___ batch index = 250 / 500 (50.00%), loss = 0.1823, time = 86.71 secondes ___\n",
      "___ batch index = 300 / 500 (60.00%), loss = 0.2555, time = 87.23 secondes ___\n",
      "___ batch index = 350 / 500 (70.00%), loss = 0.2928, time = 87.06 secondes ___\n",
      "___ batch index = 400 / 500 (80.00%), loss = 0.1105, time = 87.14 secondes ___\n",
      "___ batch index = 450 / 500 (90.00%), loss = 0.2615, time = 86.46 secondes ___\n",
      "{'avg_shortest_path_len': 0.0,\n",
      " 'centrality': 0.8333333333333284,\n",
      " 'closeness_centrality': 0.7293447293447305,\n",
      " 'clustring_coef': 0.7883597883597948,\n",
      " 'degrees': 2.0,\n",
      " 'density': 0.8333333333333284,\n",
      " 'diameter': 0.0,\n",
      " 'edge_number': 30.0,\n",
      " 'node_number': 9.0,\n",
      " 'number_clique': 4.0,\n",
      " 'number_connected_components': 1.0,\n",
      " 'number_triangles': 6.333333333333368}\n",
      "\n",
      " ******** Running time this step.. 868.0330247879028\n",
      "\n",
      "*** avg_loss : 0.23, time : ~14.0 min (868.03 sec) ***\n",
      "\n",
      "==> evaluation : avg_loss = 1.79, time : 58.48 sec\n",
      "\n",
      "=====>\t{'accuracy': 0.562, 'nb exemple': 1000, 'true_prediction': 562, 'false_prediction': 438}\n",
      "\t§§ model has been saved §§\n",
      "\n",
      "=============== EPOCH 14 / 20 ===============\n",
      "\n",
      "___ batch index = 0 / 500 (0.00%), loss = 0.5988, time = 1.79 secondes ___\n",
      "___ batch index = 50 / 500 (10.00%), loss = 0.1930, time = 87.17 secondes ___\n",
      "___ batch index = 100 / 500 (20.00%), loss = 0.1620, time = 86.80 secondes ___\n",
      "___ batch index = 150 / 500 (30.00%), loss = 0.2615, time = 86.17 secondes ___\n",
      "___ batch index = 200 / 500 (40.00%), loss = 0.2481, time = 88.93 secondes ___\n",
      "___ batch index = 250 / 500 (50.00%), loss = 0.1627, time = 92.44 secondes ___\n",
      "___ batch index = 300 / 500 (60.00%), loss = 0.1472, time = 97.49 secondes ___\n",
      "___ batch index = 350 / 500 (70.00%), loss = 0.1832, time = 92.82 secondes ___\n",
      "___ batch index = 400 / 500 (80.00%), loss = 0.2074, time = 91.03 secondes ___\n",
      "___ batch index = 450 / 500 (90.00%), loss = 0.3243, time = 92.00 secondes ___\n",
      "{'avg_shortest_path_len': 0.0,\n",
      " 'centrality': 0.8333333333333284,\n",
      " 'closeness_centrality': 0.7293447293447305,\n",
      " 'clustring_coef': 0.7883597883597948,\n",
      " 'degrees': 2.0,\n",
      " 'density': 0.8333333333333284,\n",
      " 'diameter': 0.0,\n",
      " 'edge_number': 30.0,\n",
      " 'node_number': 9.0,\n",
      " 'number_clique': 4.0,\n",
      " 'number_connected_components': 1.0,\n",
      " 'number_triangles': 6.333333333333368}\n",
      "\n",
      " ******** Running time this step.. 908.1034467220306\n",
      "\n",
      "*** avg_loss : 0.19, time : ~15.0 min (908.10 sec) ***\n",
      "\n",
      "==> evaluation : avg_loss = 1.81, time : 65.99 sec\n",
      "\n",
      "=====>\t{'accuracy': 0.583, 'nb exemple': 1000, 'true_prediction': 583, 'false_prediction': 417}\n",
      "\t§§ model has been saved §§\n",
      "\n",
      "=============== EPOCH 15 / 20 ===============\n",
      "\n",
      "___ batch index = 0 / 500 (0.00%), loss = 0.1329, time = 1.82 secondes ___\n",
      "___ batch index = 50 / 500 (10.00%), loss = 0.0917, time = 89.66 secondes ___\n",
      "___ batch index = 100 / 500 (20.00%), loss = 0.2177, time = 89.89 secondes ___\n",
      "___ batch index = 150 / 500 (30.00%), loss = 0.1283, time = 91.54 secondes ___\n",
      "___ batch index = 200 / 500 (40.00%), loss = 0.1976, time = 89.56 secondes ___\n",
      "___ batch index = 250 / 500 (50.00%), loss = 0.1835, time = 90.53 secondes ___\n",
      "___ batch index = 300 / 500 (60.00%), loss = 0.1266, time = 89.98 secondes ___\n",
      "___ batch index = 350 / 500 (70.00%), loss = 0.2161, time = 91.63 secondes ___\n",
      "___ batch index = 400 / 500 (80.00%), loss = 0.2175, time = 90.70 secondes ___\n",
      "___ batch index = 450 / 500 (90.00%), loss = 0.0716, time = 87.56 secondes ___\n",
      "{'avg_shortest_path_len': 0.0,\n",
      " 'centrality': 0.8333333333333284,\n",
      " 'closeness_centrality': 0.7293447293447305,\n",
      " 'clustring_coef': 0.7883597883597948,\n",
      " 'degrees': 2.0,\n",
      " 'density': 0.8333333333333284,\n",
      " 'diameter': 0.0,\n",
      " 'edge_number': 30.0,\n",
      " 'node_number': 9.0,\n",
      " 'number_clique': 4.0,\n",
      " 'number_connected_components': 1.0,\n",
      " 'number_triangles': 6.333333333333368}\n",
      "\n",
      " ******** Running time this step.. 898.43643450737\n",
      "\n",
      "*** avg_loss : 0.16, time : ~14.0 min (898.44 sec) ***\n",
      "\n",
      "==> evaluation : avg_loss = 1.95, time : 62.90 sec\n",
      "\n",
      "=====>\t{'accuracy': 0.535, 'nb exemple': 1000, 'true_prediction': 535, 'false_prediction': 465}\n",
      "\t§§ model has been saved §§\n",
      "\n",
      "=============== EPOCH 16 / 20 ===============\n",
      "\n",
      "___ batch index = 0 / 500 (0.00%), loss = 0.4362, time = 1.65 secondes ___\n",
      "___ batch index = 50 / 500 (10.00%), loss = 0.1423, time = 92.20 secondes ___\n",
      "___ batch index = 100 / 500 (20.00%), loss = 0.1435, time = 91.31 secondes ___\n",
      "___ batch index = 150 / 500 (30.00%), loss = 0.0924, time = 91.92 secondes ___\n",
      "___ batch index = 200 / 500 (40.00%), loss = 0.1327, time = 91.32 secondes ___\n",
      "___ batch index = 250 / 500 (50.00%), loss = 0.2169, time = 92.61 secondes ___\n",
      "___ batch index = 300 / 500 (60.00%), loss = 0.1338, time = 93.78 secondes ___\n",
      "___ batch index = 350 / 500 (70.00%), loss = 0.1507, time = 90.04 secondes ___\n",
      "___ batch index = 400 / 500 (80.00%), loss = 0.1807, time = 91.22 secondes ___\n",
      "___ batch index = 450 / 500 (90.00%), loss = 0.1702, time = 89.99 secondes ___\n",
      "{'avg_shortest_path_len': 0.0,\n",
      " 'centrality': 0.8333333333333284,\n",
      " 'closeness_centrality': 0.7293447293447305,\n",
      " 'clustring_coef': 0.7883597883597948,\n",
      " 'degrees': 2.0,\n",
      " 'density': 0.8333333333333284,\n",
      " 'diameter': 0.0,\n",
      " 'edge_number': 30.0,\n",
      " 'node_number': 9.0,\n",
      " 'number_clique': 4.0,\n",
      " 'number_connected_components': 1.0,\n",
      " 'number_triangles': 6.333333333333368}\n",
      "\n",
      " ******** Running time this step.. 917.0504014492035\n",
      "\n",
      "*** avg_loss : 0.15, time : ~15.0 min (917.05 sec) ***\n",
      "\n",
      "==> evaluation : avg_loss = 1.98, time : 62.55 sec\n",
      "\n",
      "=====>\t{'accuracy': 0.576, 'nb exemple': 1000, 'true_prediction': 576, 'false_prediction': 424}\n",
      "\t§§ model has been saved §§\n",
      "\n",
      "=============== EPOCH 17 / 20 ===============\n",
      "\n",
      "___ batch index = 0 / 500 (0.00%), loss = 0.2267, time = 1.99 secondes ___\n",
      "___ batch index = 50 / 500 (10.00%), loss = 0.1007, time = 87.74 secondes ___\n",
      "___ batch index = 100 / 500 (20.00%), loss = 0.1308, time = 89.04 secondes ___\n",
      "___ batch index = 150 / 500 (30.00%), loss = 0.1450, time = 89.93 secondes ___\n",
      "___ batch index = 200 / 500 (40.00%), loss = 0.1494, time = 88.95 secondes ___\n",
      "___ batch index = 250 / 500 (50.00%), loss = 0.1217, time = 90.64 secondes ___\n",
      "___ batch index = 300 / 500 (60.00%), loss = 0.1182, time = 93.60 secondes ___\n",
      "___ batch index = 350 / 500 (70.00%), loss = 0.1059, time = 92.33 secondes ___\n",
      "___ batch index = 400 / 500 (80.00%), loss = 0.1506, time = 93.35 secondes ___\n",
      "___ batch index = 450 / 500 (90.00%), loss = 0.1288, time = 92.04 secondes ___\n",
      "{'avg_shortest_path_len': 0.0,\n",
      " 'centrality': 0.8333333333333284,\n",
      " 'closeness_centrality': 0.7293447293447305,\n",
      " 'clustring_coef': 0.7883597883597948,\n",
      " 'degrees': 2.0,\n",
      " 'density': 0.8333333333333284,\n",
      " 'diameter': 0.0,\n",
      " 'edge_number': 30.0,\n",
      " 'node_number': 9.0,\n",
      " 'number_clique': 4.0,\n",
      " 'number_connected_components': 1.0,\n",
      " 'number_triangles': 6.333333333333368}\n",
      "\n",
      " ******** Running time this step.. 910.8836414813995\n",
      "\n",
      "*** avg_loss : 0.13, time : ~15.0 min (910.88 sec) ***\n",
      "\n",
      "==> evaluation : avg_loss = 2.01, time : 64.53 sec\n",
      "\n",
      "=====>\t{'accuracy': 0.598, 'nb exemple': 1000, 'true_prediction': 598, 'false_prediction': 402}\n",
      "\t§§ model has been saved §§\n",
      "\n",
      "=============== EPOCH 18 / 20 ===============\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "___ batch index = 0 / 500 (0.00%), loss = 0.4902, time = 1.66 secondes ___\n",
      "___ batch index = 50 / 500 (10.00%), loss = 0.1201, time = 91.60 secondes ___\n",
      "___ batch index = 100 / 500 (20.00%), loss = 0.0729, time = 92.55 secondes ___\n",
      "___ batch index = 150 / 500 (30.00%), loss = 0.1173, time = 91.09 secondes ___\n",
      "___ batch index = 200 / 500 (40.00%), loss = 0.0915, time = 90.36 secondes ___\n",
      "___ batch index = 250 / 500 (50.00%), loss = 0.0621, time = 92.89 secondes ___\n",
      "___ batch index = 300 / 500 (60.00%), loss = 0.0844, time = 90.02 secondes ___\n",
      "___ batch index = 350 / 500 (70.00%), loss = 0.1607, time = 89.99 secondes ___\n",
      "___ batch index = 400 / 500 (80.00%), loss = 0.1543, time = 89.56 secondes ___\n",
      "___ batch index = 450 / 500 (90.00%), loss = 0.1715, time = 91.83 secondes ___\n",
      "{'avg_shortest_path_len': 0.0,\n",
      " 'centrality': 0.8333333333333284,\n",
      " 'closeness_centrality': 0.7293447293447305,\n",
      " 'clustring_coef': 0.7883597883597948,\n",
      " 'degrees': 2.0,\n",
      " 'density': 0.8333333333333284,\n",
      " 'diameter': 0.0,\n",
      " 'edge_number': 30.0,\n",
      " 'node_number': 9.0,\n",
      " 'number_clique': 4.0,\n",
      " 'number_connected_components': 1.0,\n",
      " 'number_triangles': 6.333333333333368}\n",
      "\n",
      " ******** Running time this step.. 911.6397886276245\n",
      "\n",
      "*** avg_loss : 0.13, time : ~15.0 min (911.64 sec) ***\n",
      "\n",
      "==> evaluation : avg_loss = 1.96, time : 66.45 sec\n",
      "\n",
      "=====>\t{'accuracy': 0.586, 'nb exemple': 1000, 'true_prediction': 586, 'false_prediction': 414}\n",
      "\t§§ model has been saved §§\n",
      "\n",
      "=============== EPOCH 19 / 20 ===============\n",
      "\n",
      "___ batch index = 0 / 500 (0.00%), loss = 0.0417, time = 1.76 secondes ___\n",
      "___ batch index = 50 / 500 (10.00%), loss = 0.1027, time = 91.21 secondes ___\n",
      "___ batch index = 100 / 500 (20.00%), loss = 0.0472, time = 92.81 secondes ___\n",
      "___ batch index = 150 / 500 (30.00%), loss = 0.0948, time = 92.17 secondes ___\n",
      "___ batch index = 200 / 500 (40.00%), loss = 0.0658, time = 88.40 secondes ___\n",
      "___ batch index = 250 / 500 (50.00%), loss = 0.0936, time = 90.39 secondes ___\n",
      "___ batch index = 300 / 500 (60.00%), loss = 0.0967, time = 90.34 secondes ___\n",
      "___ batch index = 350 / 500 (70.00%), loss = 0.1160, time = 90.75 secondes ___\n",
      "___ batch index = 400 / 500 (80.00%), loss = 0.0931, time = 90.64 secondes ___\n",
      "___ batch index = 450 / 500 (90.00%), loss = 0.0958, time = 91.02 secondes ___\n",
      "{'avg_shortest_path_len': 0.0,\n",
      " 'centrality': 0.8333333333333284,\n",
      " 'closeness_centrality': 0.7293447293447305,\n",
      " 'clustring_coef': 0.7883597883597948,\n",
      " 'degrees': 2.0,\n",
      " 'density': 0.8333333333333284,\n",
      " 'diameter': 0.0,\n",
      " 'edge_number': 30.0,\n",
      " 'node_number': 9.0,\n",
      " 'number_clique': 4.0,\n",
      " 'number_connected_components': 1.0,\n",
      " 'number_triangles': 6.333333333333368}\n",
      "\n",
      " ******** Running time this step.. 907.8080608844757\n",
      "\n",
      "*** avg_loss : 0.10, time : ~15.0 min (907.81 sec) ***\n",
      "\n",
      "==> evaluation : avg_loss = 2.00, time : 61.49 sec\n",
      "\n",
      "=====>\t{'accuracy': 0.576, 'nb exemple': 1000, 'true_prediction': 576, 'false_prediction': 424}\n",
      "\t§§ model has been saved §§\n",
      "\n",
      "=============== EPOCH 20 / 20 ===============\n",
      "\n",
      "___ batch index = 0 / 500 (0.00%), loss = 0.0169, time = 1.74 secondes ___\n",
      "___ batch index = 50 / 500 (10.00%), loss = 0.0620, time = 93.08 secondes ___\n",
      "___ batch index = 100 / 500 (20.00%), loss = 0.0524, time = 93.21 secondes ___\n",
      "___ batch index = 150 / 500 (30.00%), loss = 0.0724, time = 92.32 secondes ___\n",
      "___ batch index = 200 / 500 (40.00%), loss = 0.0689, time = 91.89 secondes ___\n",
      "___ batch index = 250 / 500 (50.00%), loss = 0.0838, time = 86.53 secondes ___\n",
      "___ batch index = 300 / 500 (60.00%), loss = 0.1064, time = 88.69 secondes ___\n",
      "___ batch index = 350 / 500 (70.00%), loss = 0.0724, time = 87.63 secondes ___\n",
      "___ batch index = 400 / 500 (80.00%), loss = 0.1373, time = 87.90 secondes ___\n",
      "___ batch index = 450 / 500 (90.00%), loss = 0.0989, time = 87.51 secondes ___\n",
      "{'avg_shortest_path_len': 0.0,\n",
      " 'centrality': 0.8333333333333284,\n",
      " 'closeness_centrality': 0.7293447293447305,\n",
      " 'clustring_coef': 0.7883597883597948,\n",
      " 'degrees': 2.0,\n",
      " 'density': 0.8333333333333284,\n",
      " 'diameter': 0.0,\n",
      " 'edge_number': 30.0,\n",
      " 'node_number': 9.0,\n",
      " 'number_clique': 4.0,\n",
      " 'number_connected_components': 1.0,\n",
      " 'number_triangles': 6.333333333333368}\n",
      "\n",
      " ******** Running time this step.. 897.0186433792114\n",
      "\n",
      "*** avg_loss : 0.09, time : ~14.0 min (897.02 sec) ***\n",
      "\n",
      "==> evaluation : avg_loss = 2.12, time : 59.92 sec\n",
      "\n",
      "=====>\t{'accuracy': 0.582, 'nb exemple': 1000, 'true_prediction': 582, 'false_prediction': 418}\n",
      "\t§§ model has been saved §§\n",
      "\n",
      "\n",
      "$$$$ average running time per epoch (sec).. 882.4139022946358\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCH):\n",
    "\n",
    "    t0 = time.time()\n",
    "    print(f\"\\n=============== EPOCH {epoch+1} / {EPOCH} ===============\\n\")\n",
    "    batches_losses_tmp=train_loop_fun1(train_loader, model, optimizer, device)\n",
    "    epoch_loss=np.mean(batches_losses_tmp)\n",
    "    print (\"\\n ******** Running time this step..\",time.time()-t0)\n",
    "    avg_running_time.append(time.time()-t0)\n",
    "    print(f\"\\n*** avg_loss : {epoch_loss:.2f}, time : ~{(time.time()-t0)//60} min ({time.time()-t0:.2f} sec) ***\\n\")\n",
    "    t1=time.time()\n",
    "    output, target, val_losses_tmp=eval_loop_fun1(valid_loader, model, device)\n",
    "    print(f\"==> evaluation : avg_loss = {np.mean(val_losses_tmp):.2f}, time : {time.time()-t1:.2f} sec\\n\")\n",
    "    tmp_evaluate=evaluate(target.reshape(-1), output)\n",
    "    print(f\"=====>\\t{tmp_evaluate}\")\n",
    "    val_acc.append(tmp_evaluate['accuracy'])\n",
    "    val_losses.append(val_losses_tmp)\n",
    "    batches_losses.append(batches_losses_tmp)\n",
    "    print(\"\\t§§ model has been saved §§\")\n",
    "\n",
    "print(\"\\n\\n$$$$ average running time per epoch (sec)..\", sum(avg_running_time)/len(avg_running_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7ba38dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, '/scratch/smanduru/NLP/project/saved_models' + '/hipool_20eps.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a860d77f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ORC)",
   "language": "python",
   "name": "sys_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
